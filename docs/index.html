<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Asciidoctor 2.0.20">
<meta name="author" content="System: v0.6.6-dev | Docbook: v250824-4 | www.kernel0.org">
<title>RK0: The Real-Time Kernel '0'</title>
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:300,300italic,400,400italic,600,600italic%7CNoto+Serif:400,400italic,700,700italic%7CDroid+Sans+Mono:400,700">
<style>
/*! Asciidoctor default stylesheet | MIT License | https://asciidoctor.org */
/* Uncomment the following line when using as a custom stylesheet */
/* @import "https://fonts.googleapis.com/css?family=Open+Sans:300,300italic,400,400italic,600,600italic%7CNoto+Serif:400,400italic,700,700italic%7CDroid+Sans+Mono:400,700"; */
html{font-family:sans-serif;-webkit-text-size-adjust:100%}
a{background:none}
a:focus{outline:thin dotted}
a:active,a:hover{outline:0}
h1{font-size:2em;margin:.67em 0}
b,strong{font-weight:bold}
abbr{font-size:.9em}
abbr[title]{cursor:help;border-bottom:1px dotted #dddddf;text-decoration:none}
dfn{font-style:italic}
hr{height:0}
mark{background:#ff0;color:#000}
code,kbd,pre,samp{font-family:monospace;font-size:1em}
pre{white-space:pre-wrap}
q{quotes:"\201C" "\201D" "\2018" "\2019"}
small{font-size:80%}
sub,sup{font-size:75%;line-height:0;position:relative;vertical-align:baseline}
sup{top:-.5em}
sub{bottom:-.25em}
img{border:0}
svg:not(:root){overflow:hidden}
figure{margin:0}
audio,video{display:inline-block}
audio:not([controls]){display:none;height:0}
fieldset{border:1px solid silver;margin:0 2px;padding:.35em .625em .75em}
legend{border:0;padding:0}
button,input,select,textarea{font-family:inherit;font-size:100%;margin:0}
button,input{line-height:normal}
button,select{text-transform:none}
button,html input[type=button],input[type=reset],input[type=submit]{-webkit-appearance:button;cursor:pointer}
button[disabled],html input[disabled]{cursor:default}
input[type=checkbox],input[type=radio]{padding:0}
button::-moz-focus-inner,input::-moz-focus-inner{border:0;padding:0}
textarea{overflow:auto;vertical-align:top}
table{border-collapse:collapse;border-spacing:0}
*,::before,::after{box-sizing:border-box}
html,body{font-size:100%}
body{background:#fff;color:rgba(0,0,0,.8);padding:0;margin:0;font-family:"Noto Serif","DejaVu Serif",serif;line-height:1;position:relative;cursor:auto;-moz-tab-size:4;-o-tab-size:4;tab-size:4;word-wrap:anywhere;-moz-osx-font-smoothing:grayscale;-webkit-font-smoothing:antialiased}
a:hover{cursor:pointer}
img,object,embed{max-width:100%;height:auto}
object,embed{height:100%}
img{-ms-interpolation-mode:bicubic}
.left{float:left!important}
.right{float:right!important}
.text-left{text-align:left!important}
.text-right{text-align:right!important}
.text-center{text-align:center!important}
.text-justify{text-align:justify!important}
.hide{display:none}
img,object,svg{display:inline-block;vertical-align:middle}
textarea{height:auto;min-height:50px}
select{width:100%}
.subheader,.admonitionblock td.content>.title,.audioblock>.title,.exampleblock>.title,.imageblock>.title,.listingblock>.title,.literalblock>.title,.stemblock>.title,.openblock>.title,.paragraph>.title,.quoteblock>.title,table.tableblock>.title,.verseblock>.title,.videoblock>.title,.dlist>.title,.olist>.title,.ulist>.title,.qlist>.title,.hdlist>.title{line-height:1.45;color:#7a2518;font-weight:400;margin-top:0;margin-bottom:.25em}
div,dl,dt,dd,ul,ol,li,h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6,pre,form,p,blockquote,th,td{margin:0;padding:0}
a{color:#2156a5;text-decoration:underline;line-height:inherit}
a:hover,a:focus{color:#1d4b8f}
a img{border:0}
p{line-height:1.6;margin-bottom:1.25em;text-rendering:optimizeLegibility}
p aside{font-size:.875em;line-height:1.35;font-style:italic}
h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6{font-family:"Open Sans","DejaVu Sans",sans-serif;font-weight:300;font-style:normal;color:#ba3925;text-rendering:optimizeLegibility;margin-top:1em;margin-bottom:.5em;line-height:1.0125em}
h1 small,h2 small,h3 small,#toctitle small,.sidebarblock>.content>.title small,h4 small,h5 small,h6 small{font-size:60%;color:#e99b8f;line-height:0}
h1{font-size:2.125em}
h2{font-size:1.6875em}
h3,#toctitle,.sidebarblock>.content>.title{font-size:1.375em}
h4,h5{font-size:1.125em}
h6{font-size:1em}
hr{border:solid #dddddf;border-width:1px 0 0;clear:both;margin:1.25em 0 1.1875em}
em,i{font-style:italic;line-height:inherit}
strong,b{font-weight:bold;line-height:inherit}
small{font-size:60%;line-height:inherit}
code{font-family:"Droid Sans Mono","DejaVu Sans Mono",monospace;font-weight:400;color:rgba(0,0,0,.9)}
ul,ol,dl{line-height:1.6;margin-bottom:1.25em;list-style-position:outside;font-family:inherit}
ul,ol{margin-left:1.5em}
ul li ul,ul li ol{margin-left:1.25em;margin-bottom:0}
ul.circle{list-style-type:circle}
ul.disc{list-style-type:disc}
ul.square{list-style-type:square}
ul.circle ul:not([class]),ul.disc ul:not([class]),ul.square ul:not([class]){list-style:inherit}
ol li ul,ol li ol{margin-left:1.25em;margin-bottom:0}
dl dt{margin-bottom:.3125em;font-weight:bold}
dl dd{margin-bottom:1.25em}
blockquote{margin:0 0 1.25em;padding:.5625em 1.25em 0 1.1875em;border-left:1px solid #ddd}
blockquote,blockquote p{line-height:1.6;color:rgba(0,0,0,.85)}
@media screen and (min-width:768px){h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6{line-height:1.2}
h1{font-size:2.75em}
h2{font-size:2.3125em}
h3,#toctitle,.sidebarblock>.content>.title{font-size:1.6875em}
h4{font-size:1.4375em}}
table{background:#fff;margin-bottom:1.25em;border:1px solid #dedede;word-wrap:normal}
table thead,table tfoot{background:#f7f8f7}
table thead tr th,table thead tr td,table tfoot tr th,table tfoot tr td{padding:.5em .625em .625em;font-size:inherit;color:rgba(0,0,0,.8);text-align:left}
table tr th,table tr td{padding:.5625em .625em;font-size:inherit;color:rgba(0,0,0,.8)}
table tr.even,table tr.alt{background:#f8f8f7}
table thead tr th,table tfoot tr th,table tbody tr td,table tr td,table tfoot tr td{line-height:1.6}
h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6{line-height:1.2;word-spacing:-.05em}
h1 strong,h2 strong,h3 strong,#toctitle strong,.sidebarblock>.content>.title strong,h4 strong,h5 strong,h6 strong{font-weight:400}
.center{margin-left:auto;margin-right:auto}
.stretch{width:100%}
.clearfix::before,.clearfix::after,.float-group::before,.float-group::after{content:" ";display:table}
.clearfix::after,.float-group::after{clear:both}
:not(pre).nobreak{word-wrap:normal}
:not(pre).nowrap{white-space:nowrap}
:not(pre).pre-wrap{white-space:pre-wrap}
:not(pre):not([class^=L])>code{font-size:.9375em;font-style:normal!important;letter-spacing:0;padding:.1em .5ex;word-spacing:-.15em;background:#f7f7f8;border-radius:4px;line-height:1.45;text-rendering:optimizeSpeed}
pre{color:rgba(0,0,0,.9);font-family:"Droid Sans Mono","DejaVu Sans Mono",monospace;line-height:1.45;text-rendering:optimizeSpeed}
pre code,pre pre{color:inherit;font-size:inherit;line-height:inherit}
pre>code{display:block}
pre.nowrap,pre.nowrap pre{white-space:pre;word-wrap:normal}
em em{font-style:normal}
strong strong{font-weight:400}
.keyseq{color:rgba(51,51,51,.8)}
kbd{font-family:"Droid Sans Mono","DejaVu Sans Mono",monospace;display:inline-block;color:rgba(0,0,0,.8);font-size:.65em;line-height:1.45;background:#f7f7f7;border:1px solid #ccc;border-radius:3px;box-shadow:0 1px 0 rgba(0,0,0,.2),inset 0 0 0 .1em #fff;margin:0 .15em;padding:.2em .5em;vertical-align:middle;position:relative;top:-.1em;white-space:nowrap}
.keyseq kbd:first-child{margin-left:0}
.keyseq kbd:last-child{margin-right:0}
.menuseq,.menuref{color:#000}
.menuseq b:not(.caret),.menuref{font-weight:inherit}
.menuseq{word-spacing:-.02em}
.menuseq b.caret{font-size:1.25em;line-height:.8}
.menuseq i.caret{font-weight:bold;text-align:center;width:.45em}
b.button::before,b.button::after{position:relative;top:-1px;font-weight:400}
b.button::before{content:"[";padding:0 3px 0 2px}
b.button::after{content:"]";padding:0 2px 0 3px}
p a>code:hover{color:rgba(0,0,0,.9)}
#header,#content,#footnotes,#footer{width:100%;margin:0 auto;max-width:62.5em;*zoom:1;position:relative;padding-left:.9375em;padding-right:.9375em}
#header::before,#header::after,#content::before,#content::after,#footnotes::before,#footnotes::after,#footer::before,#footer::after{content:" ";display:table}
#header::after,#content::after,#footnotes::after,#footer::after{clear:both}
#content{margin-top:1.25em}
#content::before{content:none}
#header>h1:first-child{color:rgba(0,0,0,.85);margin-top:2.25rem;margin-bottom:0}
#header>h1:first-child+#toc{margin-top:8px;border-top:1px solid #dddddf}
#header>h1:only-child,body.toc2 #header>h1:nth-last-child(2){border-bottom:1px solid #dddddf;padding-bottom:8px}
#header .details{border-bottom:1px solid #dddddf;line-height:1.45;padding-top:.25em;padding-bottom:.25em;padding-left:.25em;color:rgba(0,0,0,.6);display:flex;flex-flow:row wrap}
#header .details span:first-child{margin-left:-.125em}
#header .details span.email a{color:rgba(0,0,0,.85)}
#header .details br{display:none}
#header .details br+span::before{content:"\00a0\2013\00a0"}
#header .details br+span.author::before{content:"\00a0\22c5\00a0";color:rgba(0,0,0,.85)}
#header .details br+span#revremark::before{content:"\00a0|\00a0"}
#header #revnumber{text-transform:capitalize}
#header #revnumber::after{content:"\00a0"}
#content>h1:first-child:not([class]){color:rgba(0,0,0,.85);border-bottom:1px solid #dddddf;padding-bottom:8px;margin-top:0;padding-top:1rem;margin-bottom:1.25rem}
#toc{border-bottom:1px solid #e7e7e9;padding-bottom:.5em}
#toc>ul{margin-left:.125em}
#toc ul.sectlevel0>li>a{font-style:italic}
#toc ul.sectlevel0 ul.sectlevel1{margin:.5em 0}
#toc ul{font-family:"Open Sans","DejaVu Sans",sans-serif;list-style-type:none}
#toc li{line-height:1.3334;margin-top:.3334em}
#toc a{text-decoration:none}
#toc a:active{text-decoration:underline}
#toctitle{color:#7a2518;font-size:1.2em}
@media screen and (min-width:768px){#toctitle{font-size:1.375em}
body.toc2{padding-left:15em;padding-right:0}
#toc.toc2{margin-top:0!important;background:#f8f8f7;position:fixed;width:15em;left:0;top:0;border-right:1px solid #e7e7e9;border-top-width:0!important;border-bottom-width:0!important;z-index:1000;padding:1.25em 1em;height:100%;overflow:auto}
#toc.toc2 #toctitle{margin-top:0;margin-bottom:.8rem;font-size:1.2em}
#toc.toc2>ul{font-size:.9em;margin-bottom:0}
#toc.toc2 ul ul{margin-left:0;padding-left:1em}
#toc.toc2 ul.sectlevel0 ul.sectlevel1{padding-left:0;margin-top:.5em;margin-bottom:.5em}
body.toc2.toc-right{padding-left:0;padding-right:15em}
body.toc2.toc-right #toc.toc2{border-right-width:0;border-left:1px solid #e7e7e9;left:auto;right:0}}
@media screen and (min-width:1280px){body.toc2{padding-left:20em;padding-right:0}
#toc.toc2{width:20em}
#toc.toc2 #toctitle{font-size:1.375em}
#toc.toc2>ul{font-size:.95em}
#toc.toc2 ul ul{padding-left:1.25em}
body.toc2.toc-right{padding-left:0;padding-right:20em}}
#content #toc{border:1px solid #e0e0dc;margin-bottom:1.25em;padding:1.25em;background:#f8f8f7;border-radius:4px}
#content #toc>:first-child{margin-top:0}
#content #toc>:last-child{margin-bottom:0}
#footer{max-width:none;background:rgba(0,0,0,.8);padding:1.25em}
#footer-text{color:hsla(0,0%,100%,.8);line-height:1.44}
#content{margin-bottom:.625em}
.sect1{padding-bottom:.625em}
@media screen and (min-width:768px){#content{margin-bottom:1.25em}
.sect1{padding-bottom:1.25em}}
.sect1:last-child{padding-bottom:0}
.sect1+.sect1{border-top:1px solid #e7e7e9}
#content h1>a.anchor,h2>a.anchor,h3>a.anchor,#toctitle>a.anchor,.sidebarblock>.content>.title>a.anchor,h4>a.anchor,h5>a.anchor,h6>a.anchor{position:absolute;z-index:1001;width:1.5ex;margin-left:-1.5ex;display:block;text-decoration:none!important;visibility:hidden;text-align:center;font-weight:400}
#content h1>a.anchor::before,h2>a.anchor::before,h3>a.anchor::before,#toctitle>a.anchor::before,.sidebarblock>.content>.title>a.anchor::before,h4>a.anchor::before,h5>a.anchor::before,h6>a.anchor::before{content:"\00A7";font-size:.85em;display:block;padding-top:.1em}
#content h1:hover>a.anchor,#content h1>a.anchor:hover,h2:hover>a.anchor,h2>a.anchor:hover,h3:hover>a.anchor,#toctitle:hover>a.anchor,.sidebarblock>.content>.title:hover>a.anchor,h3>a.anchor:hover,#toctitle>a.anchor:hover,.sidebarblock>.content>.title>a.anchor:hover,h4:hover>a.anchor,h4>a.anchor:hover,h5:hover>a.anchor,h5>a.anchor:hover,h6:hover>a.anchor,h6>a.anchor:hover{visibility:visible}
#content h1>a.link,h2>a.link,h3>a.link,#toctitle>a.link,.sidebarblock>.content>.title>a.link,h4>a.link,h5>a.link,h6>a.link{color:#ba3925;text-decoration:none}
#content h1>a.link:hover,h2>a.link:hover,h3>a.link:hover,#toctitle>a.link:hover,.sidebarblock>.content>.title>a.link:hover,h4>a.link:hover,h5>a.link:hover,h6>a.link:hover{color:#a53221}
details,.audioblock,.imageblock,.literalblock,.listingblock,.stemblock,.videoblock{margin-bottom:1.25em}
details{margin-left:1.25rem}
details>summary{cursor:pointer;display:block;position:relative;line-height:1.6;margin-bottom:.625rem;outline:none;-webkit-tap-highlight-color:transparent}
details>summary::-webkit-details-marker{display:none}
details>summary::before{content:"";border:solid transparent;border-left:solid;border-width:.3em 0 .3em .5em;position:absolute;top:.5em;left:-1.25rem;transform:translateX(15%)}
details[open]>summary::before{border:solid transparent;border-top:solid;border-width:.5em .3em 0;transform:translateY(15%)}
details>summary::after{content:"";width:1.25rem;height:1em;position:absolute;top:.3em;left:-1.25rem}
.admonitionblock td.content>.title,.audioblock>.title,.exampleblock>.title,.imageblock>.title,.listingblock>.title,.literalblock>.title,.stemblock>.title,.openblock>.title,.paragraph>.title,.quoteblock>.title,table.tableblock>.title,.verseblock>.title,.videoblock>.title,.dlist>.title,.olist>.title,.ulist>.title,.qlist>.title,.hdlist>.title{text-rendering:optimizeLegibility;text-align:left;font-family:"Noto Serif","DejaVu Serif",serif;font-size:1rem;font-style:italic}
table.tableblock.fit-content>caption.title{white-space:nowrap;width:0}
.paragraph.lead>p,#preamble>.sectionbody>[class=paragraph]:first-of-type p{font-size:1.21875em;line-height:1.6;color:rgba(0,0,0,.85)}
.admonitionblock>table{border-collapse:separate;border:0;background:none;width:100%}
.admonitionblock>table td.icon{text-align:center;width:80px}
.admonitionblock>table td.icon img{max-width:none}
.admonitionblock>table td.icon .title{font-weight:bold;font-family:"Open Sans","DejaVu Sans",sans-serif;text-transform:uppercase}
.admonitionblock>table td.content{padding-left:1.125em;padding-right:1.25em;border-left:1px solid #dddddf;color:rgba(0,0,0,.6);word-wrap:anywhere}
.admonitionblock>table td.content>:last-child>:last-child{margin-bottom:0}
.exampleblock>.content{border:1px solid #e6e6e6;margin-bottom:1.25em;padding:1.25em;background:#fff;border-radius:4px}
.sidebarblock{border:1px solid #dbdbd6;margin-bottom:1.25em;padding:1.25em;background:#f3f3f2;border-radius:4px}
.sidebarblock>.content>.title{color:#7a2518;margin-top:0;text-align:center}
.exampleblock>.content>:first-child,.sidebarblock>.content>:first-child{margin-top:0}
.exampleblock>.content>:last-child,.exampleblock>.content>:last-child>:last-child,.exampleblock>.content .olist>ol>li:last-child>:last-child,.exampleblock>.content .ulist>ul>li:last-child>:last-child,.exampleblock>.content .qlist>ol>li:last-child>:last-child,.sidebarblock>.content>:last-child,.sidebarblock>.content>:last-child>:last-child,.sidebarblock>.content .olist>ol>li:last-child>:last-child,.sidebarblock>.content .ulist>ul>li:last-child>:last-child,.sidebarblock>.content .qlist>ol>li:last-child>:last-child{margin-bottom:0}
.literalblock pre,.listingblock>.content>pre{border-radius:4px;overflow-x:auto;padding:1em;font-size:.8125em}
@media screen and (min-width:768px){.literalblock pre,.listingblock>.content>pre{font-size:.90625em}}
@media screen and (min-width:1280px){.literalblock pre,.listingblock>.content>pre{font-size:1em}}
.literalblock pre,.listingblock>.content>pre:not(.highlight),.listingblock>.content>pre[class=highlight],.listingblock>.content>pre[class^="highlight "]{background:#f7f7f8}
.literalblock.output pre{color:#f7f7f8;background:rgba(0,0,0,.9)}
.listingblock>.content{position:relative}
.listingblock code[data-lang]::before{display:none;content:attr(data-lang);position:absolute;font-size:.75em;top:.425rem;right:.5rem;line-height:1;text-transform:uppercase;color:inherit;opacity:.5}
.listingblock:hover code[data-lang]::before{display:block}
.listingblock.terminal pre .command::before{content:attr(data-prompt);padding-right:.5em;color:inherit;opacity:.5}
.listingblock.terminal pre .command:not([data-prompt])::before{content:"$"}
.listingblock pre.highlightjs{padding:0}
.listingblock pre.highlightjs>code{padding:1em;border-radius:4px}
.listingblock pre.prettyprint{border-width:0}
.prettyprint{background:#f7f7f8}
pre.prettyprint .linenums{line-height:1.45;margin-left:2em}
pre.prettyprint li{background:none;list-style-type:inherit;padding-left:0}
pre.prettyprint li code[data-lang]::before{opacity:1}
pre.prettyprint li:not(:first-child) code[data-lang]::before{display:none}
table.linenotable{border-collapse:separate;border:0;margin-bottom:0;background:none}
table.linenotable td[class]{color:inherit;vertical-align:top;padding:0;line-height:inherit;white-space:normal}
table.linenotable td.code{padding-left:.75em}
table.linenotable td.linenos,pre.pygments .linenos{border-right:1px solid;opacity:.35;padding-right:.5em;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}
pre.pygments span.linenos{display:inline-block;margin-right:.75em}
.quoteblock{margin:0 1em 1.25em 1.5em;display:table}
.quoteblock:not(.excerpt)>.title{margin-left:-1.5em;margin-bottom:.75em}
.quoteblock blockquote,.quoteblock p{color:rgba(0,0,0,.85);font-size:1.15rem;line-height:1.75;word-spacing:.1em;letter-spacing:0;font-style:italic;text-align:justify}
.quoteblock blockquote{margin:0;padding:0;border:0}
.quoteblock blockquote::before{content:"\201c";float:left;font-size:2.75em;font-weight:bold;line-height:.6em;margin-left:-.6em;color:#7a2518;text-shadow:0 1px 2px rgba(0,0,0,.1)}
.quoteblock blockquote>.paragraph:last-child p{margin-bottom:0}
.quoteblock .attribution{margin-top:.75em;margin-right:.5ex;text-align:right}
.verseblock{margin:0 1em 1.25em}
.verseblock pre{font-family:"Open Sans","DejaVu Sans",sans-serif;font-size:1.15rem;color:rgba(0,0,0,.85);font-weight:300;text-rendering:optimizeLegibility}
.verseblock pre strong{font-weight:400}
.verseblock .attribution{margin-top:1.25rem;margin-left:.5ex}
.quoteblock .attribution,.verseblock .attribution{font-size:.9375em;line-height:1.45;font-style:italic}
.quoteblock .attribution br,.verseblock .attribution br{display:none}
.quoteblock .attribution cite,.verseblock .attribution cite{display:block;letter-spacing:-.025em;color:rgba(0,0,0,.6)}
.quoteblock.abstract blockquote::before,.quoteblock.excerpt blockquote::before,.quoteblock .quoteblock blockquote::before{display:none}
.quoteblock.abstract blockquote,.quoteblock.abstract p,.quoteblock.excerpt blockquote,.quoteblock.excerpt p,.quoteblock .quoteblock blockquote,.quoteblock .quoteblock p{line-height:1.6;word-spacing:0}
.quoteblock.abstract{margin:0 1em 1.25em;display:block}
.quoteblock.abstract>.title{margin:0 0 .375em;font-size:1.15em;text-align:center}
.quoteblock.excerpt>blockquote,.quoteblock .quoteblock{padding:0 0 .25em 1em;border-left:.25em solid #dddddf}
.quoteblock.excerpt,.quoteblock .quoteblock{margin-left:0}
.quoteblock.excerpt blockquote,.quoteblock.excerpt p,.quoteblock .quoteblock blockquote,.quoteblock .quoteblock p{color:inherit;font-size:1.0625rem}
.quoteblock.excerpt .attribution,.quoteblock .quoteblock .attribution{color:inherit;font-size:.85rem;text-align:left;margin-right:0}
p.tableblock:last-child{margin-bottom:0}
td.tableblock>.content{margin-bottom:1.25em;word-wrap:anywhere}
td.tableblock>.content>:last-child{margin-bottom:-1.25em}
table.tableblock,th.tableblock,td.tableblock{border:0 solid #dedede}
table.grid-all>*>tr>*{border-width:1px}
table.grid-cols>*>tr>*{border-width:0 1px}
table.grid-rows>*>tr>*{border-width:1px 0}
table.frame-all{border-width:1px}
table.frame-ends{border-width:1px 0}
table.frame-sides{border-width:0 1px}
table.frame-none>colgroup+*>:first-child>*,table.frame-sides>colgroup+*>:first-child>*{border-top-width:0}
table.frame-none>:last-child>:last-child>*,table.frame-sides>:last-child>:last-child>*{border-bottom-width:0}
table.frame-none>*>tr>:first-child,table.frame-ends>*>tr>:first-child{border-left-width:0}
table.frame-none>*>tr>:last-child,table.frame-ends>*>tr>:last-child{border-right-width:0}
table.stripes-all>*>tr,table.stripes-odd>*>tr:nth-of-type(odd),table.stripes-even>*>tr:nth-of-type(even),table.stripes-hover>*>tr:hover{background:#f8f8f7}
th.halign-left,td.halign-left{text-align:left}
th.halign-right,td.halign-right{text-align:right}
th.halign-center,td.halign-center{text-align:center}
th.valign-top,td.valign-top{vertical-align:top}
th.valign-bottom,td.valign-bottom{vertical-align:bottom}
th.valign-middle,td.valign-middle{vertical-align:middle}
table thead th,table tfoot th{font-weight:bold}
tbody tr th{background:#f7f8f7}
tbody tr th,tbody tr th p,tfoot tr th,tfoot tr th p{color:rgba(0,0,0,.8);font-weight:bold}
p.tableblock>code:only-child{background:none;padding:0}
p.tableblock{font-size:1em}
ol{margin-left:1.75em}
ul li ol{margin-left:1.5em}
dl dd{margin-left:1.125em}
dl dd:last-child,dl dd:last-child>:last-child{margin-bottom:0}
li p,ul dd,ol dd,.olist .olist,.ulist .ulist,.ulist .olist,.olist .ulist{margin-bottom:.625em}
ul.checklist,ul.none,ol.none,ul.no-bullet,ol.no-bullet,ol.unnumbered,ul.unstyled,ol.unstyled{list-style-type:none}
ul.no-bullet,ol.no-bullet,ol.unnumbered{margin-left:.625em}
ul.unstyled,ol.unstyled{margin-left:0}
li>p:empty:only-child::before{content:"";display:inline-block}
ul.checklist>li>p:first-child{margin-left:-1em}
ul.checklist>li>p:first-child>.fa-square-o:first-child,ul.checklist>li>p:first-child>.fa-check-square-o:first-child{width:1.25em;font-size:.8em;position:relative;bottom:.125em}
ul.checklist>li>p:first-child>input[type=checkbox]:first-child{margin-right:.25em}
ul.inline{display:flex;flex-flow:row wrap;list-style:none;margin:0 0 .625em -1.25em}
ul.inline>li{margin-left:1.25em}
.unstyled dl dt{font-weight:400;font-style:normal}
ol.arabic{list-style-type:decimal}
ol.decimal{list-style-type:decimal-leading-zero}
ol.loweralpha{list-style-type:lower-alpha}
ol.upperalpha{list-style-type:upper-alpha}
ol.lowerroman{list-style-type:lower-roman}
ol.upperroman{list-style-type:upper-roman}
ol.lowergreek{list-style-type:lower-greek}
.hdlist>table,.colist>table{border:0;background:none}
.hdlist>table>tbody>tr,.colist>table>tbody>tr{background:none}
td.hdlist1,td.hdlist2{vertical-align:top;padding:0 .625em}
td.hdlist1{font-weight:bold;padding-bottom:1.25em}
td.hdlist2{word-wrap:anywhere}
.literalblock+.colist,.listingblock+.colist{margin-top:-.5em}
.colist td:not([class]):first-child{padding:.4em .75em 0;line-height:1;vertical-align:top}
.colist td:not([class]):first-child img{max-width:none}
.colist td:not([class]):last-child{padding:.25em 0}
.thumb,.th{line-height:0;display:inline-block;border:4px solid #fff;box-shadow:0 0 0 1px #ddd}
.imageblock.left{margin:.25em .625em 1.25em 0}
.imageblock.right{margin:.25em 0 1.25em .625em}
.imageblock>.title{margin-bottom:0}
.imageblock.thumb,.imageblock.th{border-width:6px}
.imageblock.thumb>.title,.imageblock.th>.title{padding:0 .125em}
.image.left,.image.right{margin-top:.25em;margin-bottom:.25em;display:inline-block;line-height:0}
.image.left{margin-right:.625em}
.image.right{margin-left:.625em}
a.image{text-decoration:none;display:inline-block}
a.image object{pointer-events:none}
sup.footnote,sup.footnoteref{font-size:.875em;position:static;vertical-align:super}
sup.footnote a,sup.footnoteref a{text-decoration:none}
sup.footnote a:active,sup.footnoteref a:active{text-decoration:underline}
#footnotes{padding-top:.75em;padding-bottom:.75em;margin-bottom:.625em}
#footnotes hr{width:20%;min-width:6.25em;margin:-.25em 0 .75em;border-width:1px 0 0}
#footnotes .footnote{padding:0 .375em 0 .225em;line-height:1.3334;font-size:.875em;margin-left:1.2em;margin-bottom:.2em}
#footnotes .footnote a:first-of-type{font-weight:bold;text-decoration:none;margin-left:-1.05em}
#footnotes .footnote:last-of-type{margin-bottom:0}
#content #footnotes{margin-top:-.625em;margin-bottom:0;padding:.75em 0}
div.unbreakable{page-break-inside:avoid}
.big{font-size:larger}
.small{font-size:smaller}
.underline{text-decoration:underline}
.overline{text-decoration:overline}
.line-through{text-decoration:line-through}
.aqua{color:#00bfbf}
.aqua-background{background:#00fafa}
.black{color:#000}
.black-background{background:#000}
.blue{color:#0000bf}
.blue-background{background:#0000fa}
.fuchsia{color:#bf00bf}
.fuchsia-background{background:#fa00fa}
.gray{color:#606060}
.gray-background{background:#7d7d7d}
.green{color:#006000}
.green-background{background:#007d00}
.lime{color:#00bf00}
.lime-background{background:#00fa00}
.maroon{color:#600000}
.maroon-background{background:#7d0000}
.navy{color:#000060}
.navy-background{background:#00007d}
.olive{color:#606000}
.olive-background{background:#7d7d00}
.purple{color:#600060}
.purple-background{background:#7d007d}
.red{color:#bf0000}
.red-background{background:#fa0000}
.silver{color:#909090}
.silver-background{background:#bcbcbc}
.teal{color:#006060}
.teal-background{background:#007d7d}
.white{color:#bfbfbf}
.white-background{background:#fafafa}
.yellow{color:#bfbf00}
.yellow-background{background:#fafa00}
span.icon>.fa{cursor:default}
a span.icon>.fa{cursor:inherit}
.admonitionblock td.icon [class^="fa icon-"]{font-size:2.5em;text-shadow:1px 1px 2px rgba(0,0,0,.5);cursor:default}
.admonitionblock td.icon .icon-note::before{content:"\f05a";color:#19407c}
.admonitionblock td.icon .icon-tip::before{content:"\f0eb";text-shadow:1px 1px 2px rgba(155,155,0,.8);color:#111}
.admonitionblock td.icon .icon-warning::before{content:"\f071";color:#bf6900}
.admonitionblock td.icon .icon-caution::before{content:"\f06d";color:#bf3400}
.admonitionblock td.icon .icon-important::before{content:"\f06a";color:#bf0000}
.conum[data-value]{display:inline-block;color:#fff!important;background:rgba(0,0,0,.8);border-radius:50%;text-align:center;font-size:.75em;width:1.67em;height:1.67em;line-height:1.67em;font-family:"Open Sans","DejaVu Sans",sans-serif;font-style:normal;font-weight:bold}
.conum[data-value] *{color:#fff!important}
.conum[data-value]+b{display:none}
.conum[data-value]::after{content:attr(data-value)}
pre .conum[data-value]{position:relative;top:-.125em}
b.conum *{color:inherit!important}
.conum:not([data-value]):empty{display:none}
dt,th.tableblock,td.content,div.footnote{text-rendering:optimizeLegibility}
h1,h2,p,td.content,span.alt,summary{letter-spacing:-.01em}
p strong,td.content strong,div.footnote strong{letter-spacing:-.005em}
p,blockquote,dt,td.content,td.hdlist1,span.alt,summary{font-size:1.0625rem}
p{margin-bottom:1.25rem}
.sidebarblock p,.sidebarblock dt,.sidebarblock td.content,p.tableblock{font-size:1em}
.exampleblock>.content{background:#fffef7;border-color:#e0e0dc;box-shadow:0 1px 4px #e0e0dc}
.print-only{display:none!important}
@page{margin:1.25cm .75cm}
@media print{*{box-shadow:none!important;text-shadow:none!important}
html{font-size:80%}
a{color:inherit!important;text-decoration:underline!important}
a.bare,a[href^="#"],a[href^="mailto:"]{text-decoration:none!important}
a[href^="http:"]:not(.bare)::after,a[href^="https:"]:not(.bare)::after{content:"(" attr(href) ")";display:inline-block;font-size:.875em;padding-left:.25em}
abbr[title]{border-bottom:1px dotted}
abbr[title]::after{content:" (" attr(title) ")"}
pre,blockquote,tr,img,object,svg{page-break-inside:avoid}
thead{display:table-header-group}
svg{max-width:100%}
p,blockquote,dt,td.content{font-size:1em;orphans:3;widows:3}
h2,h3,#toctitle,.sidebarblock>.content>.title{page-break-after:avoid}
#header,#content,#footnotes,#footer{max-width:none}
#toc,.sidebarblock,.exampleblock>.content{background:none!important}
#toc{border-bottom:1px solid #dddddf!important;padding-bottom:0!important}
body.book #header{text-align:center}
body.book #header>h1:first-child{border:0!important;margin:2.5em 0 1em}
body.book #header .details{border:0!important;display:block;padding:0!important}
body.book #header .details span:first-child{margin-left:0!important}
body.book #header .details br{display:block}
body.book #header .details br+span::before{content:none!important}
body.book #toc{border:0!important;text-align:left!important;padding:0!important;margin:0!important}
body.book #toc,body.book #preamble,body.book h1.sect0,body.book .sect1>h2{page-break-before:always}
.listingblock code[data-lang]::before{display:block}
#footer{padding:0 .9375em}
.hide-on-print{display:none!important}
.print-only{display:block!important}
.hide-for-print{display:none!important}
.show-for-print{display:inherit!important}}
@media amzn-kf8,print{#header>h1:first-child{margin-top:1.25rem}
.sect1{padding:0!important}
.sect1+.sect1{border:0}
#footer{background:none}
#footer-text{color:rgba(0,0,0,.6);font-size:.9em}}
@media amzn-kf8{#header,#content,#footnotes,#footer{padding:0}}
</style>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.3/styles/github.min.css">
</head>
<body class="book toc2 toc-left data-line-1">
<div id="header">
<h1>RK0: The Real-Time Kernel '0'</h1>
<div class="details">
<span id="author" class="author">System: v0.6.6-dev | Docbook: v250824-4 | www.kernel0.org</span><br>
</div>
<div id="toc" class="toc2">
<div id="toctitle">Table of Contents</div>
<ul class="sectlevel1">
<li><a href="#the_kernel_at_a_glance">1. THE KERNEL AT A GLANCE</a>
<ul class="sectlevel2">
<li><a href="#the_design_approach">1.1. The design approach</a>
<ul class="sectlevel3">
<li><a href="#architecture">1.1.1. Architecture</a></li>
<li><a href="#programming_with_rk0">1.1.2. Programming with RK0</a></li>
<li><a href="#suitable_applications">1.1.3. Suitable Applications</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#task_scheduler">2. Task Scheduler</a>
<ul class="sectlevel2">
<li><a href="#scheduler_data_structures">2.1. Scheduler Data Structures</a>
<ul class="sectlevel3">
<li><a href="#task_control_block">2.1.1. Task Control Block</a></li>
<li><a href="#task_queues">2.1.2. Task Queues</a></li>
<li><a href="#ready_queue_table">2.1.3. Ready Queue Table</a></li>
<li><a href="#waiting_queues">2.1.4. Waiting Queues</a></li>
<li><a href="#the_scheduling_algorithm">2.1.5. The scheduling algorithm</a></li>
</ul>
</li>
<li><a href="#system_tasks">2.2. System Tasks</a></li>
<li><a href="#handling_the_scheduler">2.3. Handling the scheduler</a></li>
</ul>
</li>
<li><a href="#timers_and_delays">3. Timers and Delays</a>
<ul class="sectlevel2">
<li><a href="#busy_wait_delay">3.1. Busy-wait delay</a></li>
<li><a href="#sleep_timers">3.2. Sleep Timers</a>
<ul class="sectlevel3">
<li><a href="#sleep_delay">3.2.1. Sleep Delay</a></li>
<li><a href="#sleeptimers">3.2.2. Periodic Sleep</a></li>
</ul>
</li>
<li><a href="#blocking_time_out">3.3. Blocking Time-out</a></li>
<li><a href="#callout_timers_application_timers">3.4. Callout Timers (Application Timers)</a></li>
<li><a href="#system_tick">3.5. System Tick</a></li>
</ul>
</li>
<li><a href="#memory_allocator">4. Memory Allocator</a>
<ul class="sectlevel2">
<li><a href="#how_it_works">4.1. How it works</a></li>
</ul>
</li>
<li><a href="#inter_task_communication">5. <strong><em>Inter-Task Communication</em></strong></a>
<ul class="sectlevel2">
<li><a href="#directsignals">5.1. Direct Signals (Task Notification)</a>
<ul class="sectlevel4">
<li><a href="#supervisortask">5.1.1. Usage Example: Supervisor Task</a></li>
</ul>
</li>
<li><a href="#semaphores">5.2. Semaphores</a>
<ul class="sectlevel3">
<li><a href="#counting_and_binary_semaphores">5.2.1. Counting and Binary Semaphores</a>
<ul class="sectlevel4">
<li><a href="#bilateralsynch">5.2.1.1. Usage Example: Task-to-Task Bilateral Synchronisation</a></li>
</ul>
</li>
<li><a href="#mutex_semaphores_locks">5.2.2. Mutex Semaphores (Locks)</a>
<ul class="sectlevel4">
<li><a href="#boundedbuffer">5.2.2.1. Usage Example: Bounded Buffer</a></li>
<li><a href="#priority_inversion_and_the_priority_inheritance_protocol">5.2.2.2. Priority Inversion and the Priority Inheritance Protocol</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#scheduler_lock">5.3. Scheduler Lock</a></li>
<li><a href="#sleep_queues">5.4. Sleep Queues</a></li>
<li><a href="#monitors">5.5. Monitors</a>
<ul class="sectlevel4">
<li><a href="#monitor_semantics">5.5.1. Monitor Semantics</a></li>
<li><a href="#monitor_like_patterns_in_rk0">5.5.2. Monitor-like patterns in RK0</a>
<ul class="sectlevel4">
<li><a href="#producer_consumer_solution_with_a_mesa_monitor">5.5.2.1. Producer-Consumer Solution with a Mesa Monitor</a></li>
</ul>
</li>
<li><a href="#condition_variables_in_rk0">5.5.3. Condition Variables in RK0</a>
<ul class="sectlevel4">
<li><a href="#usage_example_synchronisation_barrier">5.5.3.1. Usage Example: Synchronisation Barrier</a></li>
<li><a href="#readerswriterslock">5.5.3.2. Usage Example: Readers Writers Lock</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#mailbox">5.6. Mailbox</a>
<ul class="sectlevel3">
<li><a href="#sticky_mailbox">5.6.1. 'Sticky' Mailbox</a></li>
<li><a href="#example_multi_client_server_synchronous_command_response">5.6.2. Example: Multi-client-server synchronous command-response</a></li>
</ul>
</li>
<li><a href="#message_queues">5.7. Message Queues</a>
<ul class="sectlevel3">
<li><a href="#mail_queue">5.7.1. Mail Queue</a>
<ul class="sectlevel4">
<li><a href="#usage_example_work_queue">5.7.1.1. Usage Example: Work Queue</a></li>
<li><a href="#a_logger_pattern">5.7.1.2. A Logger Pattern</a></li>
</ul>
</li>
<li><a href="#stream_queue">5.7.2. Stream Queue</a>
<ul class="sectlevel4">
<li><a href="#stream_message_size">5.7.2.1. Stream Message-Size</a></li>
<li><a href="#usage_example_averaging_sensor_values">5.7.2.2. Usage Example: Averaging Sensor Values</a></li>
</ul>
</li>
<li><a href="#summing_up_stream_queues_vs_mail_queues">5.7.3. Summing Up: Stream Queues vs Mail Queues</a></li>
<li><a href="#installing_notify_callbacks">5.7.4. Installing Notify Callbacks</a>
<ul class="sectlevel4">
<li><a href="#usage_example_queue_select">5.7.4.1. Usage Example: Queue Select</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#message_passing_ownership">5.8. Message Passing ownership</a>
<ul class="sectlevel3">
<li><a href="#priority_inversion_on_message_passing">5.8.1. Priority Inversion on Message Passing</a></li>
</ul>
</li>
<li><a href="#mrm">5.9. Most-Recent Message Protocol (MRM)</a>
<ul class="sectlevel3">
<li><a href="#functional_description">5.9.1. Functional Description</a></li>
<li><a href="#mrm_control_block_configuration">5.9.2. MRM Control Block Configuration</a></li>
<li><a href="#usage_example">5.9.3. Usage Example</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#error_handling">6. <em><strong>Error Handling</strong></em></a>
<ul class="sectlevel2">
<li><a href="#fail_fast">6.1. Fail fast</a></li>
<li><a href="#stack_overflow">6.2. Stack Overflow</a></li>
<li><a href="#deadlocks">6.3. Deadlocks</a></li>
</ul>
</li>
<li><a href="#rk0_services_api">7. <em><strong>RK0 Services API</strong></em></a></li>
<li><a href="#scheduler_determinism">8. Scheduler Determinism</a>
<ul class="sectlevel2">
<li><a href="#preemptive_scheduling">8.1. Preemptive Scheduling</a>
<ul class="sectlevel4">
<li><a href="#using_direct_signals">8.1.1. Using Direct Signals</a></li>
<li><a href="#using_semaphores">8.1.2. Using Semaphores</a></li>
<li><a href="#using_mailboxes">8.1.3. Using Mailboxes</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#influences">9. Influences</a></li>
</ul>
</div>
</div>
<div id="content">
<div id="preamble">
<div class="sectionbody">
<!-- toc disabled -->
</div>
</div>
<div class="sect1 data-line-14">
<h2 id="the_kernel_at_a_glance">1. THE KERNEL AT A GLANCE</h2>
<div class="sectionbody">
<div class="sect2 data-line-17">
<h3 id="the_design_approach">1.1. The design approach</h3>
<div class="admonitionblock tip data-line-21">
<table>
<tr>
<td class="icon">
<i class="fa icon-tip" title="Tip"></i>
</td>
<td class="content">
<div class="paragraph data-line-22">
<p><em>RK0 is the result of independent design, a lot of research plus (a whole other lot of) learning by doing.</em>
While I may not have seen every edge case, building it from scratch has given me a unique perspective. I invite peers to review and challenge what’s been done so far.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph data-line-26">
<p><em>In the embedded realm, probably because we lack a better abstraction, we use multithreading to fine-tune our load balance and responsiveness to achieve real-time.</em></p>
</div>
<div class="exampleblock data-line-29">
<div class="content">
<div class="paragraph data-line-30">
<p><em>RK0 Blog</em>: <a href="https://kernel0.org/2025/05/16/about-processes-tasks-and-threads/">About Processes, Tasks and Threads</a></p>
</div>
</div>
</div>
<div class="paragraph data-line-33">
<p>This is an arrangement: instead of having a single super-loop, we have many, each running on its execution stack.</p>
</div>
<div class="paragraph data-line-35">
<p>This arrangement yields an operating system entity to handle—a (logical) <em>Concurrency Unit</em>: in K0, we name it a <em>Task</em> (in RK0  <em><strong>a task is a thread</strong></em>.).</p>
</div>
<div class="sect3 data-line-37">
<h4 id="architecture">1.1.1. Architecture</h4>
<div class="paragraph data-line-39">
<p>If no more details are to be provided, the kernel has a top and a bottom layer. On top of that, the executive manages the resources needed by the application. On the bottom, the Low-level Scheduler works as a software extension of the CPU. Together, they implement the Task abstraction — the Concurrency Unit that enables a multitasking environment.</p>
</div>
<div class="imageblock data-line-42">
<div class="content">
<img src="images/images/layeredkernel.png" alt="layeredkernel" width="50%">
</div>
</div>
<div class="paragraph data-line-45">
<p>In systems design jargon, the Executive enforces policy (what should happen). The Low-level Scheduler provides the mechanism (how it gets done). The services are the primitives that gradually translate policy decisions into concrete actions executed by the Scheduler.
K0&#8217;s goal is determinism on low-end devices.</p>
</div>
<div class="paragraph data-line-48">
<p>Its multitasking engine operates without mimics of <em>userland</em>: tasks run in privileged mode on a different stack pointer from the system stack. This trades the already limited 'isolation' its target architecture can provide, for tight and predictable control.</p>
</div>
</div>
<div class="sect3 data-line-52">
<h4 id="programming_with_rk0">1.1.2. Programming with RK0</h4>
<div class="paragraph data-line-54">
<p>As it may become clear throughout the document, you will notice <em>RK0</em> is built so it does not get in the programmer&#8217;s way. Its meant to be transparent, composable, deterministic and with clear semantics.</p>
</div>
</div>
<div class="sect3 data-line-56">
<h4 id="suitable_applications">1.1.3. Suitable Applications</h4>
<div class="paragraph data-line-59">
<p>Given the architecture, <em>RK0</em> targets applications with the following characteristics:</p>
</div>
<div class="olist arabic data-line-61">
<ol class="arabic">
<li class="data-line-61">
<p>They are designed to handle particular devices in which real-time responsiveness is imperative.</p>
</li>
<li class="data-line-62">
<p>Applications and middleware may be implemented alongside appropriate drivers.</p>
</li>
<li class="data-line-63">
<p>Drivers may even include the application itself.</p>
</li>
<li class="data-line-64">
<p><em>Untested programs are not loaded</em>: After the software has been tested, it can be assumed reliable.</p>
</li>
</ol>
</div>
</div>
</div>
</div>
</div>
<div class="sect1 data-line-70">
<h2 id="task_scheduler">2. Task Scheduler</h2>
<div class="sectionbody">
<div class="paragraph data-line-72">
<p><em>RK0</em> employs a Rate Monotonic Scheduler. Tasks are assigned priorities according to their request rates - i.e., tasks with shorter periods are assigned to higher priorities. The highest priority is represented by the value '0'; the lowest is represented by the value '31'.</p>
</div>
<div class="paragraph data-line-74">
<p>A scheduler remark is its constant time complexity (<em>O(1)</em>) and low latency. This was achieved by carefully composing the data structures along with an efficient <em>'choose-next'</em> algorithm. This is detailed below.</p>
</div>
<div class="admonitionblock note data-line-77">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph data-line-78">
<p><em>Time-slice</em> was deprecated on version 0.5.0.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="sect2 data-line-81">
<h3 id="scheduler_data_structures">2.1. Scheduler Data Structures</h3>
<div class="sect3 data-line-83">
<h4 id="task_control_block">2.1.1. Task Control Block</h4>
<div class="paragraph data-line-85">
<p>Threads are represented as Tasks. Every task is associated with a Task Control Block structure, which is a record for stack, resources, and time management. The table below partially represents a Task Control Block (as this document is live, this might not reflect the exact fields of the current version).</p>
</div>
<table class="tableblock frame-all grid-all stretch data-line-88">
<colgroup>
<col style="width: 100%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Task Control Block</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Task name</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Saved Stack Pointer</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Stack Address</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Stack Size</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Status</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Assigned Priority</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Current Priority</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Self-Assigned ID</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Last wake-time</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Run-To-Completion Flag</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Time-out Flag</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">List of owned Mutexes</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Aggregated Timeout Node</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Aggregated Task List Node</p></td>
</tr>
</tbody>
</table>
<div class="paragraph data-line-106">
<p>Tasks are static - they cannot be created on runtime, to be destroyed, to fork or to join.</p>
</div>
<div class="paragraph data-line-108">
<p>In practice, tasks are either <em>RUNNING</em> or '<em>waiting</em>' for their turn to run.</p>
</div>
<div class="imageblock data-line-111">
<div class="content">
<img src="images/images/taskstates.png" alt="taskstates">
</div>
</div>
<div class="paragraph data-line-114">
<p>We need to define  <em>WAITING</em> and <em>READY</em> clearly:</p>
</div>
<div class="olist arabic data-line-116">
<ol class="arabic">
<li class="data-line-116">
<p>A <em>READY</em> task will be dispatched; therefore, switch to <em>RUNNING</em> whenever it is the highest priority <em>READY</em> task.</p>
</li>
<li class="data-line-118">
<p>A <em>WAITING</em> task depends on a condition, generalised as an <em>event</em> to switch to <em>READY</em>.</p>
</li>
<li class="data-line-120">
<p>Logically, the <em>WAITING</em> state will assume different pseudo-states related to the kind of event that will switch a task to <em>READY</em>:</p>
<div class="ulist data-line-122">
<ul>
<li class="data-line-122">
<p><em>SLEEPING</em>: a task suspends itself and goes to sleep for a given period or suspends itself until receiving a <em>wake</em> signal, representing an event.</p>
</li>
<li class="data-line-124">
<p><em>PENDING</em>: the task suspended itself, waiting for a combination of signal flags.</p>
</li>
<li class="data-line-126">
<p><em>BLOCKED</em>: A task is blocked on a mutex or semaphore.</p>
</li>
<li class="data-line-128">
<p><em>SENDING/RECEIVING</em>: A producer task, when blocking on a Message Passing object, switches its status to <em>SENDING</em>, and a consumer to <em>RECEIVING</em>.</p>
</li>
</ul>
</div>
</li>
</ol>
</div>
<div class="exampleblock data-line-130">
<div class="content">
<div class="paragraph data-line-131">
<p><strong><em>The scheduler rules, not the heap.</em></strong></p>
</div>
<div class="paragraph data-line-133">
<p><em>RK0</em> tasks are static.</p>
</div>
<div class="paragraph data-line-135">
<p>It’s a design decision rooted in real-time correctness.</p>
</div>
<div class="paragraph data-line-137">
<p>Besides an application-specific system software does not need to treat tasks as 'unknown' objects.</p>
</div>
<div class="paragraph data-line-140">
<p>The wins:</p>
</div>
<div class="ulist data-line-142">
<ul>
<li class="data-line-142">
<p>A memory layout the systems programmer knows.</p>
</li>
<li class="data-line-143">
<p>No alignment traps.</p>
</li>
<li class="data-line-144">
<p>Link-time visibility:</p>
<div class="ulist data-line-145">
<ul>
<li class="data-line-145">
<p>Each task’s stack is a named symbol in the linker map.</p>
</li>
<li class="data-line-146">
<p>You can inspect and verify the memory layout before flashing.</p>
</li>
<li class="data-line-147">
<p>A simple <code>objdump</code> reveals all stack allocations — that’s peace of mind.</p>
</li>
</ul>
</div>
</li>
</ul>
</div>
</div>
</div>
<div class="imageblock data-line-153">
<div class="content">
<img src="images/images/schdatastruct.png" alt="schdatastruct" width="85%">
</div>
</div>
</div>
<div class="sect3 data-line-155">
<h4 id="task_queues">2.1.2. Task Queues</h4>
<div class="paragraph data-line-156">
<p>The backbone of the queues where tasks will wait for their turn to run is a circular doubly linked list: removing any item from a double list takes O(1) (provided we don’t need to search the item). As the kernel knows each task’s address, adding and removing is always O(1). Singly linked lists can’t achieve O(1) for removal.</p>
</div>
</div>
<div class="sect3 data-line-158">
<h4 id="ready_queue_table">2.1.3. Ready Queue Table</h4>
<div class="paragraph data-line-160">
<p>Another design choice to achieve O(1) is the global ready queue, which is a table of FIFO queues—each queue dedicated to a priority—and not a single ordered queue. So, enqueuing a ready task is always O(1). Given the sorting needed, the time complexity would be O(n) if tasks were placed on a single ready queue.</p>
</div>
</div>
<div class="sect3 data-line-162">
<h4 id="waiting_queues">2.1.4. Waiting Queues</h4>
<div class="paragraph data-line-164">
<p>The scheduler does not have a unique waiting queue. Every kernel object that can block a task has an associated waiting queue. Because these queues are a scheduler component, <em>they follow a priority discipline</em>: the highest priority task is dequeued first, <em>always</em>.</p>
</div>
<div class="paragraph data-line-166">
<p>When an event capable of switching tasks from <em>WAITING</em> to <em>READY</em> happens, one or more tasks (depending on the mechanism) are then placed on the ready list, unique to their priority. Now, they are waiting to be picked by the scheduler—that is the definition of <em>READY</em>.</p>
</div>
</div>
<div class="sect3 data-line-168">
<h4 id="the_scheduling_algorithm">2.1.5. The scheduling algorithm</h4>
<div class="paragraph data-line-170">
<p>As the ready queue table is indexed by priority - the index 0 points to the queue of ready tasks with priority 0, and so forth, and there are 32 possible priorities - a 32-bit integer can represent the state of the ready queue table. It is a BITMAP:</p>
</div>
<div class="listingblock data-line-172">
<div class="content">
<pre class="highlightjs highlight"><code class="language-none hljs">The BITMAP computation: ((1a) OR (1b)) AND (2), s.t.:

(1a) Every Time a task is readied, update: BITMAP |= (1U &lt;&lt; task-&gt;priority );
(1b) Every Time an empty READY QUEUE becomes non-empty, update: BITMAP |= (1U &lt;&lt; queueIndex)
(2): Every Time READY QUEUE becomes empty, update: BITMAP &amp;= ~(1U &lt;&lt; queueIndex);
EXAMPLE:

  Ready Queue Index :     (6)5 4 3 2 1 0
          Not empty :      1 1 1 0 0 1 0
                           -------------&gt;
                 (LOW)  Effective Priority  (HIGH)
In this case, the scenario is a system with 7 priority task levels. Queues with priorities 6, 5, 4, and 1 are not empty.</code></pre>
</div>
</div>
<div class="paragraph data-line-189">
<p>In the RK0 source code, the following routines implement the bitmap update:</p>
</div>
<div class="listingblock data-line-191">
<div class="content">
<pre class="highlightjs highlight"><code class="language-C hljs" data-lang="C">/* Enqueue a TCB on on the tail of TCB list  */
RK_ERR kTCBQEnq(RK_TCBQ *const kobj, RK_TCB *const tcbPtr)
{
    RK_ERR err = kListAddTail(kobj, &amp;(tcbPtr-&gt;tcbNode));
    if (err == 0)
    {
        /* if a task was enqueued on a list within the ready queue table, update the 'ready bitmap' */
        if (kobj == &amp;readyQueue[tcbPtr-&gt;priority])
        {
            readyQBitMask |= (1 &lt;&lt; tcbPtr-&gt;priority);
        }
    }
    return (err);
}

/* Add a TCB on the head of the TCB list  */
RK_ERR kTCBQJam(RK_TCBQ *const kobj, RK_TCB *const tcbPtr)
{
	RK_ERR err = kListAddHead(kobj, &amp;(tcbPtr-&gt;tcbNode));
	if (err == 0)
	{
		if (kobj == &amp;readyQueue[tcbPtr-&gt;priority])
		{
			readyQBitMask |= (1 &lt;&lt; tcbPtr-&gt;priority);
		}
	}
	return (err);
}

/* Dequeue the head task from a list of TCBs */
RK_ERR kTCBQDeq(RK_TCBQ *const kobj, RK_TCB **const tcbPPtr)
{
    RK_NODE *dequeuedNodePtr = NULL;
    RK_ERR err = kListRemoveHead(kobj, &amp;dequeuedNodePtr);
    if (err != RK_SUCCESS)
    {
        return (err);
    }
    *tcbPPtr = K_GET_TCB_ADDR(dequeuedNodePtr);

    RK_TCB *tcbPtr_ = *tcbPPtr;
    RK_PRIO prio_ = tcbPtr_-&gt;priority;

    /* if the list is in the ready queue table and is now empty
     update 'ready bitmap' */
    if ((kobj == &amp;readyQueue[prio_]) &amp;&amp; (kobj-&gt;size == 0))
    {
        readyQBitMask &amp;= ~(1U &lt;&lt; prio_);
    }
    return (err);
}

/* Remove a specific TCB from a TCB List */
RK_ERR kTCBQRem(RK_TCBQ *const kobj, RK_TCB **const tcbPPtr)
{
    RK_NODE *dequeuedNodePtr = &amp;((*tcbPPtr)-&gt;tcbNode);
    RK_ERR err = kListRemove(kobj, dequeuedNodePtr);
    if (err != RK_SUCCESS)
    {
        return (err);
    }
    *tcbPPtr = K_GET_TCB_ADDR(dequeuedNodePtr);

    RK_TCB *tcbPtr_ = *tcbPPtr;
    RK_PRIO prio_ = tcbPtr_-&gt;priority;
    if ((kobj == &amp;readyQueue[prio_]) &amp;&amp; (kobj-&gt;size == 0))
    {
          readyQBitMask &amp;= ~(1U &lt;&lt; prio_);
    }
    return (err);
}</code></pre>
</div>
</div>
<div class="paragraph data-line-267">
<p>Having the Ready Queue Table bitmap, we find the highest priority non-empty task list as follows:</p>
</div>
<div class="paragraph data-line-269">
<p>(1) Isolate the <strong>rightmost</strong> '1':</p>
</div>
<div class="listingblock data-line-271">
<div class="content">
<pre class="highlightjs highlight"><code class="language-none hljs">RBITMAP = BITMAP &amp; -BITMAP. (- is the bitwise operator for two's complement: ~BITMAP + 1) `</code></pre>
</div>
</div>
<div class="paragraph data-line-275">
<p>In this case:</p>
</div>
<div class="listingblock data-line-277">
<div class="content">
<pre class="highlightjs highlight"><code class="language-none hljs">                           [31]       [0]  :  Bit Position
                             0...1110010   :  BITMAP
                             1...0001110   : -BITMAP
                            =============
                             0...0000010   :  RBITMAP
                                     [1]</code></pre>
</div>
</div>
<div class="paragraph data-line-287">
<p><em>The rationale here is that, for a number N, its 2’s complement -N, flips all bits - except the rightmost '1' (by adding '1') . Then, N &amp; -N results in a word with all 0-bits except for the less significant '1'.</em></p>
</div>
<div class="paragraph data-line-289">
<p>(2) Extract the <strong>rightmost '1' <em>position</em></strong>:</p>
</div>
<div class="ulist data-line-291">
<ul>
<li class="data-line-291">
<p>For ARMv7M, we benefit from the <code>CLZ</code> instruction to count the <em>leading zeroes</em>. As they are the number of zeroes on the left of the rightmost bit, '1', this value is subtracted from 31 to find the Ready Queue index.</p>
</li>
</ul>
</div>
<div class="listingblock data-line-293">
<div class="content">
<pre class="highlightjs highlight"><code class="language-C hljs" data-lang="C">__RK_INLINE static inline
unsigned __getReadyPrio(unsigned readyQBitmap)
{
    unsigned ret;
    __ASM volatile (
        "clz    %0, %1     \n"
        "neg    %0, %0     \n"
        "add    %0, %0, #31\n"
        : "=&amp;r" (ret)
        : "r" (readyQBitmap)
        :
    );
    return (ret);
}</code></pre>
</div>
</div>
<div class="paragraph data-line-310">
<p>This instruction would return #30, and #31 - #30 = #01 in the example above.</p>
</div>
<div class="ulist data-line-313">
<ul>
<li class="data-line-313">
<p>For ARMv6M there is no suitable hardware instruction. The algorithm is written in C and counts the <em>trailing zeroes</em>, thus, the index number. Although it might vary depending on your compiler settings, it takes ~11 cycles (<em>note it is still O(1)</em>):</p>
</li>
</ul>
</div>
<div class="listingblock data-line-316">
<div class="content">
<pre class="highlightjs highlight"><code class="language-C hljs" data-lang="C">/*
  De Brujin's multiply+LUT
  (Hacker's Delight book)
*/

/* table is on a ram section  for efficiency */
__K_SECTION(getReadyTable)
const static unsigned table[32] =
{
 0, 1, 28, 2, 29, 14, 24, 3, 30, 22, 20, 15, 25, 17, 4, 8,
 31, 27, 13, 23, 21, 19, 16, 7, 26, 12, 18, 6, 11, 5, 10, 9
};

__RK_INLINE static inline
unsigned __getReadyPrio(unsigned readyQBitmap)
{
    unsigned mult = readyQBitmap * 0x077CB531U;

    /* Shift right the top 5 bits
     */
    unsigned idx = (mult &gt;&gt; 27);

    /* LUT */
    unsigned ret = (unsigned)table[idx];
    return (ret);
}</code></pre>
</div>
</div>
<div class="paragraph data-line-345">
<p>For the example above, <code>mult = 0x2 * 0x077CB531 = 0x0EF96A62</code>. The 5 leftmost bits (the index) are <code>00001</code> &#8594; <code>table[1] = 1</code>.</p>
</div>
<div class="paragraph data-line-347">
<p>During a context switch, the procedures to find the highest priority non-empty ready queue table index are as follows:</p>
</div>
<div class="listingblock data-line-349">
<div class="content">
<pre class="highlightjs highlight"><code class="language-C hljs" data-lang="C">static inline RK_PRIO kCalcNextTaskPrio_(VOID)
{
    if (readyQBitMask == 0U)
    {
        return (idleTaskPrio);
    }
    readyQRightMask = readyQBitMask &amp; -readyQBitMask;
    RK_PRIO prioVal = (RK_PRIO) (__getReadyPrio(readyQRightMask));
    return (prioVal);
}

VOID kSchSwtch(VOID)
{
    /* O(1) complexity */
	nextTaskPrio = kCalcNextTaskPrio_();

	RK_TCB* nextRunPtr = NULL;

	/* O(1) complexity */
	kTCBQDeq(&amp;readyQueue[nextTaskPrio], &amp;nextRunPtr);

	runPtr = nextRunPtr;

}</code></pre>
</div>
</div>
</div>
</div>
<div class="sect2 data-line-378">
<h3 id="system_tasks">2.2. System Tasks</h3>
<div class="paragraph data-line-380">
<p>There are two <em>System Tasks</em>: the <em>Idle Task</em> and the <em>Timer Handler Task</em>.</p>
</div>
<div class="paragraph data-line-382">
<p>The <em>Idle Task</em> runs whenever there is no other ready task to be dispatched. The CPU enters on <em>low-power</em>. The kernel assigns the <em>Idle Task</em> priority during initialisation, taking into account all priorities the user has defined. Unless user tasks occupy all 32 priorities, the Idle Task is treated as an ordinary lowest priority and has a position in the ready queue table. Otherwise, it is selected if <em>Ready Queue Bitmap</em> is <code>0x00000000</code>.</p>
</div>
<div class="paragraph data-line-384">
<p>The <em>Post Processing Task</em>, handles application demands that some services choose to defer. Currently they will dispatch Application Timer Callbacks.</p>
</div>
<div class="paragraph data-line-386">
<p>Nominally its priority is <code>0</code>, but on practice it could be considered as having priority <code>-1</code>, because it always takes precedence over other tasks with priority <code>0</code>.</p>
</div>
</div>
<div class="sect2 data-line-390">
<h3 id="handling_the_scheduler">2.3. Handling the scheduler</h3>
<div class="exampleblock data-line-392">
<div class="content">
<div class="paragraph data-line-393">
<p><em><strong>RK0 Blog:</strong></em>
<a href="https://kernel0.org/2025/06/08/about-real-time-tasks-responsiveness-x-throughput/">About Real-Time, Responsiveness and Throughput</a></p>
</div>
</div>
</div>
<div class="paragraph data-line-398">
<p>An essential characteristic of the scheduler is that it is a <em>preemptive run-to-completion</em> scheduler. This term, '<em>run-to-completion</em>' has slightly different meanings depending on the context. It is often related to strictly cooperative schedulers, in the sense
tasks must <em>yield</em> the processor. Otherwise, they monopolise the CPU.</p>
</div>
<div class="paragraph data-line-401">
<p>In <em>RK0</em>, tasks with the same priority will work cooperatively. This is different from schedulers that employ a <em>time-slice</em> or a <em>quantum</em> for round-robin: after this time expires, task is put at the <em>tail</em> of the <em>Ready Queue</em>.</p>
</div>
<div class="paragraph data-line-403">
<p>The term <em>run-to-completion</em> here is to be interpreted as follows:</p>
</div>
<div class="ulist data-line-405">
<ul>
<li class="data-line-405">
<p>The scheduler&#8217;s behaviour is to choose the highest priority READY task to run. Always.</p>
</li>
<li class="data-line-406">
<p>The scheduler works on a First-In-First-Out discipline for tasks with the same priority.</p>
</li>
<li class="data-line-407">
<p>A task must switch to the <em>READY</em> state before being eligible for scheduling.</p>
</li>
<li class="data-line-408">
<p>A task will switch from <em>RUNNING</em> to <em>READY</em> if yielding or if being preempted by a higher priority task. Otherwise it can only go to a <em>WAITING</em> state, and eventually switch back to <em>READY</em>.</p>
</li>
<li class="data-line-409">
<p>When a task is preempted by a higher priority task, it switches from <em>RUNNING</em> to <em>READY</em> and is placed back on the <em>head</em> position of its Ready Queue. This means that it will be resumed as soon as it is the highest priority ready task again.</p>
</li>
<li class="data-line-410">
<p>On the contrary, if a task <em>yields</em>, it tells the scheduler it has completed its cycle. Then, it will be enqueued on the ready queue tail - the last queue position.</p>
</li>
<li class="data-line-411">
<p>When a task <em>waits</em> it is suspended until a condition is satisfied.</p>
</li>
<li class="data-line-412">
<p>When the condition is satisfied, it switches from <em>WAITING</em> to <em>READY</em>, and is enqueued on the tail.</p>
</li>
<li class="data-line-413">
<p>So, tasks with the same priority cooperate by either <em>yielding</em> or <em>waiting</em>.</p>
</li>
<li class="data-line-414">
<p>If a task never yields or waits, other tasks with the same or lower priority <em>will starve</em>.</p>
</li>
<li class="data-line-415">
<p>Finally, Tasks with the same priority are <em>initially</em> placed on the <em>Ready Queue</em> associated with that priority in the order they are <em>created</em>.</p>
</li>
</ul>
</div>
<div class="admonitionblock note data-line-418">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph data-line-419">
<p><em>RK0</em> can handle context-switching with an extended frame when a float-point co-processor is available. This must be informed when compiling by defining the symbol <code>__FPU_PRESENT=1</code>.</p>
</div>
</td>
</tr>
</table>
</div>
</div>
</div>
</div>
<div class="sect1 data-line-422">
<h2 id="timers_and_delays">3. Timers and Delays</h2>
<div class="sectionbody">
<div class="sect2 data-line-424">
<h3 id="busy_wait_delay">3.1. Busy-wait delay</h3>
<div class="paragraph data-line-426">
<p>A busy-wait delay <code>kBusyWait(t)</code> keeps a task spinning for <code>t</code> ticks. That is, the task does nothing but does not suspend or yield (but can be preempted). This service finds its use when simulating workloads.</p>
</div>
<div class="admonitionblock tip data-line-429">
<table>
<tr>
<td class="icon">
<i class="fa icon-tip" title="Tip"></i>
</td>
<td class="content">
<div class="paragraph data-line-431">
<p>Context switching is probably the most significant overhead on a kernel. The time spent on the System Tick handler contributes to much of this overhead.</p>
</div>
<div class="paragraph data-line-433">
<p>Design Choice:</p>
</div>
<div class="ulist data-line-435">
<ul>
<li class="data-line-435">
<p>Timers are kept on a single list; only the head element needs to be updated using a delta-queue approach.</p>
</li>
<li class="data-line-437">
<p>Application Timers that trigger callbacks are run on a deferred, non-preemptible system task.</p>
</li>
</ul>
</div>
<div class="paragraph data-line-439">
<p>Benefits:</p>
</div>
<div class="ulist data-line-441">
<ul>
<li class="data-line-441">
<p>Keep the overhead of updating timers as minimal as possible with the delta queue;</p>
</li>
<li class="data-line-443">
<p>Deferring the Application Timer to a high-priority, non-preemptible system task meet the requested callback period while keeping the ability to track system ticks.</p>
</li>
</ul>
</div>
</td>
</tr>
</table>
</div>
<table class="tableblock frame-all grid-all stretch data-line-448">
<colgroup>
<col style="width: 100%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Timeout Node</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Timeout Type</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Absolute Interval (Ticks)</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Relative Interval (Ticks)</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Waiting Queue Address</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Next Timeout Node</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Previous Timeout Node</p></td>
</tr>
</tbody>
</table>
<div class="paragraph data-line-458">
<p>Every task is prone to events triggered by timers described in this section. Every Task Control Block has a node to <em>a timeout list</em>.</p>
</div>
<div class="paragraph data-line-460">
<p>This list is doubly linked, ordered as a delta list. For instance, three timers (T1,8), (T2,6) and (T3,10) will be ordered as a sequence &lt;(T2,6), (T1,2), (T3,2)&gt; - so it counts &lt;6, (6)+2, ((6)+2)+2&gt;.</p>
</div>
<div class="paragraph data-line-462">
<p>Thus, for every system tick, only the head element on the list needs to be decreased - yielding O(1)  - another design choice towards deterministic behaviour.</p>
</div>
</div>
<div class="sect2 data-line-464">
<h3 id="sleep_timers">3.2. Sleep Timers</h3>
<div class="paragraph data-line-466">
<p>A task can be suspended by an amount of time in ticks, in two distinct manners:</p>
</div>
<div class="sect3 data-line-468">
<h4 id="sleep_delay">3.2.1. Sleep Delay</h4>
<div class="paragraph data-line-469">
<p>The task sleeps for the exact number of <code>t</code> ticks on every call. Time elapsed between calls is not considered.</p>
</div>
<div class="paragraph data-line-471">
<p><em>Example</em>:</p>
</div>
<div class="listingblock data-line-472">
<div class="content">
<pre class="highlightjs highlight"><code class="language-C hljs" data-lang="C">VOID Task1(VOID* args)
{
    RK_UNUSEARGS
    UINT count = 0;
    while (1)
    {

        logPost("Task1: sleep");
        kSleep(300);
        /* wake here */
        count += 1U;
        if (count &gt;= 5)
        {
            kBusyWait(25); /* spin */
            count=0;
            /* every 5 activations there will be a drift */
        }
    }
}</code></pre>
</div>
</div>
<div class="paragraph data-line-493">
<p>Output:</p>
</div>
<div class="listingblock data-line-494">
<div class="content">
<pre class="highlightjs highlight"><code class="language-none hljs">0 ms :: Task1: sleep
300 ms :: Task1: sleep  &lt;-- +300
600 ms :: Task1: sleep  &lt;-- +300
900 ms :: Task1: sleep  &lt;-- +300
1200 ms :: Task1: sleep &lt;-- +300
1525 ms :: Task1: sleep &lt;-- +325
1825 ms :: Task1: sleep &lt;-- +300
2125 ms :: Task1: sleep &lt;-- +300
2425 ms :: Task1: sleep
2725 ms :: Task1: sleep
3050 ms :: Task1: sleep
3350 ms :: Task1: sleep
3650 ms :: Task1: sleep
3950 ms :: Task1: sleep
4250 ms :: Task1: sleep
4575 ms :: Task1: sleep</code></pre>
</div>
</div>
</div>
<div class="sect3 data-line-513">
<h4 id="sleeptimers">3.2.2. Periodic Sleep</h4>
<div class="paragraph data-line-514">
<p>This primitive is intended to create <em>periodic activations</em>. The period <code>P</code> ticks is defined at the first kernel call <code>sleepperiod(P)</code>, and adjusted internally on subsequent activations, as follows:</p>
</div>
<div class="paragraph data-line-516">
<p>Say a task is expected to return from its <code>k<sub>eth</sub></code> sleep at <code>T<sub>k+1</sub> = T<sub>k</sub> + P [ticks]</code>. If the task is resumed at <code>T<sub>k+1</sub> = T<sub>k</sub> + P + N</code>, upon detecting this drift, the kernel sets
(<code>T<sub>k+2</sub> = T<sub>k+1</sub> + P - N) &#8592;&#8594;</code>
<code>T<sub>k+2</sub> = T<sub>k</sub> + 2P</code>.
s.t., at any <code>k<sub>mth</sub></code> activation the difference to <code>k<sub>nth</sub></code> (the phase) is a mutiple of <code>P</code>, on the form <code>Phase = P(n-m), n &gt; m.</code>
`</p>
</div>
<div class="paragraph data-line-522">
<p><em>Example:</em></p>
</div>
<div class="listingblock data-line-524">
<div class="content">
<pre class="highlightjs highlight"><code class="language-C hljs" data-lang="C">VOID Task1(VOID* args)
{
    RK_UNUSEARGS
    UINT count = 0;
    while (1)
    {

        logPost("Task1: sleep periodic");
        kSleepPeriodic(300);
        /* wake here */
        count += 1U;
        if (count &gt;= 5)
        {
            kBusyWait(25); /* spin */
            count=0;
            /* every 5 activations there will be a drift */
        }
    }
}</code></pre>
</div>
</div>
<div class="paragraph data-line-547">
<p>Output:</p>
</div>
<div class="listingblock data-line-549">
<div class="content">
<pre class="highlightjs highlight"><code class="language-none hljs">0 ms :: Task1: sleep periodic
300 ms :: Task1: sleep periodic  &lt;-- (1st)
600 ms :: Task1: sleep periodic  &lt;-- (2nd)
900 ms :: Task1: sleep periodic  &lt;-- (3rd)
1200 ms :: Task1: sleep periodic &lt;-- (4th)
1525 ms :: Task1: sleep periodic &lt;-- (5th: delay)
1800 ms :: Task1: sleep periodic &lt;-- (6th: compensated)
&gt;&gt; (T@6th - T@5th = 275ms, then T@6th - T@4th = 2x300ms)
2100 ms :: Task1: sleep periodic
2400 ms :: Task1: sleep periodic
2700 ms :: Task1: sleep periodic
3025 ms :: Task1: sleep periodic &lt;-- delay
3300 ms :: Task1: sleep periodic &lt;-- compensated
3600 ms :: Task1: sleep periodic
3900 ms :: Task1: sleep periodic
4200 ms :: Task1: sleep periodic
4525 ms :: Task1: sleep periodic &lt;-- delay
4800 ms :: Task1: sleep periodic &lt;-- compensated
5100 ms :: Task1: sleep periodic
5400 ms :: Task1: sleep periodic
5700 ms :: Task1: sleep periodic
6025 ms :: Task1: sleep periodic</code></pre>
</div>
</div>
</div>
</div>
<div class="sect2 data-line-575">
<h3 id="blocking_time_out">3.3. Blocking Time-out</h3>
<div class="paragraph data-line-577">
<p>These are timers associated with kernel calls that are blocking. Thus, establishing an upper-bound waiting time might benefit them. When the time for unblocking is up, the kernel call returns, indicating a timeout. This value is passed as a number of ticks.</p>
</div>
<div class="paragraph data-line-579">
<p>When blocking is associated with a kernel object (other than the Task Control Block), the timeout node will store the object waiting for queue&#8217;s address, so it can be removed if time expires.</p>
</div>
<div class="paragraph data-line-581">
<p>A kernel call is made non-blocking, that is <em>try semantics</em>, by assigning the value <code>RK_NO_WAIT</code>, the function returns immediately if unsuccessful.
The value <code>RK_WAIT_FOREVER</code> suspends a task indefinitely until the condition is satisfied.</p>
</div>
<div class="paragraph data-line-584">
<p>In practice, we often block either using <code>RK_WAIT_FOREVER</code> or do not block (<em>try semantics</em>, <code>RK_NO_WAIT</code>).</p>
</div>
<div class="paragraph data-line-586">
<p>Use a bounded timeout only when you expect occasional misses and you know how to handle them.
If a blocking call times out and no recovery plan is feasible, it is as a system fault (on constrained devices this is usually unrecoverable at runtime; a watchdog is what is left).</p>
</div>
<div class="admonitionblock caution data-line-590">
<table>
<tr>
<td class="icon">
<i class="fa icon-caution" title="Caution"></i>
</td>
<td class="content">
<div class="paragraph data-line-591">
<p>Importantly, <em>an ISR shall <strong>never</strong> blocks</em>. Indeed, any blocking call from an ISR will hard fault if error checking is enabled.</p>
</div>
</td>
</tr>
</table>
</div>
</div>
<div class="sect2 data-line-594">
<h3 id="callout_timers_application_timers">3.4. Callout Timers (Application Timers)</h3>
<table class="tableblock frame-all grid-all stretch data-line-597">
<colgroup>
<col style="width: 100%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Timer Control Block</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Option: Reload/One-Shot</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Phase (Initial Delay)</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Callout Function Pointer</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Callout Argument</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Timeout Node</p></td>
</tr>
</tbody>
</table>
<div class="paragraph data-line-607">
<p>These are Application Timers that will issue a callback when expiring.
In addition to a callout function, an Application Timer receives an initial phase delay and a period and can choose to run once (one-shot) or auto-reload itself.</p>
</div>
<div class="paragraph data-line-610">
<p>The callback runs within a System Task with priority 0 and is non-preemptible, which makes the scheduler prioritise it over other tasks. Callouts must be short and unblocking, as they can cause high CPU contention.</p>
</div>
<div class="paragraph data-line-612">
<p>For clarity, Timer Callouts are on a separate list in the kernel, although they share the same <code>TIMEOUT</code> node.</p>
</div>
<div class="admonitionblock note data-line-615">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
Application Timers (with autoreload) will keep track of delays in between activations, to preserve phase accross calls as in  <a href="#sleeptimers">Periodic Sleep</a>
</td>
</tr>
</table>
</div>
<div class="paragraph data-line-618">
<p>Usage example of Callout Timers are found throughout this docbook.</p>
</div>
</div>
<div class="sect2 data-line-620">
<h3 id="system_tick">3.5. System Tick</h3>
<div class="paragraph data-line-622">
<p>A dedicated peripheral that generates an interrupt after a defined period provides the kernel time reference. For ARMv6/7M, this peripheral is the built-in SysTick, a 24-bit counter timer.
The handler performs some housekeeping on every tick and assesses the need to call a context switch.</p>
</div>
<div class="paragraph data-line-625">
<p>The 'housekeeping' accounts for global timer tracking and any tick-dependent condition that might change a task status.
When a timer expires, it might switch a task from <code>WAITING</code> to <code>READY</code> or dispatch a callback. In the case of a callback, this will also trigger a context-switching for the TimerHandler System Task in which the callback is executed and the related timer(s) are appropriately updated.</p>
</div>
<div class="paragraph data-line-628">
<p>Note that tasks might switch from <code>WAITING</code> to <code>READY</code> for reasons other than tick-related. In these cases, context switching might be triggered immediately if the readied task can preempt the running task.</p>
</div>
</div>
</div>
</div>
<div class="sect1 data-line-630">
<h2 id="memory_allocator">4. Memory Allocator</h2>
<div class="sectionbody">
<table class="tableblock frame-all grid-all stretch data-line-633">
<colgroup>
<col style="width: 100%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Memory Allocator Control Block</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Associated Block Pool</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Number of Blocks</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Block Size</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Number of Free Blocks</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Free Block List</p></td>
</tr>
</tbody>
</table>
<div class="paragraph data-line-642">
<p>The standard C library <code>malloc()</code>  leads to fragmentation and (also, because of that) is highly indeterministic. Unless we use it once - to allocate memory before starting up, it doesn’t fit. But often, we need to 'multiplex' memory amongst tasks over time, that is, to dynamically allocate and deallocate.</p>
</div>
<div class="paragraph data-line-644">
<p>To avoid fragmentation, we use fixed-size memory blocks. A simple approach would be a static table marking each block as free or taken. With this pattern, you will need to 'search' for the next available block, if any - the time for searching changes - bounding this search to a maximum number of blocks, or <em>O(n)</em>.
To optimise, an approach is to keep track of what is free using a dynamic table—a linked list of addresses. Now we have <em>O(1)</em>.</p>
</div>
<div class="paragraph data-line-647">
<p>We use "meta-data" to initialise the linked list. Every address holds the "next" address value. All addresses are within the range of a pool of fixed-size blocks.
This approach limits the minimal size of a block to the size of a memory address—32 bits for our supported architecture.</p>
</div>
<div class="paragraph data-line-650">
<p>Yet, this is the cheapest way to store meta-data. If not stored on the empty address itself, an extra 32-bit variable would be needed for each block, so it could have a size of less than 32 bits.</p>
</div>
<div class="admonitionblock tip data-line-653">
<table>
<tr>
<td class="icon">
<i class="fa icon-tip" title="Tip"></i>
</td>
<td class="content">
<div class="paragraph data-line-654">
<p>Allocating memory at runtime is a major source of latency (1), indeterministic (2) behaviour, and footprint overhead (3).</p>
</div>
<div class="paragraph data-line-656">
<p>Design choice: the allocator&#8217;s design achieves low-cost, deterministic, fragmentation-free memory management by using fixed-size word-aligned block sizes (1)(2) and embedding metadata within the memory blocks themselves (3).</p>
</div>
<div class="paragraph data-line-658">
<p>Benefits: Run-time memory allocation benefits have no real-time drawbacks.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph data-line-662">
<p><em>Importantly, the kernel will always round up the block size to the next multiple of 4. Say the user creates a memory pool, assigning blocks to be 6-byte wide; they will turn into 8-byte blocks.</em></p>
</div>
<div class="sect2 data-line-664">
<h3 id="how_it_works">4.1. How it works</h3>
<div class="paragraph data-line-666">
<p>When a routine calls <code>alloc()</code>, the address to be returned is the one a "free list" is pointing to, say <code>addr1</code>. Before returning <code>addr1</code> to the caller, we update the free list to point to the value stored within <code>addr1</code> - say <code>addr8</code> at that moment.</p>
</div>
<div class="paragraph data-line-668">
<p>When a routine calls <code>free(addr1)</code>, we overwrite whatever has been written in addr1 with the value-free list point to (if no more <code>alloc()</code> were issued, it would still be <code>addr8</code>), and <code>addr1</code> becomes the free list head again.</p>
</div>
<div class="paragraph data-line-670">
<p>Allocating and deallocating fixed-size blocks using this structure and storing meta-data this way is as deterministic (<em>O(1)</em>) and economical as we can get for dynamic memory allocation.</p>
</div>
<div class="paragraph data-line-672">
<p>A drawback is if a routine writes to non-allocated memory within a pool it will spoil the meta-data and the Allocator will fail.</p>
</div>
</div>
</div>
</div>
<div class="sect1 data-line-676">
<h2 id="inter_task_communication">5. <strong><em>Inter-Task Communication</em></strong></h2>
<div class="sectionbody">
<div class="exampleblock data-line-678">
<div class="content">
<div class="paragraph data-line-679">
<p>RK0 Blog:</p>
</div>
<div class="ulist data-line-681">
<ul>
<li class="data-line-681">
<p><a href="https://kernel0.org/2025/01/08/inter-task-communication-on-embedded-operating-systems/">About Inter-Task Communication - Part 1</a></p>
</li>
<li class="data-line-683">
<p><a href="https://kernel0.org/2025/01/09/about-inter-task-communication-p2/">About Inter-Task Communication - Part 2</a></p>
</li>
</ul>
</div>
</div>
</div>
<div class="paragraph data-line-688">
<p>Inter-Task Communication (ITC) refers to the mechanisms that enable tasks to coordinate/cooperate/synchronise by means of sending or receiving information that falls into two logical categories: <em>Signals</em> or <em>Messages</em>.</p>
</div>
<div class="ulist data-line-690">
<ul>
<li class="data-line-690">
<p><strong><em>Signals</em></strong>:
A signal is solely defined by its absence or presence. The meaning is implicit.</p>
</li>
<li class="data-line-693">
<p><strong><em>Messages</em></strong>: When the operations used for tasks to communicate also allow conveying a payload, these mechanisms are regarded as Message Passing.</p>
</li>
</ul>
</div>
<div class="sect2 data-line-696">
<h3 id="directsignals">5.1. Direct Signals (Task Notification)</h3>
<table class="tableblock frame-all grid-all stretch data-line-699">
<colgroup>
<col style="width: 100%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Within Task Control Block</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Event Register</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Required Flags</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Options</p></td>
</tr>
</tbody>
</table>
<div class="paragraph data-line-707">
<p>Each Task Control Block stores a 32-bit <em>Event Register</em>. We define that a 32-bit Signal carries 32 <em>event flags</em>&#8201;&#8212;&#8201;it can represent a combination of 32 different events, if defining 1 event/bit. A bit set means an event notification is pending to be detected.</p>
</div>
<div class="paragraph data-line-709">
<p>Although called <em>Signals</em> this service does not mimic POSIX or UNIX/BSD Signals that act as asynchronous software interrupts.</p>
</div>
<div class="paragraph data-line-711">
<p>Bitwise friendly, the API is written as <code>set()</code> (as to signal/post), <code>get()</code> (as to wait/pend).</p>
</div>
<div class="paragraph data-line-713">
<p>A task checks for a combination of events it is expecting. This combination can be satisfied if <code>ANY</code> (OR logic) of the required bits are set or if <code>ALL</code> of the required bits are set (AND logic).</p>
</div>
<div class="paragraph data-line-715">
<p>Thus, if the condition is not met the task can optionally suspends, switching to the logical state <code>PENDING</code>.</p>
</div>
<div class="paragraph data-line-717">
<p>When another task issues a <code>set()</code> which result satisfies the waiting condition, the task state is then <code>READY</code>.</p>
</div>
<div class="paragraph data-line-719">
<p>Upon returning, all required positions have been cleared on the Task&#8217;s Event Register.</p>
</div>
<div class="paragraph data-line-721">
<p>A <em>set</em> is always an <em>OR</em> operation of an input mask over the current value. <code>0x00</code> is invalid for both <code>set()</code> and <code>get()</code> operations.</p>
</div>
<div class="paragraph data-line-724">
<p>Additional operations are to <code>query</code> a tasks&#8217;s event register, and to <code>clear</code> its own registers.</p>
</div>
<div class="sect4 data-line-728">
<h5 id="supervisortask">5.1.1. Usage Example: Supervisor Task</h5>
<div class="paragraph data-line-731">
<p>One possible usage pattern is a task&#8217;s cycle begins checking for any events (it is able/supposed to handle).
If using it on a supervisor task&#8201;&#8212;&#8201;it can create a neat event-driven pattern for a soft/firm real-time system:</p>
</div>
<div class="listingblock data-line-734">
<div class="content">
<pre class="highlightjs highlight"><code class="language-C hljs" data-lang="C">typedef struct
{
    ULONG pendingBit;
    TaskHandle_t dstTask;
    ULONG dstSignal;
} Route_t;

static const Route_t routes[] =
{
    {
        PENDING_AIRFLOW_INCREASE,
        airFlowTaskHandle,
        AIRFLOW_INCREASE_SIGNAL
    },
    {
        PENDING_TEMP_DECREASE,
        tempTaskHandle,
        TEMP_DECREASE_SIGNAL
    },
    /* more routes */
}

VOID SupervisorTask(VOID *args)
{
    RK_UNUSEARGS;

    while(1)

    {
        ULONG gotFlags = 0UL;

        RK_ERR err = kSignalWait(0xFFFF,
                                 RK_FLAGS_ANY,
                                 &amp;gotFlags,
                                 SUPERVISOR_T_PERIOD);

        if (err == RK_SUCCESS &amp;&amp; gotFlags != 0)
        {
            for (ULONG i = 0; i &lt; ARRAY_LEN(routes); ++i)
            {
                if (gotFlags &amp; routes[i].pendingBit)
                {
                    kSignalSet(routes[i].dstTask, routes[i].dstSignal);
                }
            }
        }

        /* if there is anything to do if time out */
    }
}</code></pre>
</div>
</div>
<div class="paragraph data-line-787">
<p>Task Signals are the the only ITC primitive that cannot be disabled, thus, they are regarded as a <em>Core Mechanism</em>.</p>
</div>
</div>
</div>
<div class="sect2 data-line-789">
<h3 id="semaphores">5.2. Semaphores</h3>
<table class="tableblock frame-all grid-all stretch data-line-791">
<colgroup>
<col style="width: 100%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Semaphore Control Block</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Counter (Unsigned Integer)</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Semaphore Type (Counter/Binary)</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Waiting Queue</p></td>
</tr>
</tbody>
</table>
<div class="exampleblock data-line-798">
<div class="content">
<div class="paragraph data-line-799">
<p>A semaphore S is a nonnegative integer variable, apart from the operations it is subjected to. S is initialized to a nonnegative value. The two operations, called P and V, are defined as follows:</p>
</div>
<div class="paragraph data-line-801">
<p><code>P(S): if S &gt; 0 then S := S-1, else the process is suspended until S &gt; 0.</code></p>
</div>
<div class="paragraph data-line-803">
<p><code>V(S): if there are processes waiting, then one of them is resumed; else S := S+1.</code></p>
</div>
<div class="paragraph data-line-805">
<p>(Dijkstra, 1968)</p>
</div>
</div>
</div>
<div class="paragraph data-line-808">
<p><em>Semaphores</em> are <em>public</em> kernel objects for signalling and waiting for events.</p>
</div>
<div class="paragraph data-line-810">
<p>The primitives <code>post()</code> is the <code>V(S)</code>, and <code>pend()</code> is the <code>P(S)</code> as described above.</p>
</div>
<div class="paragraph data-line-812">
<p>When <code>pend()</code> is issued on a semaphore which counter is 0, the caller (optionally) switches to a <em>BLOCKED</em> state, and is enqueued within the semaphore queue.</p>
</div>
<div class="paragraph data-line-814">
<p>After that, every <code>post()</code> issued to a semaphore releases a single task, ordered by priority, until there is no more blocked tasks within the semaphore.</p>
</div>
<div class="paragraph data-line-816">
<p>Then, the internal counter will increase above 0, only and if only there are no tasks on the waiting queue when the semaphore is signalled.</p>
</div>
<div class="sect3 data-line-819">
<h4 id="counting_and_binary_semaphores">5.2.1. Counting and Binary Semaphores</h4>
<div class="paragraph data-line-821">
<p>The typical use case for <em>Counting Semaphores</em> is as a "credit tracker"&#8201;&#8212;&#8201;one uses it to verify (<code>wait/pend</code>) and indicate (<code>signal/post</code>) the availability of a countable resource&#8201;&#8212;&#8201;say, number of slots within a queue.</p>
</div>
<div class="paragraph data-line-823">
<p>A <em>Binary Semaphore</em> is a specialisation: it counts up to 1 and down to 0&#8201;&#8212;&#8201;they do not accumulate. We often say its <em>either FULL or EMPTY</em>. The typical use case is for task-to-task (unilateral or bi-lateral), or ISR-to-task (unilateral) synchronisation.</p>
</div>
<div class="paragraph data-line-825">
<p>In this sense, they overlap the <em>Direct Signals</em> mechanism, that can be seen as a pack of <em>private binary semaphores</em> (only the task itself can <code>pend</code> but any task can <code>post</code>). Binary Semaphores can also be used for mutual-exclusion, but it has drawbacks as will be explained later.</p>
</div>
<div class="paragraph data-line-827">
<p>It is neither common nor always desirable, but in RK0 Semaphores can <em>broadcast a signal</em>, using <code>wake(&amp;sema, n)</code>, in which at most <code>n</code> tasks will be readied. If <code>n=0</code> <em>all</em> tasks are readied, and this operation can be aliased as <code>flush(&amp;sema)</code>.</p>
</div>
<div class="paragraph data-line-829">
<p>Finally, a <code>query()</code> operation on a semaphore will return the number of waiting tasks if any, or the counter value. To differentiate, the number of waiting tasks is returned as a negative value. A nonnegative value is the semaphore&#8217;s counter value.</p>
</div>
<div class="paragraph data-line-831">
<p>Notes:</p>
</div>
<div class="olist arabic data-line-833">
<ol class="arabic">
<li class="data-line-833">
<p>The <code>post</code> and <code>pend</code> operations are aliased to <code>signal()</code> and <code>wait()</code> respectively, to satisfy those who prefer this nomenclature.</p>
</li>
<li class="data-line-834">
<p>If Binary Semaphore is initialised with a value &gt; 1, the effective value is 1.</p>
</li>
</ol>
</div>
<div class="sect4 data-line-837">
<h5 id="bilateralsynch">5.2.1.1. Usage Example: Task-to-Task Bilateral Synchronisation</h5>
<div class="paragraph data-line-839">
<p>The snippet below shows two tasks lock-stepping by posting and pending on (binary) semaphores.
Task2 depends on Task1 finishing 'work1' to perform 'work2'. And vice-versa.</p>
</div>
<div class="exampleblock data-line-842">
<div class="content">
<div class="paragraph data-line-844">
<p>(Note Direct Signals are a better choice for this use-case.)</p>
</div>
<div class="listingblock data-line-847">
<div class="content">
<pre class="highlightjs highlight"><code class="language-C hljs" data-lang="C">RK_SEMA work1Sema;
RK_SEMA work2Sema;

VOID kApplicationInit(VOID)
{
/* semaphores init at 0 */
	kSemaInit(&amp;work1Sema, RK_SEMA_BIN, 0);
	kSemaInit(&amp;work2Sema, RK_SEMA_BIN, 0);

}

VOID Task1(VOID* args)
{
    RK_UNUSEARGS
	while (1)
	{
	    doWork1();
		kSemaPost(&amp;work1Sema);
		kSemaPend(&amp;work2Sema, RK_WAIT_FOREVER);
		 /* T1 finished. Waiting for T2. */

	}
}

VOID Task2(VOID* args)
{
    RK_UNUSEARGS
	while (1)
	{
		 kSemaPend(&amp;work1Sema, RK_WAIT_FOREVER);
		 doWork2();
		 kSemaPost(&amp;work2Sema);
	}
}</code></pre>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="sect3 data-line-888">
<h4 id="mutex_semaphores_locks">5.2.2. Mutex Semaphores (Locks)</h4>
<table class="tableblock frame-all grid-all stretch data-line-891">
<colgroup>
<col style="width: 100%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Mutex Control Block</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Locked State (Boolean)</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Owner</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Waiting Queue</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Mutex Node (list node within the owner TCB)</p></td>
</tr>
</tbody>
</table>
<div class="paragraph data-line-900">
<p>Some code regions are critical in that they cannot be accessed by more than one task at once. Acquiring (<code>lock()</code>) a mutex before entering a region and releasing it when leaving makes that region mutually exclusive.</p>
</div>
<div class="paragraph data-line-902">
<p>A Mutex is another semaphore specialisation&#8201;&#8212;&#8201;it can be seen as a binary semaphore with a notion of ownership - when a task susccesfully acquires a mutex is now the <em>owner</em>, and only this task can release it.</p>
</div>
<div class="paragraph data-line-904">
<p>If a task tries to acquire an already locked mutex, it switches to <code>BLOCKED</code> state until <em>the mutex is unlocked by its owner</em>. Then, the highest priority task waiting to acquire the resource is dequeued, as on semaphores.</p>
</div>
<div class="paragraph data-line-906">
<p>However, unlike semaphores, the complementary operation, <code>unlock()</code>, when issued by a non-owner, has undefined behaviour. In K0, it will be a hard fault.</p>
</div>
<div class="paragraph data-line-908">
<p>Mutexes are solely for mutual exclusion; they cannot be used for signalling. It is common to use Counting Semaphores initialised as 1, or Binary Semaphores for mutual exclusion.</p>
</div>
<div class="paragraph data-line-910">
<p>However, particularly for a Counting Semaphore, if the count increases twice in a row, the mutual exclusion is gone. For both, <em>Priority Inversion</em> can become a problem, as will be explained.</p>
</div>
<div class="paragraph data-line-912">
<p><strong><em>PS: Mutexes in RK0 are not recursive. One cannot make reentrant calls on critical regions</em>.</strong></p>
</div>
<div class="sect4 data-line-915">
<h5 id="boundedbuffer">5.2.2.1. Usage Example: Bounded Buffer</h5>
<div class="paragraph data-line-917">
<p>The snippet below shows a <em>consumer-producer</em> pattern for a buffer with K slots (<em>bounded buffer pattern</em>). Two semaphores track the number of slots for the producer and items for the consumer. The mutex prevents any write or read from being disrupted.</p>
</div>
<div class="exampleblock data-line-919">
<div class="content">
<div class="listingblock data-line-921">
<div class="content">
<pre class="highlightjs highlight"><code class="language-C hljs" data-lang="C">RK_SEMA  item;
RK_SEMA  space;
RK_MUTEX lock;
#define N (K)
typedef struct mesg
{
 UINT field1;
 UINT field2;
 UINT field3;
 UINT field4;
} Mesg_t; /* a 16-byte message */

/* a ring buffer of messages */
Mesg_t mailbox[N]={0};

kApplicationInit(VOID)
{
    kSemaInit(&amp;item,  RK_SEMA_COUNT, 0); /* no items */
    kSemaInit(&amp;space, RK_SEMA_COUNT, N); /* N buffers available */
    kMutexInit(&amp;lock);
}
/* circular buffer handling omitted */

/* wait for space, lock, write, unlock, signal there is item */
VOID PostMail(Mesg_t* sendPtr)
{
    kSemaWait(&amp;space, RK_WAIT_FOREVER);
    kMutexLock(&amp;lock,  RK_WAIT_FOREVER);
    memcpy(&amp;mailbox[tail], sendPtr, sizeof(Mesg_t));
    kMutexUnlock(&amp;lock);
    kSemaSignal(&amp;item);
}

/* wait for item, lock, read, unlock, signal there is space */
VOID PendMail(Mesg_t* recvPtr)
{
    kSemaWait(&amp;item, RK_WAIT_FOREVER);
    kMutexLock(&amp;lock,  RK_WAIT_FOREVER);
    memcpy(recvPtr, &amp;mailbox[head], sizeof(Mesg_t));
    kMutexUnlock(&amp;lock);
    kSemaSignal(&amp;space);
}</code></pre>
</div>
</div>
</div>
</div>
<hr>
</div>
<div class="sect4 data-line-970">
<h5 id="priority_inversion_and_the_priority_inheritance_protocol">5.2.2.2. Priority Inversion and the Priority Inheritance Protocol</h5>
<div class="paragraph data-line-973">
<p>Let TH, TM, and TL be three tasks with priority high (H), medium (M) and low (L), respectively. Say TH is dispatched and blocks on a mutex that 'TL' has acquired (i.e.: <em>"TL is blocking TH</em>").</p>
</div>
<div class="paragraph data-line-975">
<p>If 'TM' does not need the resource, it will run and preempt 'TL'. And, by transition, 'TH'.</p>
</div>
<div class="paragraph data-line-977">
<p>From now on, 'TH' has an <em>unbounded waiting time</em> because any task with priority higher than 'L' that does not need the resource indirectly prevents it from being unblocked&#8201;&#8212;&#8201;<em>awful.</em></p>
</div>
<div class="paragraph data-line-979">
<p>The Priority Inheritance (PI) Protocol avoids this unbounded waiting. It is characterised by an invariant, simply put:</p>
</div>
<div class="exampleblock data-line-981">
<div class="content">
<div class="paragraph data-line-982">
<p><strong><em>At any instant a Task assumes the highest priority amongst the tasks it is blocking</em></strong>.</p>
</div>
</div>
</div>
<div class="paragraph data-line-985">
<p>If employed on the situation described above, task TM cannot preempt TL, whose effective priority would have been raised to 'H'.</p>
</div>
<div class="paragraph data-line-987">
<p>It is straightforward to reason about this when you consider the scenario of a single mutex.</p>
</div>
<div class="paragraph data-line-989">
<p>But when locks nest&#8201;&#8212;&#8201;that is, more than one critical region&#8201;&#8212;&#8201;the protocol also needs to be:</p>
</div>
<div class="ulist data-line-991">
<ul>
<li class="data-line-991">
<p>Transitive: that is, if T1 is blocking T2, and T2 is blocking T3, if T3 has the highest priority, T3 propagates its priority to T1 via T2.</p>
</li>
<li class="data-line-993">
<p>A task can own several mutexes at once. Thus, when exiting the critical region it needs to look up each waiting queue, and assume the highest priority. If there are no blocked tasks behind, its nominal priority is then restored. (As tasks are enqueued by priority, it means looking at the task waiting on the head of each waiting queue.)</p>
</li>
</ul>
</div>
<div class="paragraph data-line-995">
<p>Below, a demonstration:</p>
</div>
<div class="exampleblock data-line-996">
<div class="content">
<div class="listingblock data-line-997">
<div class="content">
<pre class="highlightjs highlight"><code class="language-C hljs" data-lang="C">/* Task1 has the Highest nominal priority */
/* Task2 has the Medium nominal priority */
/* Task3 has Lowest nominal priority */

/* Note Task3 starts as 1 and 2 are delayed */

RK_DECLARE_TASK(task1Handle, Task1, stack1, STACKSIZE)
RK_DECLARE_TASK(task2Handle, Task2, stack2, STACKSIZE)
RK_DECLARE_TASK(task3Handle, Task3, stack3, STACKSIZE)


RK_MUTEX mutexA;
RK_MUTEX mutexB;

VOID kApplicationInit(VOID)
{
	kassert(!kCreateTask(&amp;task1Handle, Task1, RK_NO_ARGS, "Task1", stack1, \
		STACKSIZE, 1, RK_PREEMPT));
	kassert(!kCreateTask(&amp;task2Handle, Task2, RK_NO_ARGS, "Task2", stack2, \
		STACKSIZE, 2, RK_PREEMPT));
	kassert(!kCreateTask(&amp;task3Handle, Task3, RK_NO_ARGS, "Task3", stack3, \
		STACKSIZE, 3, RK_PREEMPT));

/* mutexes initialised with priority inheritance enabled */
	kMutexInit(&amp;mutexA, RK_INHERIT);
	kMutexInit(&amp;mutexB, RK_INHERIT);
}



VOID Task3(VOID *args)
{
	RK_UNUSEARGS
	while (1)
	{
		printf("@ %lums: [TL] Attempting to LOCK 'A' | Eff: %d | Nom: %d\r\n", kTickGet(),
			   runPtr-&gt;priority, runPtr-&gt;prioReal);

		kMutexLock(&amp;mutexA, RK_WAIT_FOREVER);

		printf("@ %lums: [TL] LOCKED 'A' (in CS) | Eff: %d | Nom: %d\r\n", kTickGet(),
			   runPtr-&gt;priority, runPtr-&gt;prioReal);

		kBusyWait(60); /* &lt;-- important */

		printf("@%lums: [TL] About to UNLOCK 'A' | Eff: %d | Nom: %d\r\n", kTickGet(),
			   runPtr-&gt;priority, runPtr-&gt;prioReal);

		kMutexUnlock(&amp;mutexA);

		printf("---&gt;");
		printf("@%lums: [TL] Exit CS | Eff: %d | Nom: %d\r\n", kTickGet(),
			   runPtr-&gt;priority, runPtr-&gt;prioReal);

		kSleep(4);
	}
}

VOID Task2(VOID *args)
{
	RK_UNUSEARGS
	while (1)
	{
		kSleep(5);

		printf("@%lums: [TM] Attempting to LOCK 'B' | Eff: %d | Nom: %d\r\n", kTickGet(),
			   runPtr-&gt;priority, runPtr-&gt;prioReal);
		kMutexLock(&amp;mutexB, RK_WAIT_FOREVER);

		printf("@%lums: [TM] LOCKED 'B', now trying to LOCK 'A' | Eff: %d | Nom: %d\r\n",
			   kTickGet(), runPtr-&gt;priority, runPtr-&gt;prioReal);
		kMutexLock(&amp;mutexA, RK_WAIT_FOREVER);

		printf("@%lums: [TM] LOCKED 'A' (in CS) | Eff: %d | Nom: %d\r\n", kTickGet(),
			   runPtr-&gt;priority, runPtr-&gt;prioReal);
		kMutexUnlock(&amp;mutexA);

		printf("@%lums: [TM] UNLOCKING 'B' | Eff: %d | Nom: %d\r\n", kTickGet(),
			   runPtr-&gt;priority, runPtr-&gt;prioReal);

		kMutexUnlock(&amp;mutexB);

		printf("---&gt;");

		printf("@%lums: [TM] Exit CS | Eff: %d | Nom: %d\r\n", kTickGet(),
			   runPtr-&gt;priority, runPtr-&gt;prioReal);
	}
}

VOID Task1(VOID *args)
{
	RK_UNUSEARGS
	while (1)
	{
		kSleep(2);

		printf("@%lums: [TH] Attempting to LOCK 'B'| Eff: %d | Nom: %d\r\n", kTickGet(),
			   runPtr-&gt;priority, runPtr-&gt;prioReal);

		kMutexLock(&amp;mutexB, RK_WAIT_FOREVER);

		printf("@%lums: [TH] LOCKED 'B' (in CS)  | Eff: %d | Nom: %d\r\n", kTickGet(),
			   runPtr-&gt;priority, runPtr-&gt;prioReal);

		kMutexUnlock(&amp;mutexB);

		printf("---&gt;");

		printf("@%lums: [TH] Exit CS | Eff: %d | Nom: %d\r\n", kTickGet(),
			   runPtr-&gt;priority, runPtr-&gt;prioReal);
	}
}</code></pre>
</div>
</div>
</div>
</div>
<div class="paragraph data-line-1114">
<p><em>Result and comments</em>:</p>
</div>
<div class="exampleblock data-line-1116">
<div class="content">
<div class="listingblock data-line-1118">
<div class="content">
<pre>&gt;&gt;&gt;&gt; TL locks 'A'. Higher priority tasks are sleeping. &lt;&lt;&lt;&lt;

@ 14720ms: [TL] Attempting to LOCK 'A' | Eff: 3 | Nom: 3
@ 14720ms: [TL] LOCKED 'A' (in CS) | Eff: 3 | Nom: 3

@14721ms: [TM] Attempting to LOCK 'B' | Eff: 2 | Nom: 2

&gt;&gt;&gt;&gt; TM acquires 'B' and is blocked by TL on 'A'. TL inherits TM's  priority. &lt;&lt;&lt;&lt;

@14721ms: [TM] LOCKED 'B', now trying to LOCK 'A' | Eff: 2 | Nom: 2

&gt;&gt;&gt;&gt; TH will blocked by TM on 'B': &lt;&lt;&lt;&lt;

@14722ms: [TH] Attempting to LOCK 'B'| Eff: 1 | Nom: 1

&gt;&gt;&gt;&gt; TM inherits TH's priority. TL inherits TH's priority via TM. &lt;&lt;&lt;&lt;

@14780ms: [TL] About to UNLOCK 'A' | Eff: 1 | Nom: 3

&gt;&gt;&gt;&gt; Upon unlocking 'A', TL is preempted by TM. It means TL's priority has been restored, as it is no longer blocking a higher priority task. &lt;&lt;&lt;&lt;


&gt;&gt;&gt;&gt; Now TM acquires 'A' &lt;&lt;&lt;&lt;

@14780ms: [TM] LOCKED 'A' (in CS) | Eff: 1 | Nom: 2

&gt;&gt;&gt;&gt; After releasing 'A', but before releasing 'B', TM's priority is still '1', as it is blocking TH while holding 'B'. &lt;&lt;&lt;&lt;

@14780ms: [TM] UNLOCKING 'B' | Eff: 1 | Nom: 2

&gt;&gt;&gt;&gt; Upon unlocking 'B' TM is preempted by TH. (TM's priority has been restored.) &lt;&lt;&lt;&lt;

@14780ms: [TH] LOCKED 'B' (in CS)  | Eff: 1 | Nom: 1

&gt;&gt;&gt; RESULT: even though priority inversion was enforced, tasks leave the nested lock ordered by their nominal priority. &lt;&lt;&lt;

---&gt;@14780ms: [TH] Exit CS | Eff: 1 | Nom: 1
---&gt;@14780ms: [TM] Exit CS | Eff: 2 | Nom: 2
---&gt;@14780ms: [TL] Exit CS | Eff: 3 | Nom: 3</pre>
</div>
</div>
</div>
</div>
<div class="paragraph data-line-1162">
<p>Importantly, the worst-case time is bounded by the time the lowest priority task holds a lock (60 ms in the example: 14720ms &#8594; 14780ms).</p>
</div>
<div class="paragraph data-line-1164">
<p>As for each priority update we check each waiting queue for each mutex a task owns, the time-complexity is is linear <em>owner*mutex</em>. But, typically no task ever holds more than a few mutexes. Yet, one should not be encouraged to nest locks if not needed.</p>
</div>
<div class="exampleblock data-line-1166">
<div class="content">
<div class="paragraph data-line-1168">
<p><em><strong>Mutexes vs Binary Semaphores</strong></em></p>
</div>
<div class="paragraph data-line-1170">
<p>There is (or used to be) a lot of fuss about whether binary semaphores are appropriate to use as locks. As a practical guideline, if all tasks sharing the resource have the same priority, using a binary semaphore <em>can be appropriate</em>&#8201;&#8212;&#8201;because a binary semaphore is considerably faster. It all depends on the case.</p>
</div>
<div class="paragraph data-line-1172">
<p>The drawback is the lack of ownership: any task can accidentally release the resource. On a large codebase, this can become a real problem. Nonetheless, this is a problem for semaphores in general.</p>
</div>
<div class="paragraph data-line-1174">
<p>For tasks with different priorities, binary semaphores should never be considered for mutual exclusion unless priority inversion is not a problem (how?).</p>
</div>
<div class="paragraph data-line-1176">
<p>Counting semaphores initialised as 1 is too risky. Besides the priority inversion, if the count ever increases above 1, mutual exclusion is lost, and multiple tasks can enter the critical section at once.</p>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="sect2 data-line-1181">
<h3 id="scheduler_lock">5.3. Scheduler Lock</h3>
<div class="paragraph data-line-1183">
<p>Often, we need a task to perform operations without being preempted. A mutex serialises access to a code region but does not prevent a task from being preempted while operating on data. Depending on the case, this can lead to inconsistent data state.</p>
</div>
<div class="paragraph data-line-1185">
<p>An aggressive way is to disable interrupts globally. For kernel services often it is the only way to keep data integrity. On the higher level it is feasible for very short operations and/or when you need to protect data from interrupts altogether.</p>
</div>
<div class="paragraph data-line-1187">
<p>A less aggressive approach is to make the task non-preemptible with <code>kSchLock()</code> before entering the critical region and <code>kSchUnlock()</code> when leaving. This way, interrupts are still being sensed, and even higher-priority tasks might switch to a ready state, but the running thread will not be preempted.</p>
</div>
<div class="paragraph data-line-1189">
<p>The priority inversion it potentially causes is bounded. If a higher-priority task is readied while the scheduler is locked, the context switch happens immediately after unlocking.</p>
</div>
<div class="paragraph data-line-1191">
<p>Note that for locking/unlocking the scheduler the global interrupts will be disabled for the time to increment/decrement a counter, therefore, if your atomic operation is as short as that (3 to 4 cycles), disabling/enabling global interrupts is a better alternative.</p>
</div>
<hr>
<div class="paragraph data-line-1194">
<p><em>To add to the discussion, when two threads need to access the same data to 'read-modify-write', a lock-free mechanism is the LDREX/STREX operations of ARMv7M (or more generally C11 atomics). They do not avoid preemptions, and particularly in ARMv7m, if the data is touched by an ISR before the store-exclusive concludes, the ownership is lost. Typically used for multi-core spin-locking.</em></p>
</div>
<hr>
</div>
<div class="sect2 data-line-1198">
<h3 id="sleep_queues">5.4. Sleep Queues</h3>
<table class="tableblock frame-all grid-all stretch data-line-1200">
<colgroup>
<col style="width: 100%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Event Control Block</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Waiting Queue</p></td>
</tr>
</tbody>
</table>
<div class="paragraph data-line-1205">
<p>The <code>RK_SLEEP_QUEUE</code> object is simply a queue of tasks sleeping waiting for a signal/wake operation on them. That could be read 'as tasks sleeping, until they are signalled an event has happened'.</p>
</div>
<div class="paragraph data-line-1207">
<p>That&#8217;s why this primitive was formerly called <code>RK_EVENT</code>. Naturally we might name the queue as to indicate the event, which normally is a state (e.g., <code>notFull</code>, <code>notEmpty</code>) or the action it triggers (e.g., <code>goWriters</code>, <code>goReaders</code>).</p>
</div>
<div class="paragraph data-line-1209">
<p>An <em>RK_SLEEP_QUEUE</em> object does not have any records to indicate if an associated event has ever happened.
Thus, a call <code>wait(&amp;sleepq, timeout)</code> <em>always</em> put the caller task to sleep. Note that using <code>RK_NO_WAIT</code> on this primitive is meaningless, because there is nothing to 'try'. The call will just return.</p>
</div>
<div class="paragraph data-line-1212">
<p>A <code>signal(&amp;sleepq)</code> will wake-up a single task - the highest priority. A <code>wake(&amp;sleepq, n, &amp;r)</code> is a <em>broadcast</em>: at most <code>n</code> sleeping tasks will switch to <code>READY</code>.  <code>r</code> will store the number of remaining tasks, if any.</p>
</div>
<div class="paragraph data-line-1214">
<p>If willing to wake <em>all</em> tasks, one either make <code>n=0</code>, or use the <code>flush(&amp;sleepq)</code> helper.
A <code>query(&amp;sleepq)</code> operation returns the number of sleeping tasks.</p>
</div>
<div class="paragraph data-line-1217">
<p>Finally, as any synchronisation needs an associated waiting queue, <code>RK_SLEEP_QUEUE</code> is a building block for high-level synchronisation schemes.</p>
</div>
<div class="paragraph data-line-1219">
<p>To provide more flexibility, there is an option to bypass queue discipline and ready an arbitrary task identified by its Task Handle, using <code>ready(&amp;sleepq, taskHandle)</code>.</p>
</div>
</div>
<div class="sect2 data-line-1223">
<h3 id="monitors">5.5. Monitors</h3>
<div class="paragraph data-line-1226">
<p><em>Monitors</em> (Hansen) were originally a programming language feature, encapsulating conditional synchronisation and mutual exclusion. The mutual exclusion was enforced by language and there was a single waiting queue for a monitor.</p>
</div>
<div class="paragraph data-line-1229">
<p>A Monitor <em>invariant</em> is that a single task can be active&#8201;&#8212;&#8201;a task can go  inactive either by suspension or when leaving the monitor procedure. When a task is signalled it is supposed to wake up, and now this standoff has to be solved.</p>
</div>
<div class="sect4 data-line-1231">
<h5 id="monitor_semantics">5.5.1. Monitor Semantics</h5>
<div class="paragraph data-line-1233">
<p>To keep the invariant, there are at least three approaches. (I am not aware of any other).</p>
</div>
<div class="ulist data-line-1235">
<ul>
<li class="data-line-1235">
<p><em>Signal-and-leave (Hansen)</em></p>
</li>
</ul>
</div>
<div class="paragraph data-line-1237">
<p>The the signaller task will signal another task and immediately leave. It was the first proposal, as there was a single waiting queue associated to a monitor.</p>
</div>
<div class="ulist data-line-1239">
<ul>
<li class="data-line-1239">
<p><em>Signal-and-wait (Hoare)</em></p>
</li>
</ul>
</div>
<div class="paragraph data-line-1241">
<p>In this case, a task signals and blocks itself on a 'urgent queue'. When the waiter exits the monitor or blocks again, the signaller resumes.</p>
</div>
<div class="ulist data-line-1243">
<ul>
<li class="data-line-1243">
<p><em>Signal-and-continue</em> (Mesa)</p>
</li>
</ul>
</div>
<div class="paragraph data-line-1245">
<p>A signal is issued while holding a mutex the signalled task must also acquire. Thus a signaller is not enforced to either leave or suspend itself, but it does need to release the mutex before going inactive.</p>
</div>
<div class="paragraph data-line-1247">
<p><em>(Mesa is not a person, it was a Xerox Programming Language)</em>*</p>
</div>
<div class="paragraph data-line-1249">
<p>So a task signal and can keep going active in the Monitor. So between the moment a condition triggers a signal, and the signalled task reacts, we can have something between 1ns and 400 years. We need to check the condition again upon waking in the monitor: the famous test-loop <code>while(!condition) { wait(); }</code> is a characteristic of Mesa Monitors.</p>
</div>
</div>
<div class="sect3 data-line-1251">
<h4 id="monitor_like_patterns_in_rk0">5.5.2. Monitor-like patterns in RK0</h4>
<div class="paragraph data-line-1253">
<p>To appreciate the discussion on Condition Variables that follows, it is worth to show how one would build a Monitor-like pattern for the producer-consumer problem.</p>
</div>
<div class="sect4 data-line-1256">
<h5 id="producer_consumer_solution_with_a_mesa_monitor">5.5.2.1. Producer-Consumer Solution with a Mesa Monitor</h5>
<div class="exampleblock data-line-1258">
<div class="content">
<div class="listingblock data-line-1259">
<div class="content">
<pre class="highlightjs highlight"><code class="language-C hljs" data-lang="C">/* PSEUDO CODE */
/*
UINT currentItemNum : number of items in the buffer
UINT const maxItemNum     : buffer capacity (slots)
RK_MUTEX mutex      : monitor lock
RK_SLEEP_QUEUE waitingItem : tasks waiting for an item
RK_SLEEP_QUEUE waitingSlot : tasks waiting for a slot
*/


/**********************************************
 Producer-Consumer Solution with a Mesa Monitor
***********************************************/


/* MONITOR PROCEDURES */

/* INSERT AN ITEM */

    LOCK(&amp;mutex); /* lock monitor */

    /* condition for producers is having
    free slots */
    BOOL notFull = (currentItemNum &lt; maxItemNum);
    while (!notFull)
    {
        /* is full */
        DISABLE_PREEMPTION(); /* disable scheduler */

        UNLOCK(&amp;mutex);

        WAIT(&amp;waitingSlot);

        DISBLE_PREEMPTION();

    /***********************************************
    !IMPORTANT!: if preemption was allowed and  unlock(&amp;mutex) switched a higher priority task to READY, the active task would be preempted,
    compromising the entire synch logic.
    *************************************************/

        /* when waking acquire mutex again */
        LOCK(&amp;mutex);
    }

    /* buffer has slots, mutex is held */
    depositItem();
    /* increase number of items */
    currentItemNum++;

    /* signal any tasks waiting for an item */
    SIGNAL(&amp;waitingItem);

    /* release monitor and leave */
    UNLOCK(&amp;mutex);
    return;

/* PROCEDURE: EXTRACT AN ITEM */

    LOCK(&amp;mutex); /* lock monitor */
    BOOL notEmpty = (currentItemNum &gt; 0);
    while (!notEmpty)
    {
        /* is empty */
        DISABLE_PREEMPTION();

        UNLOCK(&amp;mutex);

        WAIT(&amp;waitingItem);

        ENABLE_PREEMPTION();

        /* when waking acquire mutex again */
        LOCK(&amp;mutex);
    }
    extractItem();
    /* decrease number of items */
    currentItemNum--;
    /* signal any tasks waiting for a free slot  */
    SIGNAL(&amp;waitingSlot);
    /* release monitor and leave */
    UNLOCK(&amp;mutex);
   return;</code></pre>
</div>
</div>
</div>
</div>
<div class="paragraph data-line-1347">
<p>There is plenty happenning in the above pattern: the correct combination of locking, unlocking, sleeping and signalling under a predicate, is what enforce the correct precedence of tasks accessing a shared resource. This is what a Monitor does.</p>
</div>
<div class="paragraph data-line-1349">
<p>Note that mutex LOCKs are ordered to guarantee a single task owns the <code>currentItemNum</code> variable. Importantly, when signalling a producer that the number of items decreased (or number of slots increased) even if the producer has a higher priority and is dispatched it will block trying to <code>LOCK(&amp;mutex)</code> when resuming within <code>while(!notFull)</code>.</p>
</div>
<div class="paragraph data-line-1351">
<p>The <code>UNLOCK-WAIT</code> sequence within the testing loop has preemption disabled because after releasing the lock, the task cannot be allowed to resume within the monitor again, for any reason that is
not the monitor predicate being satisfied.</p>
</div>
<div class="paragraph data-line-1354">
<p><strong><em>Lost signal?</em></strong></p>
</div>
<div class="paragraph data-line-1356">
<p>What if the task changes the <code>currentItemNum</code> and preempted before being able to signal? Because it is still holding the lock, it will resume within the monitor and with with same conditions when it was preempted.</p>
</div>
<div class="paragraph data-line-1358">
<p>This is how <em>lost wake-ups</em> are prevented.</p>
</div>
</div>
</div>
<div class="sect3 data-line-1360">
<h4 id="condition_variables_in_rk0">5.5.3. Condition Variables in RK0</h4>
<div class="paragraph data-line-1363">
<p>The producer-consumer problem using a Mesa Monitor shown above consists of two procedures (plus initialisation), that encapsulates many synchronisation details.</p>
</div>
<div class="paragraph data-line-1365">
<p>In RK0, any association of a <code>RK_MUTEX</code> and one or more <code>RK_SLEEPING_QUEUE</code> can be treated as condition variables, using the helpers</p>
</div>
<div class="ulist data-line-1367">
<ul>
<li class="data-line-1367">
<p><code>kCondVarWait(&amp;sleepq, &amp;mutex, timeout)</code></p>
</li>
<li class="data-line-1368">
<p><code>kCondVarSignal(&amp;sleepq)</code></p>
</li>
<li class="data-line-1369">
<p><code>kCondVarBroadcast(&amp;sleepq)</code></p>
</li>
</ul>
</div>
<div class="paragraph data-line-1371">
<p>that follows the same semantics of <em>Pthreads Condition Variables</em>, which in turn are aligned with Mesa Monitors.</p>
</div>
<div class="paragraph data-line-1373">
<p>When using the helpers, a testing-loop is written as:</p>
</div>
<div class="listingblock data-line-1375">
<div class="content">
<pre class="highlightjs highlight"><code class="language-C hljs" data-lang="C">  while(!condition)
  {
    /*unlock-sleep, atomic:*/
    kCondVarWait(&amp;sleepq, &amp;mutex, timeout);
    /* when waking it issues lock(&amp;mutex)  */
  }

  /* if here, condition is true and mutex is locked */</code></pre>
</div>
</div>
<div class="admonitionblock note data-line-1389">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
The helper disables preeemption by using <code>kSchLock()</code> and <code>kSchUnlock()</code>. If needing all interrupts disabled, tweak it.
</td>
</tr>
</table>
</div>
<div class="paragraph data-line-1392">
<p>The examples below clarify how Monitor-like schemes can be constructed in RK0.</p>
</div>
<div class="sect4 data-line-1394">
<h5 id="usage_example_synchronisation_barrier">5.5.3.1. Usage Example: Synchronisation Barrier</h5>
<div class="paragraph data-line-1396">
<p>A given number of tasks must reach a point in the program before <em>all can proceed</em>, so every task calls a <code>barrWait(&amp;barrier)</code> to catch up with the set of tasks it must synchronise.</p>
</div>
<div class="paragraph data-line-1398">
<p>The last task entering the barrier will broadcast a signal to all tasks waiting for the wake condition.</p>
</div>
<div class="paragraph data-line-1400">
<p>At any moment within a Monitor a single task is RUNNING (what is an invariant of the kernel), all other tasks within the monitor are either SLEEPING (for some condition) or BLOCKED (on a mutex).</p>
</div>
<div class="exampleblock data-line-1402">
<div class="content">
<div class="listingblock data-line-1403">
<div class="content">
<pre class="highlightjs highlight"><code class="language-C hljs" data-lang="C">/* Synchronisation Barrier */

typedef struct
{
    RK_MUTEX lock;
    RK_SLEEP_QUEUE allSynch;
    UINT count;        /* number of tasks in the barrier */
    UINT round;        /* increased every time all tasks synch     */
    UINT nRequired; /* number of tasks required */



} Barrier_t;

VOID BarrierInit(Barrier_t *const barPtr, UINT nRequired)
{
    kMutexInit(&amp;barPtr-&gt;lock, RK_INHERIT);
    kEventInit(&amp;barPtr-&gt;allSynch);
    barPtr-&gt;count = 0;
    barPtr-&gt;round = 0;
    barPtr-&gt;Required = nRequired;

}

VOID BarrierWait(Barrier_t *const barPtr)
{
    UINT myRound = 0;
    kMutexLock(&amp;barPtr-&gt;lock, RK_WAIT_FOREVER);

    /* save round number */
    myRound = barPtr-&gt;round;
    /* increase count on this round */
    barPtr-&gt;count++;

    if (barPtr-&gt;count == barPtr-&gt;nRequired)
    {
        /* reset counter, inc round, broadcast to sleeping tasks */
        barPtr-&gt;round++;
        barPtr-&gt;count = 0;
        kCondVarBroadcast(&amp;barPtr-&gt;allSynch);
    }
    else
    {
        /* a proper wake signal might happen after inc round */
        while ((UINT)(barPtr-&gt;round - myRound) == 0U)
        {
            RK_ERR err = kCondVarWait(&amp;barPtr-&gt;allSynch, &amp;barPtr-&gt;lock, RK_WAIT_FOREVER);
            kassert(err==RK_SUCCESS);
        }
    }

    kMutexUnlock(&amp;barPtr-&gt;lock);

}


#define N_REQUIRED 3

Barrier_t syncBarrier;

VOID kApplicationInit(VOID)
{

    kassert(!kCreateTask(&amp;task1Handle, Task1, RK_NO_ARGS, "Task1", stack1, STACKSIZE, 2, RK_PREEMPT));
    kassert(!kCreateTask(&amp;task2Handle, Task2, RK_NO_ARGS, "Task2", stack2, STACKSIZE, 3, RK_PREEMPT));
    kassert(!kCreateTask(&amp;task3Handle, Task3, RK_NO_ARGS, "Task3", stack3, STACKSIZE, 1, RK_PREEMPT));
	BarrierInit(&amp;syncBarrier, N_REQUIRED);
}
VOID Task1(VOID* args)
{
    RK_UNUSEARGS
    while (1)
    {
        kPuts("Task 1 is waiting at the barrier...\r\n");
        BarrierWait(&amp;syncBarrier);
        kPuts("Task 1 passed the barrier!\r\n");
		kSleep(8);

    }
}

VOID Task2(VOID* args)
{
    RK_UNUSEARGS
    while (1)
    {
        kPuts("Task 2 is waiting at the barrier...\r\n");
        BarrierWait(&amp;syncBarrier);
        kPuts("Task 2 passed the barrier!\r\n");
		kSleep(5);
	}
}

VOID Task3(VOID* args)
{
    RK_UNUSEARGS
    while (1)
    {
        kPuts("Task 3 is waiting at the barrier...\r\n");
        BarrierWait(&amp;syncBarrier);
        kPuts("Task 3 passed the barrier!\r\n");
        kSleep(3);
	}
}</code></pre>
</div>
</div>
<div class="paragraph data-line-1513">
<p><span class="image"><img src="https://kernel0org.wordpress.com/wp-content/uploads/2025/06/syncbarr.png" alt="syncbarr" width="40%"></span></p>
</div>
</div>
</div>
</div>
<div class="sect4 data-line-1519">
<h5 id="readerswriterslock">5.5.3.2. Usage Example: Readers Writers Lock</h5>
<div class="paragraph data-line-1521">
<p>Several readers and writers share a piece of memory. Readers can concurrently access the memory to read; a single writer is allowed (otherwise, data would be corrupted).</p>
</div>
<div class="paragraph data-line-1523">
<p>When a writer finishes, it checks for any readers waiting. If there is, the writer flushes the readers waiting queue. If not, it wakes a single writer, if any.
When the last reader finishes, it signals a writer.</p>
</div>
<div class="paragraph data-line-1526">
<p>Every read or write operation begins with an acquire and finishes with a release.</p>
</div>
<div class="exampleblock data-line-1529">
<div class="content">
<div class="listingblock data-line-1530">
<div class="content">
<pre class="highlightjs highlight"><code class="language-C hljs" data-lang="C">/* RW-Lock */

/* a single writer is allowed if there are no readers */
/* several readers are allowed if there is no writer*/
typedef struct
{
	RK_MUTEX	 lock;
	RK_SLEEP_QUEUE	 writersGo;
	RK_SLEEP_QUEUE	 readersGo;
	INT			 rwCount; /* number of active readers if &gt; 0 */
						  /* active writer if -1             */

}RwLock_t;

VOID RwLockInit(RwLock_t *const rwLockPtr)
{

	kMutexInit(&amp;rwLockPtr-&gt;lock, RK_INHERIT);
	kEventInit(&amp;rwLockPtr-&gt;writersGo);
	kEventInit(&amp;rwLockPtr-&gt;readersGo);
	rwLockPtr-&gt;rwCount = 0;
}

/* A writer can acquire if  rwCount = 0 */
/* An active writer is indicated by rwCount = -1; */
VOID RwLockAcquireWrite(RwLock_t *const rwLockPtr)
{
	kMutexLock(&amp;rwLockPtr-&gt;lock, RK_WAIT_FOREVER);
	/* if different than 0, there are either writers or readers */
	/* sleep to be signalled */
	while (rwLockPtr-&gt;rwCount != 0)
	{
	    kCondVarWait(&amp;rwLockPtr-&gt;writersGo, &amp;rwLockPtr-&gt;lock, RK_WAIT_FOREVER);
	    /* mutex is locked when waking up*/
	}
	/* woke here, set an active writer */
	rwLockPtr-&gt;rwCount = -1;
	kMutexUnlock(&amp;rwLockPtr-&gt;lock);
}

/* a writer releases, waking up all waiting readers, if any */
/* if there are no readers, a writer can get in */
VOID RwLockReleaseWrite(RwLock_t *const rwLockPtr)
{
	kMutexLock(&amp;rwLockPtr-&gt;lock, RK_WAIT_FOREVER);

	rwLockPtr-&gt;rwCount = 0; /* indicate no writers*/

	/* if there are waiting readers, flush */
	ULONG nWaitingReaders=0;
	kEventQuery(&amp;rwLockPtr-&gt;readersGo, &amp;nWaitingReaders);
	if (nWaitingReaders &gt; 0)
	{
	    /* condVarBroadcast is just an alias for an event flush */
		kEventFlush(&amp;rwLockPtr-&gt;readersGo);
	}
	else
	{
		/* wake up a single writer if any */
		kEventSignal(&amp;rwLockPtr-&gt;writersGo);
	}
	kMutexUnlock(&amp;rwLockPtr-&gt;lock);
}

/* a reader can acquire if there are no writers */
VOID RwLockAcquireRead(RwLock_t *const rwLockPtr)
{
	kMutexLock(&amp;rwLockPtr-&gt;lock, RK_WAIT_FOREVER);
	/* if there is an active writer, sleep */
	while (rwLockPtr-&gt;rwCount &lt; 0)
	{
	    kCondVarWait(&amp;rwLockPtr-&gt;readersGo, &amp;rwLockPtr-&gt;lock, RK_WAIT_FOREVER);
	    /* mutex is locked when waking up*/
	}
	/* increase rwCount, so its &gt; 0, indicating readers */
	rwLockPtr-&gt;rwCount ++;
	kMutexUnlock(&amp;rwLockPtr-&gt;lock);
}

/* a reader releases and wakes a single writer */
/* if it is the last reader */
VOID RwLockReleaseRead(RwLock_t *const rwLockPtr)
{
	kMutexLock(&amp;rwLockPtr-&gt;lock, RK_WAIT_FOREVER);
	rwLockPtr-&gt;rwCount --;
	if (rwLockPtr-&gt;rwCount == 0)
	{
		kEventSignal(&amp;rwLockPtr-&gt;writersGo);
	}
	kMutexUnlock(&amp;rwLockPtr-&gt;lock);
}</code></pre>
</div>
</div>
<div class="paragraph data-line-1626">
<p>In the image below, 4 tasks&#8201;&#8212;&#8201;a fast writer (Task 1), a slow writer (Task 4) and two readers (Task3 is faster than Task2)&#8201;&#8212;&#8201;reading from and writing to a shared UINT variable:</p>
</div>
<div class="imageblock data-line-1628">
<div class="content">
<img src="https://kernel0org.wordpress.com/wp-content/uploads/2025/06/readerwriter-4.png" alt="readerwriter 4" width="30%">
</div>
</div>
</div>
</div>
<hr>
<div class="admonitionblock tip data-line-1634">
<table>
<tr>
<td class="icon">
<i class="fa icon-tip" title="Tip"></i>
</td>
<td class="content">
<div class="paragraph data-line-1636">
<p><em><strong>Message Passing</strong></em></p>
</div>
<div class="ulist data-line-1638">
<ul>
<li class="data-line-1638">
<p>Sending a message can assume 'first-message' semantics (that is, a container where the message is retrieved from eventually gets full not accepting more messages), or 'last-message' semantics, in this case a full container will overwrite the older message that has been deposited.</p>
</li>
<li class="data-line-1640">
<p>Reading can be destructive, that is, a message is consumed  creating an empty space, or non-destructive&#8201;&#8212;&#8201;the message stays there.</p>
</li>
</ul>
</div>
<div class="paragraph data-line-1642">
<p>In real-time applications, Message Passing often encounters the following scenarios:</p>
</div>
<div class="ulist data-line-1644">
<ul>
<li class="data-line-1644">
<p>Some messages are consumed by tasks that can&#8217;t do anything before processing information — thus, these messages end up also being signals. For Example, a server needs (so it blocks) a command to process and/or a client that blocks for an answer.</p>
</li>
<li class="data-line-1646">
<p>A particular case of the above scenario is fully synchronous: client and server run on <em>lockstep</em>.</p>
</li>
<li class="data-line-1648">
<p>Two tasks with different rates need to communicate, and cannot lockstep. A faster producer might use a buffer to accommodate a relatively small burst of generated data, or a quicker consumer will drop repeated received data.</p>
</li>
<li class="data-line-1650">
<p>Other times, we need to correlate data with time for processing, so using a queue gives us the idea of data motion. <em>Eg., when calculating the mean value of a transductor on a given period</em>.</p>
</li>
<li class="data-line-1652">
<p>Past data is useless for <em>real-time</em> tasks such as servo-control loops. Consumers need the most recent data for processing. For example, a drive-by-wire system or a robot deviating from obstacles. In these cases, message passing must be lock-free while guaranteeing data integrity.</p>
</li>
</ul>
</div>
</td>
</tr>
</table>
</div>
<div class="admonitionblock note data-line-1660">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="ulist data-line-1661">
<ul>
<li class="data-line-1661">
<p>The message-passing mechanisms do not reuse any of the mechanisms presented so far.</p>
</li>
<li class="data-line-1663">
<p>The same <code>try</code> semantics applies when using the <code>RK_NO_WAIT</code> timeout option.</p>
</li>
</ul>
</div>
</td>
</tr>
</table>
</div>
</div>
</div>
</div>
<div class="sect2 data-line-1668">
<h3 id="mailbox">5.6. Mailbox</h3>
<table class="tableblock frame-all grid-all stretch data-line-1670">
<colgroup>
<col style="width: 100%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Mailbox Control Block</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Mail Address</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Waiting queue</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Owner Task*</p></td>
</tr>
</tbody>
</table>
<div class="paragraph data-line-1678">
<p>In GPOS jargon, mailboxes are queues of messages—a distinction from pipes (which are byte streams)—but in embedded system software, mailboxes are often said to have a capacity of a single item. More recently, you will not find it as a distinct mechanism—you use a 1-item queue.</p>
</div>
<div class="paragraph data-line-1680">
<p>A Mailbox allows a task to exclusively write (post) and read (pend) a memory region and to be notified when another task writes or reads to it. Therefore, its typical operation provides mutual exclusion and notification: <em>very handy</em>.</p>
</div>
<div class="paragraph data-line-1682">
<p>A message within a mailbox is the address of an object.
The sender and receiver agree on the concrete mail implementation as part of the mail interface contract; also, the data has to remain unchanged until the receiver 'consumes' it.
That is another part of the contract. .</p>
</div>
<div class="paragraph data-line-1686">
<p>The semantics are simple: a Mailbox will be <code>EMPTY</code> when its storage points to <code>NULL</code>; otherwise, it is <code>FULL</code>. It will be <code>EMPTY</code> (FULL) after a successful <code>pend()</code> (<code>post()</code>) operation.</p>
</div>
<div class="paragraph data-line-1688">
<p>When a producer <code>post()</code> to a <code>FULL</code> mailbox, it (optionally) blocks and is placed in the Mailbox waiting queue. The associated task will switch to the state <code>SENDING</code>.</p>
</div>
<div class="paragraph data-line-1690">
<p>Likewise, a consumer (optionally) blocks when issuing a <code>pend()</code> on an empty Mailbox. The task status switches to <code>RECEIVING,</code> and
is enqueued in the mailbox waiting queue.</p>
</div>
<div class="paragraph data-line-1693">
<p><em>* we will discuss ownership on message passing later.</em></p>
</div>
<div class="admonitionblock tip data-line-1696">
<table>
<tr>
<td class="icon">
<i class="fa icon-tip" title="Tip"></i>
</td>
<td class="content">
<div class="paragraph data-line-1697">
<p>Passing Messages by reference is a typical “embedded thing” – because it is cheap, deterministic and DMA-friendly.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="sect3 data-line-1700">
<h4 id="sticky_mailbox">5.6.1. 'Sticky' Mailbox</h4>
<div class="paragraph data-line-1702">
<p>Another way to see a Mailbox is as a container that, once deposited, a message will <em>'stick'</em> until it is overwritten by a producer. The message remains after a read operation. Once written, never empty again.</p>
</div>
<div class="paragraph data-line-1704">
<p>In these cases one would use the <code>peek()</code> and <code>postovw()</code> operations in pair.</p>
</div>
<div class="paragraph data-line-1706">
<p>But probably the cheaper and most-efficient solution will be to hand-roll a mechanism.</p>
</div>
<div class="paragraph data-line-1708">
<p>For 1:N Sticky Mailbox there is the <a href="#mrm">Most-Recent Message Protocol (MRM)</a>.</p>
</div>
<div class="paragraph data-line-1710">
<p>The <a href="#readerswriterslock">Usage Example: Readers Writers Lock</a> shows a generalisation for a M:N case.</p>
</div>
</div>
<div class="sect3 data-line-1713">
<h4 id="example_multi_client_server_synchronous_command_response">5.6.2. Example: Multi-client-server synchronous command-response</h4>
<div class="paragraph data-line-1715">
<p>The snippet below presents <em>two clients</em> and one server on a lock-step communication. It is a (local) procedure call, although the server simply echoes back what has received. Logically this communication is <em>unbuffered</em>.</p>
</div>
<div class="exampleblock data-line-1717">
<div class="content">
<div class="listingblock data-line-1719">
<div class="content">
<pre class="highlightjs highlight"><code class="language-C hljs" data-lang="C">/* This example includes  &lt;string.h&gt; for convenience */

RK_MBOX serverReqMbox; /*  server incoming commands */
RK_MBOX serverAckMbox; /*  server incoming reponse acks */
RK_MBOX clientMbox1;   /*  response for client 1 */
RK_MBOX clientMbox2;   /* response for client 2 */

/* Command Requests are assembled on an Application Data Unit */
typedef struct
{
    BYTE length; /* Length of the APDU payload */
    BYTE payload[32]; /* APDU payload */
    RK_MBOX *replyMbox; /* Pointer to the client's reply mailbox */
} APDU RK_ALIGN(4);

void kApplicationInit(VOID)
{
    kMboxInit(&amp;serverReqMbox,  NULL);
    kMboxInit(&amp;serverAckMbox, NULL);
    kMboxInit(&amp;clientMbox1, NULL);
    kMboxInit(&amp;clientMbox2, NULL);

}

/* Highest Priority */
/* the server response is to ECHO the request back to the client; then, it pends on a mailbox waiting for the client to acknowledge the response. So it proceeds to process further requests.  */

VOID ServerTask(VOID* args)
{
    RK_UNUSEARGS

    APDU *request, response;
    UINT* ackResp;
    while (1)
    {
        /* Wait for a request */
        if (kMboxPend(&amp;serverReqMbox, (VOID **)&amp;request, RK_WAIT_FOREVER) == RK_SUCCESS)
        {
            kprintf("[SERVER] RECV: %s\r\n", request-&gt;payload);

            /* Process the request */
            response.length = (BYTE) snprintf((char*) response.payload,
                    sizeof(response.payload), "ECHO %s",
                    request-&gt;payload);

            /* Echo to client's reply mailbox */
            if (kMboxPost(request-&gt;replyMbox, &amp;response, RK_WAIT_FOREVER) != RK_SUCCESS)
            {
                kprintf("ECHO fail\r\n");
            }
            if (kMboxPend(&amp;serverAckMbox, (VOID **)&amp;ackResp, RK_WAIT_FOREVER) == RK_SUCCESS)
                kprintf("[SERVER] CLIENT %d SERVED.\r\n", *ackResp);
            /* now it is safe to process another request */
        }
    }
}
/* same priority as Client2 */
VOID Client1Task(VOID* args)
{
    RK_UNUSEARGS

    APDU request, *response;

    while (1)
    {
        /* Prepare the request */
        snprintf((char*) request.payload, sizeof(request.payload),
                "Hello from Client 1");
        request.length = (BYTE) strlen((char*) request.payload);
        request.replyMbox = &amp;clientMbox1; /* Specify the reply mailbox */

        /* Send the request to the server */
        if (kMboxPost(&amp;serverReqMbox, &amp;request, RK_WAIT_FOREVER) == RK_SUCCESS)
        {

            /* Wait for the response */
            if (kMboxPend(&amp;clientMbox1, (VOID **)&amp;response, RK_WAIT_FOREVER)
                    == RK_SUCCESS)
            {
                kprintf("[CLIENT #1] RECV: %s\r\n", response-&gt;payload);

                /* we not use a simple direct signal or binary semaphore because the client has to block if the server has a waiting ACK message to waiting to be retrieved */
                UINT ack=1;
                kMboxPost(&amp;serverAckMbox, &amp;ack, RK_WAIT_FOREVER);
                /* now it is safe to send another request */
            }
            else
            {
                kprintf("1F\r\n");
            }
        }
        else
        {
            kprintf("1F\r\n");
        }

    }
}
VOID Client2Task(VOID* args)
{
    RK_UNUSEARGS
    APDU request, *response;

    while (1)
    {
        /* Prepare the request */
        snprintf((char*) request.payload, sizeof(request.payload),
                "Hello from Client 2");
        request.length = (BYTE) strlen((char*) request.payload);
        request.replyMbox = &amp;clientMbox2; /* Specify the reply mailbox */

        /* Send the request to the server */
        if (kMboxPost(&amp;serverReqMbox, &amp;request, RK_WAIT_FOREVER) == RK_SUCCESS)
        {

            /* Wait for the response */
            if (kMboxPend(&amp;clientMbox2, (VOID **)&amp;response, RK_WAIT_FOREVER)
                    == RK_SUCCESS)
            {
                kprintf("[CLIENT #2] RECV: %s\r\n", response-&gt;payload);
                UINT ack=2;
                kMboxPost(&amp;serverAckMbox, &amp;ack, RK_WAIT_FOREVER);
            }
            else
            {
                kprintf("2FAIL\r\n");
            }
        }
        else
        {
            kprintf("2FAIL\r\n");
        }

    }
}</code></pre>
</div>
</div>
</div>
</div>
<div class="imageblock data-line-1858">
<div class="content">
<img src="images/images\clientserver.png" alt="images\clientserver" width="30%">
</div>
</div>
<div class="paragraph data-line-1860">
<p>Had the server not blocked waiting for an ACK, the former response would have been overwritten before a client could have read it, given how priorities are set. To accommodate two clients while still passing by reference, the server would need to keep the response on different buffers.</p>
</div>
<div class="paragraph data-line-1862">
<p>If a copy were passed as a response, the server would not need to block for an ACK, provided the response was sent before receiving another request.</p>
</div>
</div>
</div>
<div class="sect2 data-line-1866">
<h3 id="message_queues">5.7. Message Queues</h3>
<div class="paragraph data-line-1868">
<p>The classic Message Queue on UNIX SVR4 is defined as the 'head of a linked list of messages'. Some RTOSes implement Message Queues using linked lists, in which case a central pool of buffers might exist.</p>
</div>
<div class="paragraph data-line-1870">
<p><em>RK0</em>, two mechanisms are present:</p>
</div>
<div class="ulist data-line-1872">
<ul>
<li class="data-line-1872">
<p>A <em>Mail Queue</em> (or a <em>Queue</em>) is a 'multi-item' Mailbox: it holds multiple generic pointers as messages.</p>
</li>
<li class="data-line-1874">
<p>A <em>Stream Queue</em> (or a <em>Stream</em>) is a ring buffer of N fixed-size messages (word-aligned). <em>Streams perform deep copies</em> - from sender storage to the stream buffer, and from the stream buffer to receiver storage.</p>
</li>
</ul>
</div>
<div class="paragraph data-line-1876">
<p>They are offered as different mechanisms because they have different best-use cases. Still, one probably does not need both, as one can manipulate Mail Queues to pass by copy, and Stream Queues to pass by pointers.</p>
</div>
<div class="sect3 data-line-1879">
<h4 id="mail_queue">5.7.1. Mail Queue</h4>
<table class="tableblock frame-all grid-all stretch data-line-1882">
<colgroup>
<col style="width: 100%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Queue Control Block</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Pointer to Buffer Address</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Write Position</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Read Position</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Max. number of mails</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Current number of mails</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Waiting queue</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Owner Task</p></td>
</tr>
</tbody>
</table>
<div class="paragraph data-line-1893">
<p>Mail Queues (or just <em>Queues</em>) are Mailboxes that can hold several messages in a FIFO queue. Indeed, a Mail Queue with a size of 1 will behave as a Mailbox.</p>
</div>
<div class="paragraph data-line-1895">
<p>The programmer must provide a buffer to hold N message addresses for a Queue. The main primitives are <code>post(), pend(), peek(), and jam().</code></p>
</div>
<div class="paragraph data-line-1897">
<p><em>Peek</em> reads the Queue front message without extracting it, and <em>Jam</em> places a message on the queue front so that this message will be <em>Last-In-First-Out</em>.</p>
</div>
<div class="paragraph data-line-1899">
<p>Mails will be enqueued in a FIFO order (except when using <code>jam()</code>).</p>
</div>
<div class="admonitionblock note data-line-1903">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph data-line-1904">
<p>A single-slot Queue behaves as a Mailbox. Still, Mailboxes are provided as a distinct service from Queues because a Queue Control Block is roughly three times larger than a Mailbox, plus Queue methods are considerably heavier. As mailboxes are extremely handy, providing them as a standalone mechanism allows them to be composed with other features while keeping queues disabled entirely.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph data-line-1907">
<p>For both Mail Queues and Mailboxes, if your message is a 4-byte message&#8201;&#8212;&#8201;e.g., a INT value&#8201;&#8212;&#8201;they can (and probably should) be passed by copy: just cast to (VOID*) when transmitting, and cast back to INT when receiving.</p>
</div>
<div class="sect4 data-line-1909">
<h5 id="usage_example_work_queue">5.7.1.1. Usage Example: Work Queue</h5>
<div class="paragraph data-line-1912">
<p>Multiple producer tasks (Sensor, PID Controller, and UI) create Job_t objects and submit their addresses to a Mail Queue (jobQueue).</p>
</div>
<div class="paragraph data-line-1914">
<p>In this example, the worker thread logs system activity. As it runs on the lowest priority, it maintains system responsiveness with minimal intrusion.</p>
</div>
<div class="paragraph data-line-1916">
<p>The same pattern can support actual processing. You could either embed a function pointer in each job for fully dynamic behaviour or define a command ID and use a central dispatch table in the worker thread to invoke appropriate handlers (loot at <a href="#supervisortask">Usage Example: Supervisor Task</a>).</p>
</div>
<div class="admonitionblock important data-line-1919">
<table>
<tr>
<td class="icon">
<i class="fa icon-important" title="Important"></i>
</td>
<td class="content">
<div class="paragraph data-line-1920">
<p>The buffer for the <em>Mail Queue</em> <em>must</em> be a <code>VOID* buf[N]</code>. The <code>kQueueInit</code> functions takes a
 <code>VOID** bufPPtr</code> paramter to identify the queue memory, and simply <code>buf</code> matches the requested type (or <code>&amp;buf[0]</code>).</p>
</div>
</td>
</tr>
</table>
</div>
<div class="listingblock data-line-1924">
<div class="content">
<pre class="highlightjs highlight"><code class="language-C hljs" data-lang="C">/* Job_t and queue definitions */
#define MAX_JOBS 8

typedef struct
{
    BYTE length;
    BYTE payload[64];
} Job_t;

static Job_t jobPoolBuf[MAX_JOBS]  __attribute__((section("_user_heap")));
static RK_MEM jobPool;

static VOID *jobQueueBuf[MAX_JOBS];

static RK_QUEUE jobQueue;

/* Plant model state */
static volatile float plantTemp = 25.0f;
static const float ambientTemp = 20.0f;

/* Convert plantTemp to integer for logging */
INT readTemp(VOID)
{
    return (INT)plantTemp;
}

/* Simulate button every 2s */
INT buttonPressed(VOID)
{
    return ((kTickGet() % 2000) &lt; 20); /* the condition will hold true for 20ms every 2s */
}

VOID kApplicationInit( VOID)
{
    kMemInit( &amp;jobPool, jobPoolBuf, sizeof(Job_t), MAX_JOBS);

    kQueueInit( &amp;jobQueue, jobQueueBuf, MAX_JOBS);
}

/* PID Controller Task (High priority) */
/* note: this is a sloppy zero-effort tunning
just for printing something */
VOID PIDControllerTask( VOID *args)
{
    RK_UNUSEARGS

    const float Kp=1.0f, Ki=0.1f, Kd=0.05f;
    float prev=plantTemp;
    float integral=0.0f;
    const float dt=0.5f;

    while(1)
    {
        /* Read plant state */
        float measure = plantTemp;
        /* PID compute */
        float error = 25.0f - measure;
        integral += error * dt;
        float derivative = (measure - prev) / dt;
        float output = Kp*error + Ki*integral - Kd*derivative;
        prev = measure;

        /* Apply to plant model */
        /* the plant cooling model: (temp-amb)*0.1  */
        plantTemp += (output - (plantTemp - ambientTemp)*0.1f) * dt;

        /* Post log job */

        Job_t *jobPtr = kMemAlloc( &amp;jobPool);
        if(jobPtr)
        {
            CHAR buf[32];

            /* this format floats for printf not supporting float  */
            formatFloat (buf, sizeof(buf), output);

            snprintf( (CHAR*)jobPtr-&gt;payload,
            sizeof(jobPtr-&gt;payload),
                     "[CTRL] O=%s T=%d", buf, readTemp());
            jobPtr-&gt;length = strlen((CHAR*)jobPtr-&gt;payload);

            if(kQueuePost( &amp;jobQueue, job, RK_NO_WAIT) != RK_SUCCESS)
            {
            /*as the worker thread is freeing the memory blocks
             if the queue is full and we do not want to block
             we free the allocated memory; otherwise, it would leak
            */
                kMemFree( &amp;jobPool, job);
            }
        }
        kSleepPeriodic(500);
    }
}

/* Sensor Task (Mid priority) */
VOID TempSensorTask( VOID *args)
{
    RK_UNUSEARGS
    while(1)
    {
        Job_t *jobPtr = kMemAlloc(&amp;jobPool);
        if(jobPtr)
        {
            snprintf( (CHAR*)jobPtr-&gt;payload, sizeof(jobPtr-&gt;payload),
                     "[SENSOR] T=%dC", readTemp());
            jobPtr-&gt;length = strlen((CHAR*)jobPtr-&gt;payload);
            if(kQueuePost( &amp;jobQueue, jobPtr, RK_NO_WAIT) != RK_SUCCESS)
            {
                kMemFree( &amp;jobPool, jobPtr);
            }
        }
        kSleepPeriodic(1000);
    }
}

/* UI Task (Low priority) */
/* this is to cause a temperature disturbance */
VOID UIButtonTask( VOID *args)
{
    RK_UNUSEARGS
    while(1)
    {
        if(buttonPressed())
        {
            plantTemp -= plantTemp*0.15f  /* disturb the temperature */
            Job_t *jobPtr = kMemAlloc( &amp;jobPool);
            if(jobPtr)
            {
                snprintf((CHAR*)jobPtr-&gt;payload, sizeof(jobPtr-&gt;payload),
                         "[BTN] Temp: %d", (INT)plantTemp);
                jobPtr-&gt;length = strlen((CHAR*)jobPtr-&gt;payload);
                if(kQueuePost( &amp;jobQueue, jobPtr, RK_NO_WAIT) != RK_SUCCESS)
                {
                    kMemFree( &amp;jobPool, jobPtr);
                }
            }
        }
        kSleepPeriodic( 2000);
    }
}

/* Worker Task (Lowest priority) */
VOID WorkerTask( VOID *args)
{
    RK_UNUSEARGS
    Job_t *jobPtr = NULL;
    while(1)
    {
        if(kQueuePend( &amp;jobQueue, (VOID**)&amp;jobPtr, RK_WAIT_FOREVER)==RK_SUCCESS)
        {
            printf("[WORKER] %s\r\n", jobPtr-&gt;payload);
            kMemFree( &amp;jobPool, jobPtr);
        }
    }
}</code></pre>
</div>
</div>
<div class="imageblock data-line-2083">
<div class="content">
<img src="images/images/workqueue.png" alt="workqueue" width="25%">
</div>
</div>
</div>
<div class="sect4 data-line-2088">
<h5 id="a_logger_pattern">5.7.1.2. A Logger Pattern</h5>
<div class="paragraph data-line-2091">
<p>This is the <code>logPost(&#8230;&#8203;)</code> utility that is being used in some examples.</p>
</div>
<div class="listingblock data-line-2093">
<div class="content">
<pre class="highlightjs highlight"><code class="language-C hljs" data-lang="C">#include &lt;stdio.h&gt; /* needed for printf */
#include &lt;stdarg.h&gt; /* needed for va_args */
#include &lt;kstring.h&gt; /* if including &lt;string.h&gt; RK_* custom string operations are remapped to string.h */

/* this version uses printf. one could just store data on the queue to inspect */

#define LOGLEN     64
#define LOGBUFSIZ  16

 struct log
 {
     RK_TICK   t;
     CHAR      s[LOGLEN];
 }RK_ALIGN(4);

typedef struct log Log_t;

/* memory partition pool */
Log_t  qMemBuf[LOGBUFSIZ]  __K_HEAP;

/* buffer for the mail queue */
 VOID  *qBuf[LOGBUFSIZ];

/* mail queue */
 RK_QUEUE    logQ;

/* mem allocator */
 RK_MEM      qMem;


/* (v)printf needs a custom _write() backend syscall, typically using UART */
 void kprintf(const char *fmt, ...)
 {
         va_list args;
         va_start(args, fmt);
         /* printing to stderr; if there is no particular stderr (fd==2) redirection on _write(), this has the same effect of vprintf(fmt, args), using stdout (fd==1) */
         vfprintf(stderr, fmt, args);
         va_end(args);
 }


 VOID logPost(const char *fmt, ...)
 {
     kSchLock();
     Log_t *logPtr = (Log_t*)kMemAlloc(&amp;qMem);
     if (logPtr)
     {
         logPtr-&gt;t = kTickGetMs();
         va_list args;
         va_start(args, fmt);
         int len = vsnprintf(logPtr-&gt;s, sizeof(logPtr-&gt;s), fmt, args);
         va_end(args);
         /* if len &gt;= size of the message it has been truncated */
         if (len &gt;= (int)sizeof(logPtr-&gt;s))
         {
             /* add  "..." to replace" where the truncation happend */
             if (len &gt; 4)
                 RK_STRCPY(&amp;logPtr-&gt;s[len - (int)4], "...");
         }
         /* if queue post fails, free memory so it doesnt leak */
         if (kQueuePost(&amp;logQ, (VOID*)logPtr, RK_NO_WAIT) != RK_SUCCESS)
             kMemFree(&amp;qMem, logPtr);
     }
     kSchUnlock();
 }


/* you can make it as a module and publish the logPost API  */

/* Usage Example: */

VOID someTask(VOID* args)
{
  ...

  while(1)
  {

    UINT value = getValue();
    logPost("Value is %d", value);

   }

}

/* If printing to a terminal, use a low-priority LogTask: */
VOID LogTask(void* args)
 {
     (void)args;
     while (1)
     {
         VOID* recvPtr=NULL;
         if (kQueuePend(&amp;logQ, &amp;recvPtr, RK_WAIT_FOREVER) == RK_SUCCESS)
         {
             kassert(recvPtr!=NULL);
             Log_t* logPtr = (Log_t*)recvPtr;
             kprintf("%lu ms :: %s\r\n", logPtr-&gt;t, logPtr-&gt;s);
             kMemFree(&amp;qMem, recvPtr);
         }
     }
 }</code></pre>
</div>
</div>
</div>
</div>
<div class="sect3 data-line-2198">
<h4 id="stream_queue">5.7.2. Stream Queue</h4>
<table class="tableblock frame-all grid-all stretch data-line-2200">
<colgroup>
<col style="width: 100%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Message Stream Control Block</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Storage address</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Write Address</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Read Address</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Message Block Size</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Max of messages</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Message Count</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Waiting Queue</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Owner Task</p></td>
</tr>
</tbody>
</table>
<div class="paragraph data-line-2211">
<p>Streams resemble classic (named) Pipes. The difference is that messages have <em>a fixed size</em>. On the other hand, pipes transmit and receive any number of bytes for each operation.</p>
</div>
<div class="paragraph data-line-2213">
<p>For each Stream, the user provides a buffer address with enough capacity (number of messages <em>x</em> message size). Then, the kernel will handle it as a ring buffer.</p>
</div>
<div class="paragraph data-line-2215">
<p>The message size associated with a Message Stream instance is defined at its initialisation. On transmission, a message is _ deep copied _ from the sender&#8217;s storage to the queue; on reception, it moves from the queue to the receiver&#8217;s storage.</p>
</div>
<div class="admonitionblock note data-line-2218">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
Although a message size is associated with a Stream Queue object, the concrete message type depends on the application.
</td>
</tr>
</table>
</div>
<div class="paragraph data-line-2221">
<p>The important primitives for Message Streams are <code>send()</code>, <code>recv()</code>, <code>jam()</code> and <code>peek()</code>.</p>
</div>
<div class="paragraph data-line-2223">
<p>Sending to a full queue (optionally) blocks the sender. Likewise, receiving from an empty queue.</p>
</div>
<div class="sect4 data-line-2225">
<h5 id="stream_message_size">5.7.2.1. Stream Message-Size</h5>
<div class="paragraph data-line-2227">
<p><strong>Stream Queues must have <em>fixed</em> message-sizes multiples of a <em>WORD</em>. Besides, they must be a power-of-two: 1, 2, 4, 8, 16, 32&#8230;&#8203; (words).</strong></p>
</div>
<div class="paragraph data-line-2229">
<p><em>RK0</em> does not establish an upper bound, although I would say that a good cap is 4 words for the regular <em>RK0</em> target. One has to experiment, though. If a message becomes too large, it introduces prohibitive latency, so the user needs to transmit the message address—i.e., configure the Stream to carry a 1-word message size.</p>
</div>
<div class="ulist data-line-2231">
<ul>
<li class="data-line-2231">
<p>Load/Store instructions are optimised to fetch 32-bit words. If message sizes are bounded on a 4-byte boundary, these operations can be executed in a single cycle.</p>
</li>
<li class="data-line-2233">
<p>A power-of-two constraint is a CPU-aware design choice to prevent unalignment issues.</p>
</li>
<li class="data-line-2235">
<p>Misaligned memory makes castings unsafe, leading to complex faults, performance penalties or undefined behaviour.</p>
</li>
</ul>
</div>
<div class="admonitionblock tip data-line-2240">
<table>
<tr>
<td class="icon">
<i class="fa icon-tip" title="Tip"></i>
</td>
<td class="content">
<div class="paragraph data-line-2241">
<p>Deep Copies are usually needed for message passing but introduce significant overhead.</p>
</div>
<div class="paragraph data-line-2243">
<p>Design choice: Be CPU-aware and constrain data size to power of two words.</p>
</div>
<div class="paragraph data-line-2245">
<p>Benefits: speeds up the copy, achieves more deterministic behaviour, improves run-time safety.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph data-line-2248">
<p>Code-wise, we optimise using pointer arithmetics on pointer to words:</p>
</div>
<div class="exampleblock data-line-2249">
<div class="content">
<div class="listingblock data-line-2250">
<div class="content">
<pre class="highlightjs highlight"><code class="language-C hljs" data-lang="C">/* Optimised deep copy; guaranteed mesgSize&gt;0 */
/* destPtr and srcPtr are pointers to a word */
#define RK_CPY(destPtr, srcPtr, mesgSize) \
do {                                   \
      while (--mesgSize)               \
      {                                \
     /* if mesgSize is 1, this is NOT executed */
        *(destPtr++) = *(srcPtr++)     \
      };                               \
     /* the last or the only copy is executed now */
     *(destPtr++) = *(srcPtr++)       \
   } while(0U)</code></pre>
</div>
</div>
</div>
</div>
<div class="admonitionblock tip data-line-2269">
<table>
<tr>
<td class="icon">
<i class="fa icon-tip" title="Tip"></i>
</td>
<td class="content">
<div class="paragraph data-line-2270">
<p><em>Stream Queues</em> are a way less error-prone than Mail Queues, given the intensive pointer handling performed with Mail Queues.</p>
</div>
</td>
</tr>
</table>
</div>
</div>
<div class="sect4 data-line-2273">
<h5 id="usage_example_averaging_sensor_values">5.7.2.2. Usage Example: Averaging Sensor Values</h5>
<div class="paragraph data-line-2275">
<p>Below is an illustrative snippet of a <em>Queueing Pattern</em>.</p>
</div>
<div class="paragraph data-line-2277">
<p>The goal is to calculate the average value of 4 types of sensors.</p>
</div>
<div class="paragraph data-line-2279">
<p>It is convenient to highlight an important aspect here: Given its reactive nature, real-time system software is typically <em>I/O bounded</em>. Tasks that are sensitive to I/O activity have higher priority than <em>CPU-bounded</em> tasks, i.e., those processing data.</p>
</div>
<div class="paragraph data-line-2281">
<p>A task receives measured sensor values from an ISR on a periodic rate. (A Soft Timer emulates the ISR).</p>
</div>
<div class="paragraph data-line-2283">
<p>Then it enqueues this data to a consumer - that will process the average value for each of 4 sensors.</p>
</div>
<div class="paragraph data-line-2285">
<p>The inter-task communication is designed as follows:</p>
</div>
<div class="olist arabic data-line-2287">
<ol class="arabic">
<li class="data-line-2287">
<p>The producer pends on a Mailbox that an ISR posts to. An application timer emulates this ISR.</p>
</li>
<li class="data-line-2289">
<p>The data extracted from the Mailbox is placed in a queue with the processing task as the consumer.</p>
</li>
<li class="data-line-2291">
<p>As the producer&#8217;s priority must be higher than that of the consumer, eventually, the queue will get full.</p>
</li>
<li class="data-line-2293">
<p>The producer drops the last message when the queue is full and signals the consumer.</p>
</li>
<li class="data-line-2295">
<p>Now the consumer has a batch of data to work until the next sensor update. It will block (pend on a signal) whenever the queue is empty.</p>
</li>
</ol>
</div>
<div class="paragraph data-line-2298">
<p>Here, the queue size was set at 8 items. This is an arbitrary value; the optimal queue size would take into account system edge cases: 'What is the state that represents the most inconvenient time to be interrupted by this sensor?'</p>
</div>
<div class="exampleblock data-line-2301">
<div class="content">
<div class="listingblock data-line-2302">
<div class="content">
<pre class="highlightjs highlight"><code class="language-c hljs" data-lang="c">#define kPend(timeout)                                \
    do                                                \
    {                                                 \
        kSignalGet(0x1, RK_FLAGS_ANY, NULL, timeout); \
    } while (0)

#define kSignal(taskhandle)           \
    do                                \
    {                                 \
        kSignalSet(taskhandle, 0x01); \
    } while (0)



typedef enum
{
	TEMPERATURE=1, HUMIDITY, CO2, FLOW
}SensorType_t;



/* sensor types */
struct sensorMsg
{
    SensorType_t sensorType;
    ULONG sensorValue;

};

typedef struct sensorMsg Mesg_t;

#define N_MESSAGE 8
#define MESSAGE_SIZE (sizeof(Mesg_t) + sizeof(ULONG) - 1)/(sizeof(ULONG) /* WORDS! */
#define N_SENSOR    4
#define AVG_WINDOW_SIZE   10 /* 10 samples */

RK_STREAM sensorStream;/* the stream kobject */
Mesg_t mesgBuf[N_MESSAGE] = {0};/* queue buffer */
RK_TIMER timerT1;
RK_MBOX sensorBox;
static Mesg_t sample = {0};
static UINT sampleErr;

VOID callBackISR(VOID* ARGS);

VOID kApplicationInit( VOID)
{
    RK_ERR err = kStreamInit(&amp;sensorStream, (VOID*) mesgBuf, MESSAGE_SIZE,
    N_MESSAGE);

    kassert(err==RK_SUCCESS);

    /* timer @ every 10 ms */
    err = kTimerInit(&amp;timerT1, 0, 10, callBackISR, NULL, RK_TIMER_RELOAD);
    kassert( err==RK_SUCCESS);

    err = kMboxInit(&amp;sensorBox, NULL);
    kassert( err==RK_SUCCESS);
}

VOID callBackISR(VOID *args)
{
    RK_UNUSEARGS
    sample.sensorType = (rand() % 4) + 1;
    switch (sample.sensorType)
    {
        case TEMPERATURE:
            sample.sensorValue = ( ULONG) rand() % 50;
            break;
        case HUMIDITY:
            sample.sensorValue = ( ULONG) rand() % 100;
            break;
        case CO2:
            sample.sensorValue = ( ULONG) rand() % 1000;
            break;
        case FLOW:
            sample.sensorValue = ( ULONG) rand() % 10;
            break;
        default:
            break;
    }
    RK_ERR err = kMboxPost( &amp;sensorBox, &amp;sample, RK_NO_WAIT);
    if (err != RK_SUCCESS)
        sampleErr ++;

}

/* Producer - higher priority, blocks on mailbox */
VOID Task1(VOID *args)
{
    RK_UNUSEARGS
    Mesg_t *recvSample = NULL;
    while (1)
    {
        RK_ERR errmbox = kMboxPend( &amp;sensorBox, ( VOID**) &amp;recvSample,
                RK_WAIT_FOREVER);
        kassert( errmbox==RK_SUCCESS);
        RK_ERR err = kStreamSend( &amp;sensorStream, &amp;sample, RK_NO_WAIT);
        kassert(err &gt;= 0); /* either succesful or unsuccesful */
        if (err == RK_SUCCESS)
        {
            CHAR const *sensorTypeStr = NULL;
            if (recvSample-&gt;sensorType == 1)
                sensorTypeStr = "TEMP";
            if (recvSample-&gt;sensorType == 2)
                sensorTypeStr = "HUM";
            if (recvSample-&gt;sensorType == 3)
                sensorTypeStr = "CO2";
            if (recvSample-&gt;sensorType == 4)
                    sensorTypeStr = "FLOW";
            printf( "ENQ: [@%lums, %s, %lu] \r\n", kTickGet(), sensorTypeStr,
                        recvSample-&gt;sensorValue);
        }
        else if (err == RK_ERR_STREAM_FULL)
        {
            kSignal(task2Handle);
        }
    }
}

/* for each sensor:
     . a ring buffer of AVG_WINDOW_SIZE values
 . sum of values
 . an index table (=enum - 1 eg., HUMIDITY IDX=2-1=1)
 */
static ULONG ringBuf[N_SENSOR][AVG_WINDOW_SIZE];
static ULONG ringSum[N_SENSOR] = {0};
static UINT ringIndex[N_SENSOR] = {0};

void Task2( void *args)
{

    RK_UNUSEARGS
    Mesg_t readSample;
    while (1)
    {
        RK_ERR err = kStreamRecv(&amp;sensorStream, (VOID*)&amp;readSample,
        RK_NO_WAIT);
        if (err == RK_SUCCESS)
        {
            UINT sensorIdx = readSample.sensorType - 1;

/* remove oldest sample */
            ULONG oldest = ringBuf[sensorIdx][ringIndex[sensorIdx]];
            ringSum[sensorIdx] -= oldest;

/* push new sample */
            ringBuf[sensorIdx][ringIndex[sensorIdx]] = readSample.sensorValue;
            ringSum[sensorIdx] += readSample.sensorValue;

/* index incr-wrap */
            ringIndex[sensorIdx] ++;
            ringIndex[sensorIdx] %= AVG_WINDOW_SIZE;

/* simple average */
            ULONG avg = ringSum[sensorIdx] / AVG_WINDOW_SIZE;


            CHAR const *sensorTypeStr = NULL;
            if (readSample.sensorType == 1)
                sensorTypeStr = "TEMP";
            if (readSample.sensorType == 2)
                sensorTypeStr = "HUM";
            if (readSample.sensorType == 3)
                sensorTypeStr = "CO2";
            if (readSample.sensorType == 4)
                sensorTypeStr = "FLOW";

            printf( "DEQ: [@%lums, %s, %lu] | AVG: %lu \r\n", kTickGet(),
                    sensorTypeStr, readSample.sensorValue, avg);

        }
        else
        {
            kPend(RK_WAIT_FOREVER);
        }

    }
}</code></pre>
</div>
</div>
<div class="listingblock data-line-2487">
<div class="content">
<pre class="highlightjs highlight"><code class="language-none hljs">OUTPUT:

ENQ: [@550ms, CO2, 571]
ENQ: [@560ms, FLOW, 4]
ENQ: [@570ms, FLOW, 4]
ENQ: [@580ms, HUM, 25]
ENQ: [@590ms, CO2, 931]
ENQ: [@600ms, CO2, 487]
ENQ: [@610ms, FLOW, 7]
ENQ: [@620ms, HUM, 79]

&gt;&gt;&gt; Queue is full. Now offload and process. Note the order remains &lt;&lt;&lt;

DEQ: [@630ms, CO2, 571] | AVG: 460
DEQ: [@631ms, FLOW, 4] | AVG: 5
DEQ: [@632ms, FLOW, 4] | AVG: 5
DEQ: [@633ms, HUM, 25] | AVG: 52
DEQ: [@634ms, CO2, 931] | AVG: 553
DEQ: [@635ms, CO2, 487] | AVG: 549
DEQ: [@636ms, FLOW, 7] | AVG: 5
DEQ: [@637ms, HUM, 79] | AVG: 55

&gt;&gt;&gt; Consumer is preempted &lt;&lt;&lt;
ENQ: [@640ms, CO2, 913]
ENQ: [@650ms, CO2, 134]
ENQ: [@660ms, HUM, 47]
ENQ: [@670ms, HUM, 30]
ENQ: [@680ms, TEMP, 7]
ENQ: [@690ms, CO2, 726]
ENQ: [@700ms, FLOW, 7]
ENQ: [@710ms, TEMP, 43]

DEQ: [@720ms, CO2, 913] | AVG: 578
DEQ: [@721ms, CO2, 134] | AVG: 543
DEQ: [@722ms, HUM, 47] | AVG: 51
DEQ: [@723ms, HUM, 30] | AVG: 44
DEQ: [@724ms, TEMP, 7] | AVG: 20
DEQ: [@725ms, CO2, 726] | AVG: 592
DEQ: [@726ms, FLOW, 7] | AVG: 5
DEQ: [@727ms, TEMP, 43] | AVG: 23</code></pre>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="sect3 data-line-2535">
<h4 id="summing_up_stream_queues_vs_mail_queues">5.7.3. Summing Up: Stream Queues vs Mail Queues</h4>
<div class="paragraph data-line-2537">
<p>The table below summarises how Stream and Mail Queues differ:</p>
</div>
<table class="tableblock frame-all grid-all stretch data-line-2540">
<colgroup>
<col style="width: 30%;">
<col style="width: 35%;">
<col style="width: 35%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Feature</th>
<th class="tableblock halign-left valign-top">Mail Queue (Pointer-Based)</th>
<th class="tableblock halign-left valign-top">Stream Queue (Deep Copy-Based)</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Message Storage</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Stores <strong>pointers</strong> to messages</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Stores <strong>deep copies</strong> of messages</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Message Size</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">4-byte. Typical use case is to transmit the address of a message.</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Fixed (defined at queue initialisation).</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Memory Management</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Typically used along with a partition pool, for zero-copy or one-copy message passing.</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Static buffer, N-words/message. N is a power of 2.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Data Ownership</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Sender/receiver manage lifecycle</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Kernel.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Performance</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">A 'zero-copy' transmission is faster.</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Deterministic. Kernel Optimised deep-copy.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Best Use Cases</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Work Queues, Client-Server with dynamic payload, any case where zero-copy or 1-copy is feasible</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Real-time data streaming (e.g., sensor pipelines, inter-device communication).</p></td>
</tr>
</tbody>
</table>
</div>
<div class="sect3 data-line-2552">
<h4 id="installing_notify_callbacks">5.7.4. Installing Notify Callbacks</h4>
<div class="paragraph data-line-2554">
<p><em>Mailboxes, Mail and Stream Queues</em> allow installing callbacks to notify send/receive operations.</p>
</div>
<div class="paragraph data-line-2556">
<p>A callback has the signature <code>VOID cbk(&lt;RK_(MBOX/QUEUE/STREAM&gt; *)</code>.</p>
</div>
<div class="sect4 data-line-2558">
<h5 id="usage_example_queue_select">5.7.4.1. Usage Example: Queue Select</h5>
<div class="paragraph data-line-2560">
<p>It is not uncommon to have a gatekeeper or supervisor task listening to several queues at once.</p>
</div>
<div class="paragraph data-line-2562">
<p>In this snippet, a <code>sendNotify</code> is installed on for each queue that signal the supervisor task. This task runs every 100ms coalescing about two <code>post</code> of each mail queue.</p>
</div>
<div class="exampleblock data-line-2563">
<div class="content">
<div class="listingblock data-line-2564">
<div class="content">
<pre class="highlightjs highlight"><code class="language-C hljs" data-lang="C">/* Notify Callback on Queues */

/* each queue has registered this send callback */
VOID sendNotify(RK_QUEUE *qPtr)
{
    UINT i = 0;
    for (i = 0; i &lt; 3; ++i)
    {
        if (queues[i] == qPtr)
        {
            ULONG qFlag = 1UL &lt;&lt; i;
            kSignalSet(superHandle, qFlag);
            break;
        }
    }
}

/* tasks sending follow this pattern */

VOID Task2(VOID *args)
{
    RK_UNUSEARGS

    UINT num = 0x20;

    while (1)
    {
        MESG_t *ptr;
        ptr = (MESG_t *)kMemAlloc(&amp;queueMem);
        if (ptr)
        {
            ptr-&gt;num = num++;
            ptr-&gt;senderID = RK_RUNNING_PID;
            RK_ERR err = kQueuePost(&amp;queue2, (VOID *)ptr, RK_NO_WAIT);
            if (err != RK_SUCCESS)
            {
                kMemFree(&amp;qMem, (VOID *)ptr);
                kprintf("Q2 FULL\n\r");
            }
         }
        kSleepPeriodic(50);
    }
}

/* supervisor listening on 3 queues */

/*
although it has a higher period, it has also the highest priority, what is acceptable for supervisors
you see the tasks posting to queues are keeping its two succesfull posts the supervisor drains
every 100ms
*/

VOID SupervisorTask(VOID *args)
{    RK_UNUSEARGS

    static ULONG gotFlags = 0UL;

    while (1)
    {
        gotFlags = 0UL;
        kSignalGet(0x7, RK_FLAGS_ANY, &amp;gotFlags, RK_NO_WAIT);

        UINT k = 0;

        /* we drain each queue that is set */
        for (k = 0; k &lt; 3; ++k)
        {
            if (gotFlags &amp; (1UL &lt;&lt; k))
            {
                VOID *recvPtr = NULL;

                while (
                    kQueuePend(queues[k], &amp;recvPtr,
                               RK_NO_WAIT) == RK_SUCCESS)
                {
                    MESG_t *m = (MESG_t*)recvPtr;
                    /*1-copy message passing */
                    UINT id = m-&gt;senderID;
                    UINT num = m-&gt;num;

                    kMemFree(&amp;queueMem, recvPtr);

                    UINT sel = k + 1; /* bit position */
                    logPost(" sel: %d, senderID: %d, payload: 0x%02X", sel, id, num);

                }
            }
        }
        kSleepPeriodic(100);
    }
}</code></pre>
</div>
</div>
<div class="listingblock data-line-2657">
<div class="content">
<pre class="highlightjs highlight"><code class="language-none hljs">OUTPUT:
36500 ms ::  sel: 1, senderID: 5, payload: 0x2E8
36500 ms ::  sel: 1, senderID: 5, payload: 0x2E9
36500 ms ::  sel: 2, senderID: 3, payload: 0x2F8
36500 ms ::  sel: 2, senderID: 3, payload: 0x2F9
36500 ms ::  sel: 3, senderID: 4, payload: 0x308
36500 ms ::  sel: 3, senderID: 4, payload: 0x309
36600 ms ::  sel: 1, senderID: 5, payload: 0x2EA
36600 ms ::  sel: 1, senderID: 5, payload: 0x2EB
36600 ms ::  sel: 2, senderID: 3, payload: 0x2FA
36600 ms ::  sel: 2, senderID: 3, payload: 0x2FB
36600 ms ::  sel: 3, senderID: 4, payload: 0x30A
36600 ms ::  sel: 3, senderID: 4, payload: 0x30B
36700 ms ::  sel: 1, senderID: 5, payload: 0x2EC
36700 ms ::  sel: 1, senderID: 5, payload: 0x2ED
36700 ms ::  sel: 2, senderID: 3, payload: 0x2FC
36700 ms ::  sel: 2, senderID: 3, payload: 0x2FD
36700 ms ::  sel: 3, senderID: 4, payload: 0x30C
36700 ms ::  sel: 3, senderID: 4, payload: 0x30D
36800 ms ::  sel: 1, senderID: 5, payload: 0x2EE
36800 ms ::  sel: 1, senderID: 5, payload: 0x2EF
36800 ms ::  sel: 2, senderID: 3, payload: 0x2FE
36800 ms ::  sel: 2, senderID: 3, payload: 0x2FF
36800 ms ::  sel: 3, senderID: 4, payload: 0x30E
36800 ms ::  sel: 3, senderID: 4, payload: 0x30F
36900 ms ::  sel: 1, senderID: 5, payload: 0x2F0
36900 ms ::  sel: 1, senderID: 5, payload: 0x2F1
36900 ms ::  sel: 2, senderID: 3, payload: 0x300</code></pre>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="sect2 data-line-2690">
<h3 id="message_passing_ownership">5.8. Message Passing ownership</h3>
<div class="paragraph data-line-2693">
<p>Using queues to communicate between multiple tasks is chaos. Many senders to many receivers can be unpredictable. We often want N:1 (senders:receiver, N can be 1). This <em>1</em> makes it easier to reason about the dynamics.</p>
</div>
<div class="paragraph data-line-2695">
<p>In real-time design, we often expect to see blocking <em>send()</em> operations on 1:1 or N:1 channels—a blocking <em>send()</em> on a 1:N (broadcast) would be very odd.</p>
</div>
<div class="sect3 data-line-2697">
<h4 id="priority_inversion_on_message_passing">5.8.1. Priority Inversion on Message Passing</h4>
<div class="admonitionblock tip data-line-2700">
<table>
<tr>
<td class="icon">
<i class="fa icon-tip" title="Tip"></i>
</td>
<td class="content">
<div class="paragraph data-line-2701">
<p>Priority Inversion happens on Message-Passing for similar but subtly different reasons from resource sharing.</p>
</div>
<div class="paragraph data-line-2703">
<p><em>Design Choice</em>: Add an ownership mechanism for a message-passing object—a well-defined receiver—so priority propagation can be applied.</p>
</div>
<div class="paragraph data-line-2705">
<p>Benefit: This preserves strict real-time guarantees, making sure a high-priority task never waits indefinitely for a lower-priority task to finish message operations</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph data-line-2708">
<p>While sharing some similarities, there are subtle differences between blocking on a shared resource (by blocking on a locked mutex) and blocking on a message-passing object.</p>
</div>
<div class="paragraph data-line-2710">
<p><em>Assuming cases we do not want messages to be overwritten</em>, a sender, when accessing a queue, is acquiring an empty buffer. A receiver is acquiring a full buffer. They are competing for the same object but in different states. Thus, they depend on each other to change the object state.</p>
</div>
<div class="paragraph data-line-2713">
<p>When a sender blocks on a full shared message object, it does not mean another writer is using the resource; By design it is also unlikely there is a reader blocked on the waiting queue of the object, since every time a write operation completes, any reader blocked on the queue is readied. Whether it is dispatched or not is a scheduler concern. If its priority is higher than the task just finished, it will be immediately dispatched. If not, it is enqueued in the ready queue until it is eventually picked up.</p>
</div>
<div class="admonitionblock tip data-line-2716">
<table>
<tr>
<td class="icon">
<i class="fa icon-tip" title="Tip"></i>
</td>
<td class="content">
This means the priority inversion problem arises from waiting for the consumer rather than from direct contention among multiple senders.
</td>
</tr>
</table>
</div>
<div class="paragraph data-line-2718">
<p>So, if the sender&#8217;s priority is higher, it could be propagated to the reader. But, <em>which</em> reader? (This is why semaphores cannot implement priority inheritance protocol&#8201;&#8212;&#8201;the waiter task cannot know a potential signaller).</p>
</div>
<div class="paragraph data-line-2720">
<p>With that in mind, there is the option to set <em>ownership</em>: <code>set owner (mesgpass, task handle)</code>. From now on, only the owner task can receive from that service object—a blocking <code>send()</code> knows the target task and can raise its priority.</p>
</div>
<div class="paragraph data-line-2722">
<p>(As 1:N communication normally non-blocking on real-time systems, there is no mechanism to establish 'sender ownership'.)</p>
</div>
<div class="paragraph data-line-2724">
<p>If another task that is not the owner tries to receive from a kernel message-passing object that has an owner, it fails.</p>
</div>
<div class="paragraph data-line-2726">
<p>These kernel objects now will resemble an aspect of Ports, a common way of representing tasks on message-passing kernels. (Strictly, they are not Ports,  as RK0 is not a message-passing kernel—although I do like the approach.)</p>
</div>
</div>
</div>
<div class="sect2 data-line-2729">
<h3 id="mrm">5.9. Most-Recent Message Protocol (MRM)</h3>
<table class="tableblock frame-all grid-all stretch data-line-2732">
<colgroup>
<col style="width: 100%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">MRM Control Block</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">MRM Buffer Allocator</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Data Buffer Allocator</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Current MRM Buffer Address</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Data Size (Message Size)</p></td>
</tr>
</tbody>
</table>
<table class="tableblock frame-all grid-all stretch data-line-2741">
<colgroup>
<col style="width: 100%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">MRM Buffer</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Data Buffer Address</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Readers Count</p></td>
</tr>
</tbody>
</table>
<table class="tableblock frame-all grid-all stretch data-line-2749">
<colgroup>
<col style="width: 100%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Data Buffer</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><em>Application-dependent</em></p></td>
</tr>
</tbody>
</table>
<div class="admonitionblock tip data-line-2755">
<table>
<tr>
<td class="icon">
<i class="fa icon-tip" title="Tip"></i>
</td>
<td class="content">
<div class="paragraph data-line-2756">
<p>There is little practical difference between a message that does not arrive and one with no valid (stale) data. But when wrong (or stale) data is processed - e.g., to define a set point on a loop - a system can fail badly.</p>
</div>
<div class="paragraph data-line-2758">
<p>Design Choice: provide a broadcast asynchronous message-passing scheme that guarantees data freshness and integrity for all readers.</p>
</div>
<div class="paragraph data-line-2760">
<p>Benefits: The system has a mechanism to meet strict deadlines that cannot be predicted on design time.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph data-line-2763">
<p>Control loops reacting to unpredictable time events—like a robot scanning an environment or a drive-by-wire system—require a different message-passing approach. Readers cannot "look at the past" and cannot block. The most recent data must be delivered lock-free and have guaranteed integrity.</p>
</div>
<div class="sect3 data-line-2765">
<h4 id="functional_description">5.9.1. Functional Description</h4>
<div class="paragraph data-line-2768">
<p>An <em>MRM</em> works as a <em>1-to-many asynchronous Mailbox</em> - with a lock-free specialisation that enables several readers to get the most recent deposited message with no integrity issues. Whenever a reader reads an MRM buffer, it will find the most recent data transmitted. It can also be seen as an extension of the Double Buffer pattern for a 1:N communication.</p>
</div>
<div class="paragraph data-line-2770">
<p>The core idea of the MRM protocol is that readers can only access the buffer that is classified as the '<em>most recent buffer</em>'. After a writer <em>publish()</em> a message, that will be the only message readers can <em>get()</em>&#8201;&#8212;&#8201;any former message being processed by a reader was grabbed <em>before</em> a new <em>publish()</em> - and, from now on, can only be <em>unget()</em>, eventually returning to the pool.</p>
</div>
<div class="paragraph data-line-2772">
<p>To clarify further, the communication steps are listed:</p>
</div>
<div class="olist arabic data-line-2774">
<ol class="arabic">
<li class="data-line-2774">
<p>A producer first reserves an MRM Buffer - the reserved MRM Buffer is not available for reading until it is published.</p>
</li>
<li class="data-line-2776">
<p>A message buffer is allocated and filled, and its address is within an MRM Buffer. The producer <em>publishes</em> the message. From now on, it is <em>the most recent message</em>. Any former published buffer is no longer visible to new readers</p>
</li>
<li class="data-line-2778">
<p>A reader starts by <em>getting</em> an MRM Buffer.  A <code>get()</code> operation delivers a copy of the message to the reader&#8217;s scope. Importantly, this operation increases the number of readers associated to that MRM Buffer.</p>
</li>
</ol>
</div>
<div class="paragraph data-line-2780">
<p>Before ending its cycle, the task releases (<code>unget()</code>) the buffer; on releasing, the kernel checks if the caller task is the last reader and if the buffer being released is not the current MRM Buffer.</p>
</div>
<div class="paragraph data-line-2782">
<p>If the above conditions are met, the <code>unget()</code> operation will return the MRM buffer to the pool. If there are more readers, OR if it is the current buffer, it remains available.</p>
</div>
<div class="paragraph data-line-2784">
<p>When the <code>reserve</code> operation detects that the most recent buffer still has readers, a new buffer is allocated to be written and published. If it has no readers, it is reused.</p>
</div>
<div class="paragraph data-line-2786">
<p>This way, the worst case is a sequence of <code>publish()</code> with no <code>unget()</code> at all&#8201;&#8212;&#8201;this would lead to the writer finding no buffer to reserve. This is prevented by making: <code>N Buffers = N tasks + 1</code>.</p>
</div>
</div>
<div class="sect3 data-line-2788">
<h4 id="mrm_control_block_configuration">5.9.2. MRM Control Block Configuration</h4>
<div class="paragraph data-line-2791">
<p>What might lead to some confusion when initialising an MRM Control Block is the need for two different pools:</p>
</div>
<div class="ulist data-line-2793">
<ul>
<li class="data-line-2793">
<p>One pool will be the storage for the MRM Buffers, which is the data structure for the mechanism.</p>
</li>
<li class="data-line-2795">
<p>Another pool is for the actual payload. The messages.</p>
</li>
</ul>
</div>
<div class="paragraph data-line-2797">
<p>Both pools must have the same number of elements: the number of tasks communicating + 1.</p>
</div>
<div class="ulist data-line-2799">
<ul>
<li class="data-line-2799">
<p>The size of the data buffers is application-dependent - and is passed as a number of <em>words</em>. The minimal message size is 32-bit.</p>
</li>
<li class="data-line-2801">
<p>If using data structures, keep it aligned to 4 to take advantage of the performance of aligned memory.</p>
</li>
</ul>
</div>
</div>
<div class="sect3 data-line-2804">
<h4 id="usage_example">5.9.3. Usage Example</h4>
<div class="paragraph data-line-2806">
<p>Consider a modern car - speed variations are of interest in many modules. With a somehow "naive" approach, let us consider three modules and how they should react when speed varies:</p>
</div>
<div class="olist arabic data-line-2808">
<ol class="arabic">
<li class="data-line-2808">
<p><strong>Cruiser Control:</strong> For the Cruiser Control, a speed increase might signify the driver wants manual control back, and it will likely turn off.</p>
</li>
<li class="data-line-2810">
<p><strong>Windshield Wipers:</strong> If they are on, a speed change can affect the electric motor&#8217;s adjustments to the air resistance.</p>
</li>
<li class="data-line-2812">
<p><strong>Radio:</strong> Speed changes reflect the aerodynamic noise - the radio volume might need adjustment.</p>
</li>
</ol>
</div>
<div class="paragraph data-line-2814">
<p>As the variations are unpredictable, we need a mechanism to deliver the last speed in order of importance for all these modules. From highest to lowest priority, Cruise, Wipers, and Radio are the three modules that range from safety to comfort.</p>
</div>
<div class="paragraph data-line-2816">
<p>To emulate this scenario, we can write an application with a higher priority task that sleeps and wakes up at pseudo-random times to produce random values that represent the (unpredictable) speed changes.</p>
</div>
<div class="paragraph data-line-2818">
<p>The snippet below has 4 periodic tasks. Tasks sleep for absolute periods. The producer publishes new data at a random interval, so it can either interrupt before one has the chance to finish or be inactive while it runs more than once.</p>
</div>
<div class="exampleblock data-line-2820">
<div class="content">
<div class="listingblock data-line-2822">
<div class="content">
<pre class="highlightjs highlight"><code class="language-C hljs" data-lang="C">typedef struct
{
	UINT 	speed;
	RK_TICK timeStamp;
} Mesg_t;

#define N_MRM (5)  /* Number of MRMs N Tasks + 1 */
#define MRM_MESG_SIZE (sizeof(Mesg_t)/4) /* In WORDS */
RK_MRM MRMCtl;/* MRM control block */
RK_MRM_BUF buf[N_MRM];/* MRM pool */
Mesg_t data[N_MRM];/* message data pool */


VOID kApplicationInit( VOID)
{
	kCreateTask(&amp;speedSensorHandle, SpeedSensorTask, RK_NO_ARGS, "SpeedTsk", stack1, STACKSIZE, 1, RK_PREEMPT);
	kCreateTask(&amp;cruiserHandle, CruiserTask, RK_NO_ARGS, "CruiserTsk", stack2, STACKSIZE, 2, RK_PREEMPT);
	kCreateTask(&amp;wiperHandle, WiperTask, RK_NO_ARGS, "WiperTsk", stack3, STACKSIZE, 3, RK_PREEMPT);
	kCreateTask(&amp;radioHandle, RadioTask, RK_NO_ARGS, "RadioTsk", stack4, STACKSIZE, 4, RK_PREEMPT);
    kMRMInit( &amp;MRMCtl, buf, data, N_MRM, MRM_MESG_SIZE);
 }

VOID SpeedSensorTask( VOID *args)
{
    RK_UNUSEARGS

	Mesg_t sendMesg = {0};
	while (1)
    {
        RK_TICK currTick = kTickGet();
        UINT speedValue = (UINT) (rand() % 170) + 1;
		sendMesg.speed = speedValue;
		sendMesg.timeStamp = currTick;
		/* grab a buffer */
        RK_MRM_BUF *bufPtr =  kMRMReserve( &amp;MRMCtl);
        if (bufPtr != NULL)
        {
            kMRMPublish( &amp;MRMCtl, bufPtr,  &amp;sendMesg);
        }
        else
        {/* cannot fail */
            kassert( 0);
        }
/* publish  */
         printf( "! @ %dT: SPEED UPDATE: %u \r\n", currTick, speedValue);
		RK_TICK sleepTicks = (( RK_TICK) rand() % 15) + 1;
        kSleepPeriodic( sleepTicks);
	}
}

VOID CruiserTask( VOID *args)
{
    RK_UNUSEARGS

	Mesg_t recvMesg = {0};
    while (1)
    {
        RK_MRM_BUF *readBufPtr = kMRMGet( &amp;MRMCtl,  &amp;recvMesg);
		printf( "@ %dT CRUISER: (%u, %uT) \r\n", kTickGet(), recvMesg.speed, recvMesg.timeStamp);
 		kMRMUnget( &amp;MRMCtl, readBufPtr);
		kSleepPeriodic( 4);

    }
}


VOID WiperTask( VOID *args)
{
    RK_UNUSEARGS
	Mesg_t recvMesg = {0};

    while (1)
    {

        RK_MRM_BUF *readBufPtr = kMRMGet( &amp;MRMCtl,  &amp;recvMesg);
		printf( "@ %dT WIPERS: (%u, %uT) \r\n", kTickGet(), recvMesg.speed, recvMesg.timeStamp);
 		kMRMUnget( &amp;MRMCtl, readBufPtr);
		kSleepPeriodic( 8);


    }
}
VOID RadioTask( VOID *args)
{
    RK_UNUSEARGS
	Mesg_t recvMesg = {0};

    while (1)

    {
        RK_MRM_BUF *readBufPtr = kMRMGet( &amp;MRMCtl, &amp;recvMesg);
		printf( "@ %dT RADIO: (%u, %uT) \r\n", kTickGet(), recvMesg.speed, recvMesg.timeStamp);
 		kMRMUnget( &amp;MRMCtl, readBufPtr);
 		kSleepPeriodic(12);

    }

}</code></pre>
</div>
</div>
</div>
</div>
<div class="paragraph data-line-2926">
<p>Thus, different situations can happen:</p>
</div>
<div class="ulist data-line-2928">
<ul>
<li class="data-line-2928">
<p>All tasks read the updated pair (speed, time)</p>
</li>
<li class="data-line-2929">
<p>Not all tasks receive the updated pair because another update happens in between.</p>
</li>
<li class="data-line-2930">
<p>No tasks receive an update - because another happens too soon.</p>
</li>
<li class="data-line-2931">
<p>All tasks receive an update and will keep rereading the same values.</p>
</li>
</ul>
</div>
<div class="paragraph data-line-2933">
<p>All these cases are on the image:</p>
</div>
<div class="imageblock data-line-2935">
<div class="content">
<img src="images/images/pumpdrop.png" alt="pumpdrop" width="30%">
</div>
</div>
<div class="admonitionblock tip data-line-2938">
<table>
<tr>
<td class="icon">
<i class="fa icon-tip" title="Tip"></i>
</td>
<td class="content">
<em>The highlight is that controllers can keep their pace, while receiving fresh data - you can see it on the timestamp on the image.</em> Again, they might receive the same data more than once or miss samples; what is important is that <em>they are not lagging and consuming stale data.</em>
</td>
</tr>
</table>
</div>
</div>
</div>
</div>
</div>
<div class="sect1 data-line-2940">
<h2 id="error_handling">6. <em><strong>Error Handling</strong></em></h2>
<div class="sectionbody">
<div class="sect2 data-line-2942">
<h3 id="fail_fast">6.1. Fail fast</h3>
<div class="paragraph data-line-2944">
<p>While tracing and error handling are yet to be largely improved (and that is when the 1.0.0 version will be released), currently
<em>RK0</em> employs a policy of <em>failing fast</em> in <strong>debug mode</strong>.</p>
</div>
<div class="paragraph data-line-2947">
<p>When Error Checking is enabled, every kernel call will be 'defensive', checking for correctness of parameters and invariants, null dereferences, etc.</p>
</div>
<div class="paragraph data-line-2949">
<p>In these cases is more useful to allow the first error to halt the execution by calling an Error Handler function to observe the program state.</p>
</div>
<div class="paragraph data-line-2951">
<p>A trace structure records the address of the running TCB, its current stack pointer, the link register (that is, the PC at kErrHandler was called), and a time stamp.</p>
</div>
<div class="paragraph data-line-2953">
<p>This record is on a <code>.noinit</code> RAM section, so it is visible if CPU resets. A fault code is stored in a global <code>faultID</code> and on the trace structure.
Developers can hook in custom behaviour.</p>
</div>
<div class="paragraph data-line-2956">
<p>If the kernel is configured to not halt on a fault, but Error Checking is enabled, functions will return negative values in case of an error.</p>
</div>
<div class="paragraph data-line-2958">
<p>On the other hand, when Error Checking is disabled or <code>NDEBUG</code> is defined nothing is checked, reducing code size and improving performance.</p>
</div>
<div class="paragraph data-line-2960">
<p>(<em>Some deeper internal calls have assertion. For those, only <code>NDEBUG</code> defined ensures they are disabled.</em>)</p>
</div>
</div>
<div class="sect2 data-line-2962">
<h3 id="stack_overflow">6.2. Stack Overflow</h3>
<div class="paragraph data-line-2964">
<p>Stack overflow is detected (not prevented) using a "stack painting" with a sentinel word. Stack Overflow detection is enabled by defining the assembler preprocessor <code>__K_DEF_STACKOVFLW</code> when compiling.</p>
</div>
<div class="paragraph data-line-2966">
<p>One can take advantage of the static task model - <em>it is possible to predict offline</em> the deepest call within any task.  The <code>-fstack-usage compiler</code> flag will create <code>.su</code> files indicating the depth of every function within a module. This is an example for the results of a compilation unit:</p>
</div>
<div class="listingblock data-line-2968">
<div class="content">
<pre class="highlightjs highlight"><code class="language-none hljs">core/src/ksynch.c:61:8:kSignalGet	112	static
core/src/ksynch.c:163:8:kSignalSet	72	static
core/src/ksynch.c:214:8:kSignalClear	40	static
core/src/ksynch.c:237:8:kSignalQuery	56	static
core/src/ksynch.c:260:8:kEventInit	40	static
core/src/ksynch.c:279:8:kEventSleep	112	static
core/src/ksynch.c:343:8:kEventWake	88	static
core/src/ksynch.c:383:8:kEventSignal	64	static
core/src/ksynch.c:412:7:kEventQuery	16	static
core/src/ksynch.c:429:8:kSemaInit	88	static
core/src/ksynch.c:470:8:kSemaPend	96	static
core/src/ksynch.c:555:8:kSemaPost	88	static
core/src/ksynch.c:603:8:kSemaWake	72	static
core/src/ksynch.c:640:5:kSemaQuery	40	static
core/src/ksynch.c:662:8:kMutexInit	16	static
core/src/ksynch.c:681:8:kMutexLock	120	static
core/src/ksynch.c:761:8:kMutexUnlock	96	static
core/src/ksynch.c:834:6:kMutexQuery	56	static</code></pre>
</div>
</div>
<div class="paragraph data-line-2989">
<p>These are the worst cases. Now, you identify the depth of the longest <em>chain of calls</em> for a task using these services and add a generous safety margin&#8201;&#8212;&#8201;30%. The cap depends on your budget.</p>
</div>
<div class="paragraph data-line-2991">
<p>Importantly, you also have to size the System Stack. This initial size is defined on <code>linker.ld</code> and on the symbol <code>Min_Stack_Size</code>. In this case, you need to account for the depth of <code>main()</code>, <code>kApplicationInit()</code>, and all interrupt handlers&#8201;&#8212;&#8201;again, inspect the the longest call chain depth. Assume interrupts will always add to the worst static depth, and make sure to account for <em>nested interrupts</em>.</p>
</div>
</div>
<div class="sect2 data-line-2993">
<h3 id="deadlocks">6.3. Deadlocks</h3>
<div class="paragraph data-line-2995">
<p>There are deadlock recovering mechanisms in the literature, a pity they are unfeasible here. The kernel provides bounded waiting, enforces every waiting queue to priority discipline, and applies priority inheritance to mutexes and message passing. Besides, it provides lock-free primitives and compensates for time drifting if a period is enforced on tasks. Well, none of these techniques can prevent deadlocks (right, with bounded blocking and lock free primitives one can get <em>livelocks</em>).</p>
</div>
<div class="ulist data-line-2998">
<ul>
<li class="data-line-2998">
<p>Ordered Locking:</p>
</li>
</ul>
</div>
<div class="paragraph data-line-3000">
<p>For those programming the application, despite following the RMS rule of higher priority for higher request rate tasks, the golden rule for locking is acquiring resources in an unidirectional order <em>throughout the entire application</em>:</p>
</div>
<div class="listingblock data-line-3001">
<div class="content">
<pre class="highlightjs highlight"><code class="language-none hljs">acquire(A);
acquire(B);
acquire(C);
   .
   .
   .
release(C);
release(B);
release(A);</code></pre>
</div>
</div>
<div class="paragraph data-line-3017">
<p>This breaks circular waiting.</p>
</div>
<div class="paragraph data-line-3019">
<p>For instance:</p>
</div>
<div class="listingblock data-line-3021">
<div class="content">
<pre class="highlightjs highlight"><code class="language-none hljs">TaskA:
   wait(R1);
   wait(R2);
    /* critical section */
   signal(R2);
   signal(R1);

TaskB:
   wait(R1);
   wait(R2);
    /* critical section */
   signal(R2);
   signal(R1);</code></pre>
</div>
</div>
<div class="paragraph data-line-3039">
<p>But, if:</p>
</div>
<div class="listingblock data-line-3041">
<div class="content">
<pre class="highlightjs highlight"><code class="language-none hljs">TaskA:
    wait(R1);
    wait(R2);
    .
    .

TaskB:
    wait(R2);
    wait(R1);
    .
    .</code></pre>
</div>
</div>
<div class="paragraph data-line-3055">
<p>There are some possible outcomes:</p>
</div>
<div class="olist arabic data-line-3057">
<ol class="arabic">
<li class="data-line-3057">
<p>Deadlock:</p>
<div class="ulist data-line-3059">
<ul>
<li class="data-line-3059">
<p>TaskA runs: acquires R1</p>
</li>
<li class="data-line-3060">
<p>TaskB runs: acquires R2</p>
</li>
<li class="data-line-3061">
<p>TaskA runs: tries to acquire R2 — blocked</p>
</li>
<li class="data-line-3062">
<p>TaskB runs: tries to acquire R1 — blocked</p>
</li>
</ul>
</div>
</li>
<li class="data-line-3064">
<p>No deadlock:</p>
<div class="ulist data-line-3066">
<ul>
<li class="data-line-3066">
<p>TaskA runs: acquires R1</p>
</li>
<li class="data-line-3068">
<p>TaskA runs: acquires R2 (nobody is holding R2)</p>
</li>
<li class="data-line-3070">
<p>TaskA releases both; TaskB runs and acquires both (in either order)</p>
</li>
</ul>
</div>
</li>
</ol>
</div>
<div class="paragraph data-line-3073">
<p>Overall, there is no deadlock if tasks do not overlap in critical sections. That is why systems run for years without deadlocks and eventually: <em>ploft</em>.</p>
</div>
<div class="ulist data-line-3076">
<ul>
<li class="data-line-3076">
<p>Use a 'master-lock' with 'try' semantics:</p>
</li>
</ul>
</div>
<div class="paragraph data-line-3078">
<p>Another technique that can be employed is if one needs to acquire multiple locks—acquire them all or none using a try-lock (<code>RK_NO_WAIT</code>). If any of the tries fail, the task gives up on acquiring the resources and backs off, releasing all successful locks to retry later (most simply, using a sleep queue). That is easier said than done, though, and, as mentioned, if not well done, instead of deadlocks, one gets livelocks.</p>
</div>
<div class="paragraph data-line-3080">
<p>(<em>Livelocks</em> are when a couple of tasks keep running, but the program does not advance.)</p>
</div>
</div>
</div>
</div>
<div class="sect1 data-line-3083">
<h2 id="rk0_services_api">7. <em><strong>RK0 Services API</strong></em></h2>
<div class="sectionbody">
<div class="paragraph data-line-3085">
<p><strong>Convention</strong></p>
</div>
<div class="ulist data-line-3087">
<ul>
<li class="data-line-3087">
<p>A kernel call starts with a lowercase <code>k</code>. Typically it is followed by a kernel object identifier and an action.</p>
</li>
</ul>
</div>
<div class="listingblock data-line-3089">
<div class="content">
<pre class="highlightjs highlight"><code class="language-c hljs" data-lang="c">kSemaPend(&amp;sema, 800); /* pend on a semaphore; 800 ticks time-out */</code></pre>
</div>
</div>
<div class="ulist data-line-3093">
<ul>
<li class="data-line-3093">
<p>When <code>k</code> is followed by an action, it is acting on the caller task.</p>
</li>
</ul>
</div>
<div class="listingblock data-line-3095">
<div class="content">
<pre class="highlightjs highlight"><code class="language-c hljs" data-lang="c">kSleep(150); /* sleep-delay the caller task for 150 ticks */</code></pre>
</div>
</div>
<div class="ulist data-line-3099">
<ul>
<li class="data-line-3099">
<p>Some calls can act either on the caller or on another task:</p>
</li>
</ul>
</div>
<div class="listingblock data-line-3101">
<div class="content">
<pre class="highlightjs highlight"><code class="language-C hljs" data-lang="C">/* stores the signal flags of the task identified by task1Handle on queryValue */
kSignalQuery(task1Handle, &amp;queryValue);

/* retrieves its own signal flags */
kSignalQuery(NULL, &amp;queryValue);</code></pre>
</div>
</div>
<div class="paragraph data-line-3110">
<p><strong>Return Values</strong></p>
</div>
<div class="paragraph data-line-3112">
<p>With a few exceptions, kernel calls return a <code>RK_ERR</code> error code. <code>0</code> is a successful operation <code>(RK_SUCCCESS)</code> and any negative value is an error that indicates failure.
A positive value is an unsuccesful operation, but will not lead the system to failure (e.g., any unsuccesful <code>try</code> operation).</p>
</div>
<div class="ulist data-line-3115">
<ul>
<li class="data-line-3115">
<p>Find the most up to date RK0 API at the repo:
<a href="https://github.com/antoniogiacomelli/RK0/blob/main/core/inc/kapi.h">RK0 API</a></p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1 data-line-3118">
<h2 id="scheduler_determinism">8. Scheduler Determinism</h2>
<div class="sectionbody">
<div class="sect2 data-line-3120">
<h3 id="preemptive_scheduling">8.1. Preemptive Scheduling</h3>
<div class="paragraph data-line-3121">
<p>This is a simple test to establish some evidence the scheduler obeys the pre-emption criteria: a higher priority task always pre-empts a lower priority task.</p>
</div>
<div class="sect4 data-line-3123">
<h5 id="using_direct_signals">8.1.1. Using Direct Signals</h5>
<div class="paragraph data-line-3125">
<p>Tasks 1, 2, 3, and 4 are in descending order of priority. If the scheduler is well-behaved, we shall see counters differing by "1."</p>
</div>
<div class="listingblock data-line-3127">
<div class="content">
<pre class="highlightjs highlight"><code class="language-C hljs" data-lang="C">VOID Task1(VOID* args)
{
    RK_UNUSEARGS
	while(1)
	{
		counter1++;
		kPend(RK_WAIT_FOREVER);
	}
}

VOID Task2(VOID* args)
{
    RK_UNUSEARGS
	while(1)
	{
	    kSignal(task1Handle); /* shall immediately be preempted by task1 */
		counter2++;
		kPend(RK_WAIT_FOREVER);    /* suspends again */
	}
}


VOID Task3(VOID* args)
{
    RK_UNUSEARGS
	while(1)
	{
		kSignal(task2Handle);  /* shall immediately be preempted by task2 */
		counter3++;
		kPend(RK_WAIT_FOREVER); /* suspends again */
	}
}

VOID Task4(VOID* args)
{
    RK_UNUSEARGS
	while(1)
	{
	    /* shall immediately be preempted by task3 */
	    kSignal(task3Handle); /
	    /* only resumes after all tasks are pending again */
	    counter4++;
	}
}</code></pre>
</div>
</div>
<div class="paragraph data-line-3176">
<p>This is the output after some time running:</p>
</div>
<div class="imageblock data-line-3178">
<div class="content">
<img src="images/images/signaldet.png" alt="signaldet">
</div>
</div>
</div>
<div class="sect4 data-line-3180">
<h5 id="using_semaphores">8.1.2. Using Semaphores</h5>
<div class="listingblock data-line-3182">
<div class="content">
<pre class="highlightjs highlight"><code class="language-C hljs" data-lang="C">RK_SEMA sema1;
RK_SEMA sema2;
RK_SEMA sema3;
RK_SEMA sema4;

VOID kApplicationInit(VOID)
{
	kSemaInit(&amp;sema1, RK_SEMA_COUNT, 0);
	kSemaInit(&amp;sema2, RK_SEMA_COUNT, 0);
	kSemaInit(&amp;sema3, RK_SEMA_COUNT, 0);
	kSemaInit(&amp;sema4, RK_SEMA_COUNT, 0);

}

VOID Task1(VOID* args)
{
    RK_UNUSEARGS
	while (1)
	{
		counter1++;
		kSemaWait(&amp;sema1, RK_WAIT_FOREVER);
	}
}

VOID Task2(VOID* args)
{
    RK_UNUSEARGS
	while (1)
	{
		kSemaSignal(&amp;sema1);
		counter2++;
		kSemaWait(&amp;sema2, RK_WAIT_FOREVER);
	}
}

VOID Task3(VOID* args)
{
    RK_UNUSEARGS
	while (1)
	{
		kSemaSignal(&amp;sema2);
		counter3++;
		kSemaWait(&amp;sema3, RK_WAIT_FOREVER);
	}
}

VOID Task4(VOID* args)
{
    RK_UNUSEARGS
	while (1)
	{

		kSemaSignal(&amp;sema3);
		counter4++;
	}
}</code></pre>
</div>
</div>
<div class="imageblock data-line-3242">
<div class="content">
<img src="images/images/determsema.png" alt="determsema">
</div>
</div>
<div class="paragraph data-line-3244">
<p>Here, the tick is running @ 0.5us</p>
</div>
</div>
<div class="sect4 data-line-3247">
<h5 id="using_mailboxes">8.1.3. Using Mailboxes</h5>
<div class="listingblock data-line-3250">
<div class="content">
<pre class="highlightjs highlight"><code class="language-C hljs" data-lang="C">/**************************************************
 * Mailboxes are initialised empty.
 *  kMboxInit(&amp;mbox1, NULL);
 *  kMboxInit(&amp;mbox2, NULL);
 *	kMboxInit(&amp;mbox3, NULL);
 *	kMboxInit(&amp;mbox4, NULL);
 *	kMboxInit(&amp;mbox5, NULL);
 *
 * Highest Prio (Task1), Lowest (Task5)
 *
 * Using Mailboxes to pass tokens as signals.
 *
 ***********************************************    ***/


VOID Task1( VOID *args)
{
	RK_UNUSEARGS
	UINT *p;
	while (1)
	{
		counter1++;
		kMboxPend(&amp;mbox1, (VOID*) &amp;p, RK_WAIT_FOREVER);
	}
}

VOID Task2( VOID *args)
{
	RK_UNUSEARGS
	UINT mesg = 1;
	UINT *p;

	while (1)
	{
		kMboxPost(&amp;mbox1, &amp;mesg, RK_WAIT_FOREVER);

		counter2++;

		kMboxPend(&amp;mbox2, (VOID*) &amp;p, RK_WAIT_FOREVER);

	}
}
VOID Task3( VOID *args)
{
	RK_UNUSEARGS
	UINT mesg = 1;
	UINT *p;
	while (1)
	{

		kMboxPost(&amp;mbox2, &amp;mesg, RK_WAIT_FOREVER);

		counter3++;

		kMboxPend(&amp;mbox3, (VOID*) &amp;p, RK_WAIT_FOREVER);

	}
}

VOID Task4( VOID *args)
{
	RK_UNUSEARGS
	UINT mesg = 1;
	UINT *p;
	while (1)
	{

		kMboxPost(&amp;mbox3, &amp;mesg, RK_WAIT_FOREVER);

		counter4++;

		kMboxPend(&amp;mbox4, (VOID*) &amp;p, RK_WAIT_FOREVER);

	}
}
VOID Task5( VOID *args)
{
    RK_UNUSEARGS
    UINT mesg=1;
    while(1)
	{

		kMboxPost(&amp;mbox4, &amp;mesg, RK_WAIT_FOREVER);
		counter5++;

	}
}</code></pre>
</div>
</div>
<div class="imageblock data-line-3344">
<div class="content">
<img src="https://kernel0org.wordpress.com/wp-content/uploads/2025/06/mboxbench.png" alt="mboxbench">
</div>
</div>
</div>
</div>
</div>
</div>
<div class="sect1 data-line-3347">
<h2 id="influences">9. Influences</h2>
<div class="sectionbody">
<div class="paragraph data-line-3349">
<p>For the sake of intellectual honesty and acknowledgment I will list the systems that have inspired RK0 (still incipient) design:</p>
</div>
<div class="ulist data-line-3351">
<ul>
<li class="data-line-3351">
<p>Pure 'RTOSish' things heavily influenced by: <em>Nucleus, ThreadX and uCOS/OS</em>.</p>
</li>
<li class="data-line-3353">
<p>The MRM Protocol is inspired by <em>HARTIK RTOS</em>.</p>
</li>
<li class="data-line-3355">
<p>Doubly Linked Lists and Delta-Lists are inspired by Linux 2.6.x and 4.4BSD, respectively.</p>
</li>
</ul>
</div>
<hr>
<div class="imageblock text-center data-line-3360">
<div class="content">
<img src="images/images/mascott.png" alt="mascott" width="10%">
</div>
</div>
<div class="paragraph data-line-3362">
<p>&#169; <em>2025 Antonio Giacomelli | All Rights Reserved | <a href="http://kernel0.org/">www.kernel0.org</a></em></p>
</div>
</div>
</div>
</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.3/highlight.min.js"></script>
<script>
if (!hljs.initHighlighting.called) {
  hljs.initHighlighting.called = true
  ;[].slice.call(document.querySelectorAll('pre.highlight > code[data-lang]')).forEach(function (el) { hljs.highlightBlock(el) })
}
</script>
</body>
</html>
