= RK0: The Real-Time Kernel '0'
System: 0.5.0-dev | Docbook: 250516-1 
:toc: left
:toclevels: 4
:sectnums:
:sectnumlevels: 4
:icons: font
:source-highlighter: highlight.js
:experimental:

== THE KERNEL ON A GLANCE


[NOTE]
====

- This Docbook is not meant to cover API details. API details can be found at
https://github.com/antoniogiacomelli/RK0/[the project repository]

- *_RK0_* is a work in progress.
====

=== The design approach


*RK__0__* (or simply _K0_) is built on the fundamental understanding that systems programming in general, particularly embedded systems programming, is (very) unique and requires a direct approach - rather than an application-like framework model.

=== Scheduler

====
_RK0 Blog_: https://kernel0.org/2025/05/16/about-processes-tasks-and-threads/[About Processes, Tasks and Threads]
====


.*A Real-Time _Executive_*

On the embedded realm, probably because we lack a better abstraction, we use multithreading to fine-tune our load balance and, therefore, responsiveness to achieve real-time. 

This is an arrangement: instead of having a single super-loop, we have many, each running on its own execution stack.

This arrangement yields an operating system entity to handle—a (logical) _Concurrency Unit_: in K0, we name it a _Task_ (in RK0  _**a task is a thread**_.).

.*Architecture*
If no more details are to be provided, the kernel has a top and a bottom layer. On the top, the Executive manages the resources needed by the application. On the bottom, the Low-level Scheduler works as a software extension of the CPU. Together, they implement the Task abstraction — the Concurrency Unit that enables a multitasking environment.

In systems design jargon, the Executive enforces policy (what should happen). The Low-level Scheduler provides the mechanism (how it gets done). The services are the primitives that gradually translate policy decisions into concrete actions executed by the Scheduler.



K0's goal is determinism on low-end devices. Its multitasking engine operates with no mimics of _userland_.

image::images/layeredkernel.png[width=50%]


==  *Core Mechanisms*

This section provides a high-level description of the kernel core mechanisms: scheduler, timers, and memory allocator.




_RK0_ employs a Rate Monotonic Scheduler. Tasks are assigned priorities accordingly to their request rates - i.e., tasks with shorter periods are assigned to higher priorities. The highest priority is represented by the value '0'; the lowest is represented by the value '31'.

A scheduler remark is its constant time-complexity (_O(1)_) and low-latency. This was achieved by careful composing the data structures along with a efficient _'choose-next'_ algorithm. This is detailed below.

[NOTE]
====

Time-slice was deprecated on version 0.5.0

====
==== Scheduler Data Structures

===== Task Control Block

Threads are represented as Tasks. Every task is associated to a Task Control Block structure. This is a record for stack, resources and time management. The table below partially represents a Task Control Block (as this document is live, this might not reflect the exact fields of the current version).

[width="100%",options="header"]
|====================
|Task Control Block
|Task name
|Saved Stack Pointer
|Stack Address
|Stack Size
|Status 
|Assigned Priority
|Current Priority
|Self-Assigned ID
|Last wake-time
|Run-To-Completion Flag
|Time-out Flag
|Aggregated Timeout Node
|Aggregated Task List Node
|====================

Tasks are static - they cannot be created on runtime, to be destroyed, to fork or join.

In practice, tasks are either _RUNNING_ or '_waiting_' for their turn to run. Now, we need to clearly define  _WAITING_ and _READY_.

. A _READY_ task will be dispatched - therefore, switch to _RUNNING_, whenever it is the highest priority _READY_ task.

. A _WAITING_ task depends on a condition, generalised as an _event_ to switch to _READY_.


image::images/taskstates.png[]

Logically, the _WAITING_ state will assume different pseudo-states, related to the kind of event that will switch a task to _READY_: 

* _SLEEPING_: a task suspends itself and goes to sleep for a given period; or suspends itself until receiving a _wake_ signal, representing an event.

* _PENDING_ : the task suspended itself waiting for a combination of signal flags.

* _BLOCKED_: A task is blocked on a mutex or semaphore.

* _SENDING/RECEIVING_: A producer task when blocking on a Message Passing object switches is status to _SENDING_. A consumer, to _RECEIVING_.


====
*_The scheduler rules, not the heap._*

_RK0_ tasks are static.

It’s a design decision rooted in real-time correctness.

Stacks are defined and passed explicitly when creating a task.

The wins:

* A memory layout the systems programmer actually knows.
* No alignment traps.
* Link-time visibility:
** Each task’s stack is a named symbol in the linker map.
** You can inspect and verify the memory layout before flashing. 
** A simple `objdump` reveals all stack allocations — that’s peace of mind.

====

image::images/schdatastruct.png[width=85%]

===== Task Queues
The backbone of the queues where tasks will wait for their turn to run is a circular doubly linked list: removing any item from a doubly list takes O(1) (provided we don’t need to search the item). As the kernel knows each task’s address, adding and removing is always O(1). Singly linked lists, can’t achieve O(1) for removal in any case.

===== Ready Queue Table

Another design choice towards achieving O(1) is the global ready queue, which is a table of FIFO queues—each queue dedicated to a priority—and not a single ordered queue. So, enqueuing a ready task is always O(1). Given the sorting needed, if tasks were placed on a single ready queue, the time complexity would be O(n).

===== Waiting Queues

The scheduler does not have a unique waiting queue. Every kernel object that has the ability to block a task has an associated waiting queue. Because these  queues are a scheduler component, _they follow a priority discipline_: the highest priority task is dequeued first, _always_.

When an event capable to switch tasks from _WAITING_ to _READY_ happens, one or more tasks (depending on the mechanism) are then placed on the ready list, unique to is priority. Now they are waiting to be picked by the scheduler - that is _READY_ definition. 

==== The scheduling algorithm

As the ready queue table is indexed by priority - the index 0 points to the queue of ready tasks with priority 0, and so forth, and there are 32 possible priorities - a 32-bit integer can represent the state of the ready queue table. It is a BITMAP:

``` 

The BITMAP computation: ((1a) OR (1b)) AND (2), s.t.:

(1a) Every Time a task is readied, update: BITMAP |= (1U << task->priority );
(1b) Every Time an empty READY QUEUE becomes non-empty, update: BITMAP |= (1U << queueIndex)
(2): Every Time READY QUEUE becomes empty, update: BITMAP &= ~(1U << queueIndex);
EXAMPLE:

  Ready Queue Index :     (6)5 4 3 2 1 0
          Not empty :      1 1 1 0 0 1 0
                           ------------->
                 (LOW)  Effective Priority  (HIGH)
In this case, the scenario is a system with 7 priority task levels. Queues with priorities 6, 5, 4, and 1 are not empty.


``` 
In RK0 source code, the following routines implement the bitmap update:

```C

/* Enqueue a TCB on on the tail of TCB list  */
RK_ERR kTCBQEnq( RK_TCBQ *const kobj, RK_TCB *const tcbPtr)
{

    RK_CR_AREA
    RK_CR_ENTER
    if (kobj == NULL || tcbPtr == NULL)
    {
        kErrHandler( RK_FAULT_OBJ_NULL);
    }
    RK_ERR err = kListAddTail( kobj, &(tcbPtr->tcbNode));
    if (err == 0)
    {
        /* if a task was enqueued on a list within the ready queue table, update the 'ready bitmap' */
        if (kobj == &readyQueue[tcbPtr->priority])
            readyQBitMask |= 1 << tcbPtr->priority;
    }
    RK_CR_EXIT
    return (err);
}

/* Add a TCB on on the head of TCB list  */
RK_ERR kTCBQJam( RK_TCBQ *const kobj, RK_TCB *const tcbPtr)
{

	RK_CR_AREA
	RK_CR_ENTER
	if (kobj == NULL || tcbPtr == NULL)
	{
		kErrHandler( RK_FAULT_OBJ_NULL);
	}
	RK_ERR err = kListAddHead( kobj, &(tcbPtr->tcbNode));
	if (err == 0)
	{
		if (kobj == &readyQueue[tcbPtr->priority])
			readyQBitMask |= 1 << tcbPtr->priority;
	}
	RK_CR_EXIT
	return (err);
}

/* Dequeue the head task from a list of TCBs */
RK_ERR kTCBQDeq( RK_TCBQ *const kobj, RK_TCB **const tcbPPtr)
{
    if (kobj == NULL)
    {
        kErrHandler( RK_FAULT_OBJ_NULL);
    }
    RK_NODE *dequeuedNodePtr = NULL;
    RK_ERR err = kListRemoveHead( kobj, &dequeuedNodePtr);

    if (err != RK_SUCCESS)
    {
        return (err);
    }
    *tcbPPtr = RK_LIST_GET_TCB_NODE( dequeuedNodePtr, RK_TCB);

    if (*tcbPPtr == NULL)
    {
        kErrHandler( RK_FAULT_OBJ_NULL);
        return (RK_ERR_OBJ_NULL);
    }
    RK_TCB *tcbPtr_ = *tcbPPtr;
    RK_PRIO prio_ = tcbPtr_->priority;

    /* if the list is in the ready queue table and is now empty 
     update 'ready bitmap' */
    if ((kobj == &readyQueue[prio_]) && (kobj->size == 0))
        readyQBitMask &= ~(1U << prio_);
        
    return (RK_SUCCESS);
}

/* Remove an specific TCB from a TCB List */
RK_ERR kTCBQRem( RK_TCBQ *const kobj, RK_TCB **const tcbPPtr)
{
    if (kobj == NULL || tcbPPtr == NULL)
    {
        kErrHandler( RK_FAULT_OBJ_NULL);
    }
    RK_NODE *dequeuedNodePtr = &((*tcbPPtr)->tcbNode);
    RK_ERR err = kListRemove( kobj, dequeuedNodePtr);
    if (err != RK_SUCCESS)
    {
        return (err);
    }
    *tcbPPtr = RK_LIST_GET_TCB_NODE( dequeuedNodePtr, RK_TCB);
    if (*tcbPPtr == NULL)
    {
        kErrHandler( RK_FAULT_OBJ_NULL);
    }
    RK_TCB *tcbPtr_ = *tcbPPtr;
    RK_PRIO prio_ = tcbPtr_->priority;
    if ((kobj == &readyQueue[prio_]) && (kobj->size == 0))
          readyQBitMask &= ~(1U << prio_);
    return (RK_SUCCESS);
}

```
The Idle Task priority is assigned by the kernel, during initialisation, taking into account all priorities the system programmer has defined. Unless user tasks are occupying all 32 priorities, the Idle Task is treated as an ordinary lowest priority and has a position in the ready queue table. If not, the idle task on practice will have no queue position and will be selected when the BITMAP is 0. In the above bitmap, the idle task is in readyQueue[6].

Having Ready Queue Table bitmap, we find the highest priority non-empty task list as follows:

(1) Isolate the *rightmost* '1':

```
RBITMAP = BITMAP & -BITMAP. (- is the bitwise operator for two's complement: ~BITMAP + 1) `
``` 

In this case:

``` 

                           [31]       [0]  :  Bit Position
                             0...1110010   :  BITMAP
                             1...0001110   : -BITMAP
                            =============
                             0...0000010   :  RBITMAP
                                     [1]
``` 

_The rationale here is that, for a number N, its 2’s complement -N, flips all bits - except the rightmost '1' (by adding '1') . Then, N & -N results in a word with all 0-bits except for the less significant '1'._

(2) Extract the *rightmost '1' _position_*:

- For ARMv7M, we benefit from the `CLZ` instruction to count the _leading zeroes_. As they are the the number of zeroes on the left of the rightmost bit '1', this value subtracted from 31 to find the Ready Queue index. 

```C
__RK_INLINE static inline 
unsigned __getReadyPrio(unsigned readyQBitmap)
{
    unsigned ret;
    __ASM volatile (
        "clz    %0, %1     \n"
        "neg    %0, %0     \n"
        "add    %0, %0, #31\n"
        : "=&r" (ret)         
        : "r" (readyQBitmap)               
        :                      
    );
    return (ret);
}
``` 

In the example above, this instruction would return #30, and #31 - #30 = #01.


- For ARMv6M there is no suitable hardware instruction. The algorithm is totally written in C and counts the _trailing zeroes_, thus, the index number. Although it might vary depending on your compiler settings, it takes ~11 cycles (_note it is still O(1)_):

[source,C]
----
/* 
  De Brujin's multiply+LUT 
  (Hacker's Delight book)
*/

/* table is on a ram section  for efficiency */
__K_SECTION(getReadyTable)
static unsigned table[32] = 
{
 0, 1, 28, 2, 29, 14, 24, 3, 30, 22, 20, 15, 25, 17, 4, 8,
 31, 27, 13, 23, 21, 19, 16, 7, 26, 12, 18, 6, 11, 5, 10, 9
};

__RK_INLINE static inline 
unsigned __getReadyPrio(unsigned readyQBitmap)
{
    unsigned mult = readyQBitmap * 0x077CB531U;  

    /* Shift right the top 5 bits
     */
    unsigned idx = (mult >> 27);

    /* LUT */
    unsigned ret = (unsigned)table[idx];
    return (ret);
}
----

For the example above, `mult = 0x2 * 0x077CB531 = 0x0EF96A62`. The 5 leftmost bits (the index) are `00001` -> `table[1] = 1`.

During a context switch, the procedures to find the highest priority non-empty ready queue table index are as follows:
[source,C]
----

static inline PRIO kCalcNextTaskPrio_()
{
    if (readyQBitMask == 0U)
    {
        return (idleTaskPrio);
    }
    readyQRightMask = readyQBitMask & -readyQBitMask;
    PRIO prioVal = (PRIO) (__getReadyPrio(readyQRightMask));
    return (prioVal);
}

VOID kSchSwtch(VOID)
{
    /* O(1) complexity */
	nextTaskPrio = calcNextTaskPrio_(); 
	
	RK_TCB* nextRunPtr = NULL;
	
	/* O(1) complexity */
	RK_ERR err = kTCBQDeq( &readyQueue[nextTaskPrio], &nextRunPtr);
	if ((nextRunPtr == NULL) || (err != RK_SUCCESS))
	{
	    kErrHandler(FAULT_READYQ);
	}
	runPtr = nextRunPtr;

}
----

[NOTE]
_RK0_ is able to handle context-switching with extended frame when a float-point unit co-processor is available. This must be informed when compiling by defining the symbol `__FPU_PRESENT=1`.

==== Scheduler Determinism

===== Preemptive Scheduling
This is a simple test to establish some evidence the scheduler obeys the pre-emption criteria: a higher priority task always pre-empts a lower priority task.

Task1, 2, 3, 4 are in descending order of priority. If the scheduler is well-behaved, we shall see counters differing by "1".

```C

VOID Task1(VOID* args)
{
    RK_UNUSEARGS
	while(1)
	{
		counter1++;
		kPend(RK_WAIT_FOREVER);
	}
}

VOID Task2(VOID* args)
{
    RK_UNUSEARGS
	while(1)
	{
		counter2++;
		kSignal(task1Handle); /* shall immediately be preempted by task1 */
		kPend(RK_WAIT_FOREVER);    /* suspends again */
	}
}


VOID Task3(VOID* args)
{
    RK_UNUSEARGS
	while(1)
	{
		counter3++;
		kSignal(task2Handle);  /* shall immediately be preempted by task2 */
		kPend(RK_WAIT_FOREVER); /* suspends again */
	}
}

VOID Task4(VOID* args)
{
    RK_UNUSEARGS
	while(1)
	{
	    counter4++;
	    /* shall immediately be preempted by task3 */
	    kSignal(task3Handle); /
	    /* only resumes after all tasks are pending again */
	}
}

```

This is the output after some time running:

image::images/signaldet.png[]

In the above example we have used direct signals. Using semaphores:

```C

RK_SEMA sema1;
RK_SEMA sema2;
RK_SEMA sema3;
RK_SEMA sema4;

VOID kApplicationInit(VOID)
{
	kSemaInit(&sema1, RK_SEMA_COUNTER, 0);
	kSemaInit(&sema2, RK_SEMA_COUNTER, 0);
	kSemaInit(&sema3, RK_SEMA_COUNTER, 0);
	kSemaInit(&sema4, RK_SEMA_COUNTER, 0);

}

VOID Task1(VOID* args)
{
    RK_UNUSEARGS
	while (1)
	{
		counter1++;
		kSemaWait(&sema1, RK_WAIT_FOREVER);
	}
}

VOID Task2(VOID* args)
{
    RK_UNUSEARGS
	while (1)
	{
		counter2++;
		kSemaSignal(&sema1);
		kSemaWait(&sema2, RK_WAIT_FOREVER);
	}
}

VOID Task3(VOID* args)
{
    RK_UNUSEARGS
	while (1)
	{
		counter3++;
		kSemaSignal(&sema2);
		kSemaWait(&sema3, RK_WAIT_FOREVER);
	}
}

VOID Task4(VOID* args)
{
    RK_UNUSEARGS
	while (1)
	{

		counter4++;
		kSemaSignal(&sema3);
	}
}

```
image::images/determsema.png[]

Here tick is running @ 0.5us

===== Cooperative Scheduling

If we set all tasks at the same priority and every tasks yields the processor, they will run on a round-robin fashion, one after another. So, every time we pause chances are we will be "somewhere in the middle" of a round.

If every task increases a counter before yielding what we expect to see is a set of counters on a fashion {K, K, K, K-1, K-1, K-1}. Importantly a counter will not offset another by more than 1 if the scheduler is deterministic.

```C
/* All tasks have the same priority */
VOID Task1(VOID* args)
{
    RK_UNUSEARGS
    
	while (1)
	{
		count1 += 1;
		kYield();
	}
}

VOID Task2(VOID* args)
{
    RK_UNUSEARGS
	while (1)
	{
		count2 += 1;
		kYield();
	}
}

VOID Task3(VOID* args)
{
    RK_UNUSEARGS
	while (1)
	{
		count3 += 1;
		kYield();
	}
}

VOID Task4(VOID* args)
{
    RK_UNUSEARGS
	while (1)
	{
		count4 += 1;
		kYield();
	}

}

VOID Task5(VOID* args)
{
    RK_UNUSEARGS
	while (1)
	{
		count5 += 1;
		kYield();
	}

}
``` 

The picture below show the results after ~ 13 million rounds.


image::images/determrr.png[]


[TIP]
====
The mindful design choices for data structures and algorithms yielded a core system with highly deterministic behaviour - maintained even under stringent time constraints. 
====

==== Common scheduling pitfalls


To avoid the most common pitfalls when scheduling tasks the system programmer should be aware that:

- The scheduler behaviour is to choose the highest priority READY task to run. Always.
- For a set of tasks with the same priority, the schedule works on a First-In-First-Out discipline

- A task must switch to _READY_ state before being eligible for scheduling. 

- A task will switch from _RUNNING_ to _READY_ if yielding or if being preempted by a higher priority task.

- Otherwise it can only go to a _WAITING_ state, and eventually switch back to _READY_.

- When a task is preempted it switches from _RUNNING_ to _READY_ and is placed back to the _head_ position of its Ready Queue. It means that it will be resumed as soon as it is the highest priority ready task again. 

- On the contrary, if a task _yields_, it is telling the scheduler that it has completed its cycle. Then, it will be enqueued on the ready queue tail - the last queue position.

- When switching from _WAITING_ to _READY_, the task is also enqueued on the tail.


- So, tasks with the same priority will _round-robin_ as long as they _yield_ or _wait_. 


[NOTE]
====
The bottomline is that, tasks with the same priorities run on FIFO discipline. A task with priority 'N' can make other tasks with priority 'N' to starve if it never _yields_ or _wait_ -- it got _in_, but never _out_.
====

=== *Timers*

[TIP]
====

Context switching is probably the largest overhead on a kernel. The time spent on the System Tick handler contributes to a large portion of this overhead.

Design Choice: 

- Timers are kept on a single list, and only the head element needs to be updated by using a delta-queue approach. 

- Application Timers that trigger callbacks are run on a deferred run-to-completion system task.

Benefits: 

- Keep the overhead of updating timers as minimal as possible with the delta-queue; 

- Deferring the Application Timer to a run-to-completion task, meet the requested callback period, while keeping the ability to track system ticks.
====


[width="100%",options="header"]
|===============================
|Timeout Node
|Timeout Type
|Absolute Interval (Ticks)
|Relative Interval (Ticks)
|Waiting Queue Address
|Next Timeout Node
|Previous Timeout Node
|===============================

Every task is prone to events triggered by timers, which are described in this section. Every Task Control Block has a node to _a timeout list_.

This list is a doubly linked list, ordered as a delta list. For instance, three timers (T1,8), (T2,6) and (T3,10) will be ordered as a sequence <(T2,6), (T1,2), (T3,2)> - so it counts <6, (6)+2, ((6)+2)+2>.

Thus, for every system tick, only the head element on the list needs to be decreased - yielding O(1)  - another design choice towards deterministic behaviour.  

==== Sleep Timers

The primitive `sleep(t)` suspends a task on a SLEEPING state, for `t` ticks start to count when called. 

For periodic activations, use `sleepuntil(p)` in which p is an absolute suspension period in ticks. The kernel adjusts any time drift/jitters that might happen in between calls.  

==== Blocking Time-out

These are timers associated with kernel calls that are blocking. Thus, establishing an upper bound waiting time might benefit them. When the time for unblocking is up, the kernel call returns, indicating a timeout error. When blocking is associated to a kernel object (other than the Task Control Block), the timeout node will store the object waiting queue's address, so it can be removed if time expires.

==== Callout Timers

[width="100%",options="header"]
|===============================
|Timer Control Block
|Option: Reload/One-Shot
|Phase (Initial Delay)
|Callout Function Pointer
|Callout Argument
|Timeout Node
|===============================


These are Application Timers that will issue a callback when expiring.
In addition to a callout function, an Application Timer receives an initial phase delay and a period and can choose to run once (one-shot) or auto-reload itself.

The callback runs within a System Task with priority 0 and is run-to-completion - what makes the scheduler prioritise it over other tasks. Callouts must be made short and unblocking - as they can cause high CPU contention.

For clarity, Timer Callouts are on a separate list in the kernel, although they share the same `TIMEOUT` node.

=== *System Tick*

A dedicated peripheral that generates an interrupt after a defined period provides the kernel time reference. For ARMv6/7M, this peripheral is the built-in SysTick, a 24-bit counter timer. 
On every tick, the handler performs some housekeeping and assesses the need to call a context switch.

The "housekeeping" accounts for global timer tracking and any tick-dependent condition that might change a task status.
When a timer expires, it might switch a task from `WAITING` to `READY` or dispatch a callback. In the case of a callback, this will also trigger a context-switching for the TimerHandler System Task in which the callback is executed and the related timer(s) are updated properly.

Note that tasks might switch from `WAITING` to `READY` for reasons other than tick-related. In these cases, context switching might be triggered immediately if the readied task can preempt the running task.

=== *Memory Allocator*

[width="100%",options="header"]
|===============================
|Memory Allocator Control Block
|Associated Block Pool
|Number of Blocks
|Block Size
|Number of Free Blocks
|Free Block List
|===============================

Remember that the standard `malloc()`  leads to fragmentation and (also, because of that) is highly indeterministic. Unless we use it once - to allocate memory before starting up, it doesn’t fit. But often, we need to 'multiplex' memory amongst tasks over time, that is, to dynamically allocate and deallocate.

To avoid fragmentation, we use fixed-size memory blocks. A simple approach would be a static table marking each block as free or taken. With this pattern, you will need to 'search' for the next available block, if any - the time for searching changes - what is indeterministic.
A suitable approach is to keep track of what is free using a dynamic table—a linked list of addresses. 

We use "meta-data" to initialise the linked list. Every address holds the "next" address value. All addresses are within the range of a pool of fixed-size blocks.
This approach limits the minimal size of a block to the size of a memory address - 32-bit for our supported architecture. 

Yet, this is the cheapest way to store meta-data. If not stored on the empty address itself, an extra 32-bit variable would be needed for each block, so it could have a size that is less than 32-bit.

[TIP]
=====
Allocating memory on run-time is a major source of latency (1), indeterministic (2) behaviour and footprint overhead (3).

Design choice: the allocator's design achieves low-cost, deterministic, fragmentation-free memory management by using fixed-size word-aligned block sizes (1)(2), and embedding metadata within the memory blocks themselves (3).

Benefits: run-time memory allocation benefits are provided with  no real-time drawbacks. 
=====


_Importantly, the kernel will always round up the block size to the next multiple of 4. Say the user creates a memory pool, assining blocks to 6-byte wide; they will turn into 8-byte blocks._

==== How it works

When a routine calls `alloc()`, the address to be returned is the one a "free list" is pointing to, say `addr1`. Before returning `addr1` to the caller, we update the free list to point to the value stored within `addr1` - say `addr8` at that moment.

When a routine calls `free(addr1)`, we overwrite whatever has been written in addr1 with the value-free list point to (if no more `alloc()` were issued, it would still be `addr8`), and `addr1` becomes the free list head again.

Allocating and deallocating fixed-size blocks using this structure and storing meta-data this way is as deterministic (_O(1)_) and economic as we can get for dynamic memory allocation.

A drawback is if having a routine writing  non-allocated memory within a pool it will spoil the meta-data and the Allocator will fail.

==== Memory Allocator Determinism

The memory allocator (if well employed) will never fail; it might take the same amount of time to allocate and free a block. In the test below, three tasks with the same priority are allocating, increasing a counter, and freeing a block of _128 bytes_. If the allocator exhibits deterministic behaviour, these counters might differ by at most 1 whenever we pause the device.

====
```C

#include "application.h"

INT stack1[STACKSIZE];
INT stack2[STACKSIZE];
INT stack3[STACKSIZE];

RK_MEM bufPool;
#define BLOCK_SIZE	128
#define	N_BLOCKS	3
BYTE buf[N_BLOCKS][BLOCK_SIZE];


VOID kApplicationInit(VOID)
{
	kMemInit(&bufPool, buf, BLOCK_SIZE, N_BLOCKS);
}

volatile int counter1, counter2, counter3=0;

VOID Task1(VOID* args)
{
    RK_UNUSEARGS
	while (1)
	{
		BYTE* addr = kMemAlloc(&bufPool);
		kassert(addr!=NULL);
		RK_ERR err = kMemFree(&bufPool, addr);
		kassert(err==0);
		counter1++;
		kYield();
	}
}

VOID Task2(VOID* args)
{
    RK_UNUSEARGS
	while (1)
	{

		BYTE* addr = kMemAlloc(&bufPool);
		kassert(addr!=NULL);
		RK_ERR err = kMemFree(&bufPool, addr);
		kassert(err==0);
		counter2++;
		kYield();
	}
}

VOID Task3(VOID* args)
{
    RK_UNUSEARGS
	while (1)
	{

		BYTE* addr = kMemAlloc(&bufPool);
		kassert(addr!=NULL);
		RK_ERR err = kMemFree(&bufPool, addr);
		kassert(err==0);
		counter3++;
		kYield();
	}

}

``` 
====
Below are the results after ~2.5 million ticks of 0.5 ms.

image::images/determmem.png[width=75%] 

---

== *__Inter-task Communication__*

====
RK0 Blog:

- https://kernel0.org/2025/01/08/inter-task-communication-on-embedded-operating-systems/[About Inter-Task Communication - Part 1]

- https://kernel0.org/2025/01/09/about-inter-task-communication-p2/[About Inter-Task Communication - Part 2]

====


Inter-task communication refers to the mechanisms that enable tasks to coordinate/cooperate/synchronise by means of sending or receiving information that falls into two logical categories: _Events_ or _Messages_. 

* *__Events__*:
An event is solely defined by its absence or presence. Events are used to notify tasks that something has occurred. Nothing else. Formally, _events are pure signals_ - the meaning is implicit.

* *__Messages__*: When the operations used for tasks to communicate also allow conveying a _payload_, these mechanisms are regarded as _Message Passing_.


=== Sleep-Wake Events

[width="100%",options="header"]
|===============================
|Event Control Block
|Sleeping Queue
|Timeout Node
|===============================

The simplest mechanism to handle events are the methods `sleep()`, `wake()` and `signal()` acting on an `EVENT` kernel object.

The sole purpose of the `EVENT` kernel object is to have a unique waiting queue associated to an event. There is no other data -- e.g., one cannot know if the event has ever happened. 

Thus, the operation `sleep(&event, timeout)`  _always_ put to the caller task to sleep (except if using a timeout=0, the call has no effect). 
 
A `signal(&event)` will wake-up a single task - the highest priority.

A `wake(&event)` is a _broadcast_: all sleeping tasks will switch to `READY`.

If there are no tasks sleeping for an event these operations will have no effect at all -- _signals are lost._

While this mechanism _is useful as is_, it finds larger applicability when composed with _Mutexes_ to create _Condition Variables_.

[NOTE]
====
All blocking calls accept a timeout parameter. To make it non-blocking, the parameter `NO_WAIT` is passed. 

When used within ISRs, any blocking call must have its timeout set to `NO_WAIT`.
====

=== Direct Signals 

[width="100%",options="header"]
|===============================
|Within Task Control Block
|Current Flags
|Required Flags
|Options
|===============================

_(This mechanism does not mimic POSIX or UNIX/BSD Signals.)_

Each Task Control Block stores event notifications other tasks will raise. Often we define that a 32-bit Signal carries 32 _signal/event flags_ -- it can represent a combination of 32 different events, if defining 1 event/bit. A bit  set means an event is pending to be detected. A detected event is always _consumed_, that is, the bit is cleared. 


Bitwise friendly, the API is written as `set()` (as to signal/post), `get()` (as to wait/pend). 

A task checks for a combination of events it is expecting. This combination can be satisfied if `ANY` (OR logic) of the required bits are set or if `ALL` of the required bits are set (AND logic).

Thus, if the condition is not met the task can optionally suspends, switching to the logical state `PENDING`.

When another task issues a `set()` which result satisfies the waiting condition, the task state is then `READY`. The _matched flags are consumed_ (cleared).
A _set_ is always an _OR_ operation of an input mask over the current value.

Others operations are to `query` and to `clear` its own signal flags.

One possible usage pattern is a task's cycle begins checking for any events (it is able/supposed to handle).

If using it on a supervisor task -- it can create a neat event-driven pattern for a soft/firm real-time system.

```C

VOID SupervisorTask(VOID *args)
{
    RK_UNUSEARGS
    
    ULONG gotFlags = 0UL;

    while(1)
    {
        /*  range: 0x01-0xFFFF, any bit. store in gotFlags. do not block.*/
        RK_ERR err = kSignalGet(0xFFFF, RK_FLAGS_ANY, &gotFlags, RK_NO_WAIT); 
        if (err == RK_SUCCESS)
        {
           
            if (gotFlags & PENDING_AIRFLOW_INCREASE):
            {    /* notify actuator's task with the proper signal */
                   kSignalSet(airFlowTaskHandle, AIRFLOW_INCREASE_SIGNAL);
            } 
                /* others... */
            
        
        }
    
        kSleepUntil(SUPERVISOR_T_PERIOD);
    }

}

```

Task Signals are the the only synchronisation primitive that cannot be enabled/disabled.

`0x00` is invalid for both `set()` and `get()` operations.

=== Semaphores
[width="100%",options="header"]
|===============================
|Semaphore Control Block
|Counter (Signed Integer)
|Semaphore Type (Counter/Binary)
|Waiting Queue
|Timeout Node
|===============================

==== Counter Semaphores
A _counter_ semaphore is an event counter. It means the primitives `post()` and `pend()` will increase (record the event) and decrease (consume the event), respectively, the signed value 'N' of a given semaphore (a semaphore cannot be initialised with a negative value). 

When `pend()` returns a negative value, the caller is blocked within the semaphore queue. The negative value of a counter semaphore immediately informs us how many tasks are blocked waiting for a signal. 

After becoming negative, every signal issued to a semaphore releases a task until its counter reaches 0—meaning there are no enqueued tasks or recorded events.

Thus, tasks cooperate over a public semaphore by signalling and waiting for events. A counter semaphore is often seen as a "credit tracker" as it can be used to verify (wait/pend) and indicate (signal/post) the availability of a countable resource -- say, number of slots within a queue.

==== Binary Semaphores
A special case of counter semaphore is a binary semaphore - it counts up to 1 and down to 0 -- it is also said that the semaphore is either _FULL_ or _EMPTY_. A task that pends on an _EMPTY_ binary semaphore is _BLOCKED_. 

For a signal/post, there is a subtle difference from counter semaphores - the counter flips from 0 to 1 only and if only there are no tasks pending on the semaphore. On the other hand, a wait/pend on a binary semaphore that is 1 always decrements the counter to 0. 


Binary semaphores are typically used for task-to-task synchronisation/notification - unilateral or bilateral - and although not ideal, it is common to see binary semaphores used for mutual exclusion.


Note the _Task Flags_ described earlier can be seen as a pack of _private binary semaphores_. Because they are private, only the task itself can wait (pend) on it, but any task can signal (post).


_PS: often on the snippets it is written `kSignal(taskHandle)/kPend(timeout)` -- this is an application-defined *alias* for when a task is using a single bit of its signal flags, as if having a single binary private semaphore._

====


The snippet below shows two tasks lock-stepping by posting and pending on (binary) semaphores. 
Task2 depends on Task1 finishing 'work1' to perform 'work2'. And vice-versa.

(Note Direct Signals are a better choice for this use-case.)


```C

RK_SEMA work1Sema; 
RK_SEMA work2Sema; 

VOID kApplicationInit(VOID)
{
/* semaphores init at 0 */
	kSemaInit(&work1Sema, RK_SEMA_BIN, 0); 
	kSemaInit(&work2Sema, RK_SEMA_BIN, 0);  

}

VOID Task1(VOID* args)
{
    RK_UNUSEARGS
	while (1)
	{
	    doWork1();
		kSemaPost(&work1Sema);
		kSemaPend(&work2Sema, RK_WAIT_FOREVER);
		 /* T1 finished. Waiting for T2. */ 
    
	}
}
VOID Task2(VOID* args)
{
    RK_UNUSEARGS
	while (1)
	{
		 kSemaPend(&work1Sema, RK_WAIT_FOREVER);
		 doWork2();
		 kSemaPost(&work2Sema);
	}
}
``` 
====



=== Mutex  

[width="100%",options="header"]
|===============================
|Mutex Control Block
|Locked State (Boolean)
|Owner
|Waiting Queue
|Timeout Node
|===============================

Some code regions are critical in that they cannot be accessed by more than one task at once. Acquiring a mutex before entering a region and releasing it when leaving makes that region mutually exclusive.
A mutex is a lock with a notion of ownership: only the task that owns a mutex can unlock it.

If a task tries to acquire an already locked mutex, it switches to `BLOCKED` state until the mutex is unlocked by its owner; thus the highest priority task waiting to acquire the resource is dequeued, as on semaphores. However, unlike semaphores, the complementary operation `unlock()`, when issued by a non-owner, has undefined behaviour. In K0, it will be a hard fault. 

====

The snippet below shows a _consumer-producer_ pattern for a buffer with K slots (_bounded buffer pattern_). Two semaphores track the number of slots for the producer and items for the consumer. The mutex prevents any write or read from being disrupted.

```C
RK_SEMA  item;  
RK_SEMA  space;
RK_MUTEX lock;
#define N (K) /*some K>1*/
typedef struct mesg
{
 UINT field1;
 UINT field2;
 UINT field3;
 UINT field4;
} Mesg_t; /* a 16-byte message */

/* a ring buffer of messages */
Mesg_t mailbox[N]={0};

kApplicationInit(VOID)
{
    kSemaInit(&item,  RK_SEMA_COUNTER, 0); /* no items */    
    kSemaInit(&space, RK_SEMA_COUNTER, N); /* N buffers available */
    kMutexInit(&lock);
}
/* circular buffer handling ommited */

/* wait for space, lock, write, unlock, signal there is item */
VOID PostMail(Mesg_t* sendPtr)
{
    kSemaWait(&space, RK_WAIT_FOREVER);
    kMutexLock(&lock, RK_INHERIT, RK_WAIT_FOREVER);
    memcpy(&mailbox[tail], sendPtr, sizeof(Mesg_t));
    kMutexUnlock(&lock);
    kSemaSignal(&item);
}

/* wait for item, lock, read, unlock, signal there is space */
VOID PendMail(Mesg_t* recvPtr)
{
    kSemaWait(&item, RK_WAIT_FOREVER);
    kMutexLock(&lock, RK_INHERIT, RK_WAIT_FOREVER);
    memcpy(recvPtr, &mailbox[head], sizeof(Mesg_t));
    kMutexUnlock(&lock);
    kSemaSignal(&space);
}
```

====


Mutexes are solely for mutual exclusion; they cannot be used for signalling. It is common  to see Counter Semaphores initialised as 1, or Binary Semaphores used for mutual exclusion. 

However, particularly for a counter semaphore, if the count increases twice in a row, the mutual exclusion is gone. For both, _Priority Inversion_ can become a problem, as will be explained. 

==== Priority Inversion


Let TH, TM, and TL be three tasks with priority high (H), medium (M) and low (L), respectively. Say TH is dispatched to be blocked on a semaphore 'TL' has acquired. Say 'TM' is dispatched, and it does not need the resource 'TL' holds. It will pre-empt 'TL'.

Now 'TH' has an _unbounded waiting time_ because any task with priority higher than 'L' that does not need the resource indirectly prevents it from being unblocked.

Mutexes in K0 can implement a protocol called priority inheritance. While holding the resource, 'TL' will have its priority raised to 'H', so 'TM' can no longer pre-empt it. Thus, consider using mutexes for resource sharing.



=== Condition Variables

Composing _Events_ and _Mutexes_   leverages _Condition Variables_. Whenever a task needs to test for a condition before proceeding, it locks a mutex to test the condition within a critical region. If the condition evaluates true, it proceeds and unlock the mutex at the end.

If the condition is evaluated as false, the task goes to sleep - to wait for a wake signal when the condition is true. The detail is that it goes to sleep and unlocks the mutex all in an atomic operation. When task is awaken it locks the mutex again. 
The example below, shows a _Synchronisation Barrier_. Three tasks need to reach a specific point of the program before proceeding. This is done by calling a function `synch()`. When a third task enters the synchronisation barrier, it does not sleep; instead it broadcasts a waking signal to the other two. 

Note that the mutex enforces a single active task within the barrier. They enter and leave on a 'turnstile'.

====
```C

/* Synchronisation Barrier */

RK_EVENT syncEvent; 
UINT syncCounter; 
RK_MUTEX syncMutex; 
#define SYNC_CONDITION (syncCounter>=3) 

VOID kApplicationInit(VOID)
{
	kMutexInit(&syncMutex);
	kEventInit(&syncEvent);
	syncCounter = 0;
}

static VOID synch(VOID)
{
	kMutexLock(&syncMutex, RK_NO_INHERIT, RK_WAIT_FOREVER);
	syncCounter += 1;
	if (!(SYNC_CONDITION))
	{
	    /* must be atomic */
	    kDisableIRQ();
		kMutexUnlock(&syncMutex);
		kEventSleep(&syncEvent, RK_WAIT_FOREVER);
		kEnableIRQ();
		/* task wakes here */
    	kMutexLock(&syncMutex, RK_NO_INHERIT, RK_WAIT_FOREVER);
	   
	}
	else
	{
        kPuts("All task synch'd.\n\r");
        syncCounter = 0;
		kEventWake(&syncEvent);
	
    }
    /* every task that leaves, unblock a task from the mutex waiting queue */
    kMutexUnlock(&syncMutex);
}

VOID Task1(VOID* args)
{
    RK_UNUSEARGS
	while (1)
	{
		kSleep(5);
        kPuts("Task 1 is synching...\n\r");
        synch();
        
	}
}
VOID Task2(VOID* args)
{
    RK_UNUSEARGS
	while (1)
	{
		kSleep(8);
        kPuts("Task 2 is synching...\n\r");
        synch();
	}
}
VOID Task3(VOID* args)
{
    RK_UNUSEARGS
	while (1)
	{
		kSleep(3);
        kPuts("Task 3 is synching...\n\r");
        synch();
	}
}



```

image:images\syncbarr.png[width=20%]

====

Monitor-like constructions follow the pattern:
====
```C 
 lock();
 while(!condition) 
 { 
    unlock(); 
    sleep(); 
    lock(); 
 } 
```
====
that is, the task loops testing the condition. To illustrate a monitor-like construction, consider the producer-consumer problem for a buffer with a single slot
(aka Mailbox):
====
```C
ULONG        *mailPtr = NULL; 
RK_EVENT     notFull;
RK_EVENT     notEmpty;
RK_MUTEX     lockMail;
VOID kApplicationInit( VOID)
{
    kEventInit(&notFull);
    kEventInit(&notEmpty);
    kMutexInit(&lockMail);
}
VOID MailSend( UINT* sendPtr)
{
     kMutexLock(&lockMail, RK_INHERIT, RK_WAIT_FOREVER);
     while (mailPtr != NULL) /* mailbox is full */
     {
        /*atomic unlock, sleep */
         kDisableIRQ(); 
         kMutexUnlock(&lockMail);
         kEventSleep(&notFull, RK_WAIT_FOREVER);
         kEnableIRQ();
         /* wake when not full, lock and test again */
         kMutexLock(&lockMail, RK_INHERIT, RK_WAIT_FOREVER);
     }
     /* deposit mail, signal a consumer, and unlock */
      mailPtr = sendPtr;
      kEventSignal(&notEmpty);
      kMutexUnlock(&lockMail);
}

VOID MailRecv( UINT** recvPPtr)
{
     kMutexLock(&lockMail, RK_INHERIT, RK_WAIT_FOREVER);
     while (mailPtr == NULL) /* mailbox is empty */
     {
         kDisableIRQ();
         kMutexUnlock(&lockMail);
         kEventSleep(&notEmpty, RK_WAIT_FOREVER);
         kEnableIRQ();
         kMutexLock(&lockMail, RK_INHERIT, RK_WAIT_FOREVER);
     }
     /* extract mail, signal a producer, and unlock */
      *recvPPtr = mailPtr; /*  copies the value of mailPtr to *recvPPtr */
      mailPtr = NULL; /* empty mailbox */
      kEventSignal(&notFull);
      kMutexUnlock(&lockMail);
}
```
====



[NOTE]
====
In real-time applications, Message Passing often encounters the following scenarios:

- Some messages are consumed by tasks that can't do anything before processing information — thus, these messages end up also being signals. For Example, a server needs (so it blocks) for a command to process and/or a client that blocks for an answer. 

- A particular case of the above scenario is fully synchronous: client and server run on _lockstep_.

- Two tasks with different rates need to communicate, and cannot lockstep. A faster producer might use a buffer to accommodate a relatively small burst of generated data, or a quicker consumer will drop repeated received data. 

- Other times, we need to correlate data with time for processing, so using a queue gives us the idea of data motion. _Eg., when calculating the mean value of a transductor on a given period_.

- For _real-time_ tasks such as servo-control loops, past data is useless. Consumers need the most recent data for processing. For example, a _drive-by-wire_ system, or a robot deviating from obstacles. In these cases the message-passing must be lock-free while guaranteeing data integrity.
====


=== Mailbox 
[width="100%",options="header"]
|===============================
|Mailbox Control Block 
|Mail Address
|Waiting queue
|Owner Task*
|===============================

While in GPOS jargon, mailboxes are queues of messages - as a distinction from pipes (that are byte streams) - in embedded system software, often mailboxes are said to have a capacity of a single item, and more recently, you will not find it as a distinct mechanism - you use a 1-item queue.  

A Mailbox allows a task to exclusively write (post) and read (pend) a memory region and to be notified when another task writes or reads to it. Therefore its typical operation provides mutual exclusion and notification altogether: _very handy_.

A message within a mailbox is the address of an object. The sender and receiver agree on the concrete mail implementation as part of the mail interface contract; also the data pointed to has to remain unchanged until the receiver 'consumes' it. That is another part of the contract.

The semantics are simple: a Mailbox will be `EMPTY` when its storage points to `NULL`; otherwise, it is `FULL`. The mailbox will be empty/full after a successful `pend()`/`post()` operation.

When a producer `post()` to a `FULL` mailbox, it (optionally) blocks and is placed in the Mailbox waiting queue. The associated task will switch to the state `SENDING`.

Likewise, a consumer (optionally) blocks when issuing a `pend()` on an empty Mailbox. The task status switches to `RECEIVING,` and is enqueued in the mailbox waiting queue.

[NOTE]
A mailbox can be initialised as FULL if the initial pointer provided is non-null. 

Typical use-case is when one wants to deliver a signal along with a payload--a _message as a signal_.


Besides `post()` and `pend()`, other primitives are `peek()` to read without removing (non-destructive) and `postovw()` to overwrite whatever is in a full mailbox. 


_* we discuss ownership on message passing later._

[TIP]
Passing Messages by reference is a typical “embedded thing” – because it is cheap, deterministic and DMA-friendly. 

#### Usage Example: _Zero-Buffer_ Channel

Some communications are unreliable or important enough so we need guarantees that not only the message could be sent, but also that it could be read. 

On a _zero-buffer_ channel we do not allow messages to be waiting so they are picked. The sender blocks, waiting for a confirmation that the message was retrieved by the receiver:

====
```C

/*(...) details ommited */

/* sender needs to be sure message has arrived */
SenderTask:

   err = kMboxPost(...., timeout);
   if (err=ERR_TIMEOUT)
       retryPost();
   if (err==success)
   {
       /* pend on private bin semaphore, to wait for confirmation it was read */
       err = kPend(timeout); 
       if (err == ERR_TIMEOUT)
       /* receiver did not ack before time-out */
   
   }


ReceiverTask:
    err = kMboxPend( ..., timeout);
    if (err==ERR_TIMEOUT)
        retryPend();
    if(err==SUCCESS)
       /* post to sender's semaphore, to ack message was received */
       kSignal(senderTaskHandle); 

 
/* using a mailbox instead of a binary semaphore */
 

K_MBOX reqBox; /* request message */
K_MBOX ackBox; /* ack message */

SenderTask:

   err = kMboxPost(&reqBox, &reqMesg, timeout);
   if (err=ERR_TIMEOUT)
       retryPost();
   if (err==SUCCESS)
   {
       /* the acknowledgment mail can be a dummy message */

       err = kMboxPend(&ackBox, &recvmesg, timeout); 
       if (err == ERR_TIMEOUT)
       /* receiver did not ack before time-out */
   
   }


ReceiverTask:
    err = kMboxPend( ..., timeout);
    if (err==ERR_TIMEOUT)
        retryPend();
    if(err==SUCCESS)
    {
       err = kMboxPost(&ackBox, &ackMesg, timeout); 
       if (err==ERR_TIMEOUT)
       /* in this case, the sender has not retrieved
          a previous ack */
    }

```
====


Mailboxes are well-suited for 1:1 communication - fully synchronous (lockstep) command-response or when a task waits for a notification plus a payload (say, the last data read by an Interrupt routine). 

#### Example: Multi-client-server synchronous command-response

The snippet below presents _two clients_ and one server on a lock-step communication.

It is shown to stress how data scope is kept and can be lost. In this case,
_both client and server blocking for a response/ACK_ keeps the data scope. 


=====

```C
/* this example includes  <string.h> for convenience */
 
RK_MBOX serverReqMbox; /*  server incoming commands */
RK_MBOX serverAckMbox; /*  server incoming reponse acks */
RK_MBOX clientMbox1;   /*  response for client 1 */
RK_MBOX clientMbox2;   /* response for client 2 */

/* Command Requests are assembled on an Application Data Unit */
typedef struct
{
    BYTE length; /* Length of the APDU payload */
    BYTE payload[32]; /* APDU payload */
    RK_MBOX *replyMbox; /* Pointer to the client's reply mailbox */
} APDU __attribute__((aligned(4))); 

void kApplicationInit(VOID)
{
    kMboxInit(&serverReqMbox,  NULL);
    kMboxInit(&serverAckMbox, NULL);
    kMboxInit(&clientMbox1, NULL);
    kMboxInit(&clientMbox2, NULL);

}

/* Highest Priority */
/* the server response is to ECHO the request back to the client; then it pends on a mailbox waiting the client to acknowledge the response. so it proceeds to process further requests.  */ 

VOID ServerTask(VOID* args)
{
    RK_UNUSEARGS

    APDU *request, response;
    UINT* ackResp;
    while (1)
    {
        /* Wait for a request */
        if (kMboxPend(&serverReqMbox, (VOID **)&request, RK_WAIT_FOREVER) == RK_SUCCESS)
        {
            kprintf("[SERVER] RECV: %s\n\r", request->payload);

            /* Process the request */
            response.length = (BYTE) snprintf((char*) response.payload,
                    sizeof(response.payload), "ECHO %s",
                    request->payload);

            /* Echo to client's reply mailbox */
            if (kMboxPost(request->replyMbox, &response, RK_WAIT_FOREVER) != RK_SUCCESS)
            {
                kprintf("ECHO fail\n\r");
            }
            if (kMboxPend(&serverAckMbox, (VOID **)&ackResp, RK_WAIT_FOREVER) == RK_SUCCESS)
                kprintf("[SERVER] CLIENT %d SERVED.\n\r", *ackResp);
            /* now it is safe to process another request */
        }
    }
}
/* same priority as Client2 */
VOID Client1Task(VOID* args)
{
    RK_UNUSEARGS

    APDU request, *response;

    while (1)
    {
        /* Prepare the request */
        snprintf((char*) request.payload, sizeof(request.payload),
                "Hello from Client 1");
        request.length = (BYTE) strlen((char*) request.payload);
        request.replyMbox = &clientMbox1; /* Specify the reply mailbox */

        /* Send the request to the server */
        if (kMboxPost(&serverReqMbox, &request, RK_WAIT_FOREVER) == RK_SUCCESS)
        {

            /* Wait for the response */
            if (kMboxPend(&clientMbox1, (VOID **)&response, RK_WAIT_FOREVER)
                    == RK_SUCCESS)
            {
                kprintf("[CLIENT #1] RECV: %s\n\r", response->payload);
                UINT ack=1;
                kMboxPost(&serverAckMbox, &ack, RK_WAIT_FOREVER);
                /* now it is safe to send another request */
            }
            else
            {
                kprintf("1F\n\r");
            }
        }
        else
        {
            kprintf("1F\n\r");
        }

    }
}
VOID Client2Task(VOID* args)
{
    RK_UNUSEARGS
    APDU request, *response;

    while (1)
    {
        /* Prepare the request */
        snprintf((char*) request.payload, sizeof(request.payload),
                "Hello from Client 2");
        request.length = (BYTE) strlen((char*) request.payload);
        request.replyMbox = &clientMbox2; /* Specify the reply mailbox */

        /* Send the request to the server */
        if (kMboxPost(&serverReqMbox, &request, RK_WAIT_FOREVER) == RK_SUCCESS)
        {

            /* Wait for the response */
            if (kMboxPend(&clientMbox2, (VOID **)&response, RK_WAIT_FOREVER)
                    == RK_SUCCESS)
            {
                kprintf("[CLIENT #2] RECV: %s\n\r", response->payload);
                UINT ack=2;
                kMboxPost(&serverAckMbox, &ack, RK_WAIT_FOREVER);
            }
            else
            {
                kprintf("2FAIL\n\r");
            }
        }
        else
        {
            kprintf("2FAIL\n\r");
        }

    }
}
```
=====

image::images\clientserver.png[width=30%]

Had the server not block waiting for an ACK, the former response would be overwritten before a client could have read it - given how priorities are set. To accomodate two clients while still passing by reference, the server would need to keep the response on different buffers. 

If a copy was passed as a reponse, the server would not need to block for an ACK, provided the response was sent before receiving another request.


=== Signals as a Direct Channel

_Direct Signals_ is the only Inter-Task Communication service that is always enabled. It can also be used for a message-passing -- and no, it is not a "hack".

The kernel does not provide it as a explicit service though, because there at least two well-defined semantics that can be used: either first-message semantics, or last-message semantics. _The user knows better_.

A _possible_ pattern is as follows -- in this case an _unbuffered direct channel_:

====
```C
/* first-message semantics */
RK_ERR send(RK_TASK_HANDLE const taskHandle, ULONG const mesg)
{
    /* return error if there is a message */
    /* for last message semantics, skip this check */
    ULONG query=0;
    RK_ERR err = kSignalQuery(taskHandle, &query);
    if (err < 0) 
        return (err);
    if (query != 0) /* receiver is 'full' */
        return (RK_ERROR);
    err = kSignalSet(taskHandle, mesg);
    return (err);
}


RK_ERR recv(ULONG *const recvPtr, RK_TICK const timeout)
{
    /* require all bits set, wait for any, when returning
    all bits are cleared */
    RK_ERR err = kSignalGet(0xFFFFFFFF, RK_FLAGS_ANY, recvPtr, timeout);
    return (err);
}

/***** 

Note that ZERO is not a valid parameter for a Signal. 

You can establish a contract such as splitting the 32-bit message on different fields; besides, you can always pass a pointer:

*****/


/* EXAMPLE: fully synchronous message-passing */

struct mesg
{
    RK_TASK_HANDLE senderHandle;
    CHAR   mesg[8];
    
    /* others fields */
    
} __K_ALIGN(4);  

typedef struct mesg Mesg;

#define ACK 0x01U

VOID RecvTask( VOID *args)
{
    RK_UNUSE_ARGS
    
    ULONG mesg = 0; 
    
    while(1)
    {
        /* pend for receiving */
        recv(&mesg, RK_WAIT_FOREVER);
        
        /* treat 4-byte received value as a pointer to Mesg */
        Mesg* mesgPtr = (Mesg*) mesg;
        
        /* process mesg, whatever it means */
        doWork(mesgPtr);
        
        /* ack reception */
        kSignalSet(mesgPtr->senderHandle, ACK);
    
    }
}


VOID SenderTask( VOID *args)
{
    RK_UNUSE_ARGS
    
    Mesg sendMesg;

    while(1)
    {
        /* generate message */
        writeMesg(sendMesg, ...);
        
        /* pass the address of sendMesg as a ULONG */
        RK_ERR err = send(recvTaskHandle, (ULONG)&sendMesg);
      
        /* block for an ACK */
        kSignalGet(ACK, RK_FLAGS_ALL, NULL, RK_WAIT_FOREVER);
        
    }
    
}
```
====

=== Message Queues

The classic Message Queue on UNIX SVR4 is defined as the 'head of a linked list of messages'. Some RTOSes implement Message Queues using linked lists, in which case a central pool of buffers might exist. 

The design approach in RK0 does not use lists for message queues  -- lists add processing and memory overhead and are suitable for _unbounded_ queues.

As _unbounded_ is a forbidden word in _RK0_ design, every message queue has a fixed capacity along with a _dedicated_ pool of buffers, avoiding contention. 

Two mechanisms for enqueueing messages are offered:

- A _Mail Queue_ (or a _Queue_) is a 'multi-item' Mailbox—it holds multiple generic pointers as messages. 

- A _Stream Queue_ (or a _Stream_) is a ring buffer of N fixed-size messages (word-aligned). _Streams perform deep copies_ - from sender storage to the stream buffer, and from the stream buffer to receiver storage.

They are offered as different mechanisms because they have different best-use cases and semantics.


==== Mail Queue

[width="100%",options="header"]
|===============================
|Queue Control Block
|Buffer Address
|Write Position
|Read Position
|Max. number of mails
|Current number of mails
|Waiting queue
|Owner Task  
|===============================

Mail Queues (or just _Queues_) are Mailboxes that can hold several messages in a FIFO queue. Indeed, a Mail Queue with a size of 1 will behave as a Mailbox.

The programmer must provide a buffer to hold N message addresses for a Queue. The main primitives are `post(), pend(), peek(), and jam().` 

_Peek_ reads the Queue front message without extracting it, and _Jam_ places a message on the queue front so that this message will be _Last-In-First-Out_.

Mails will be enqueued in a FIFO order (except when using `jam()`). 

[NOTE]
====
A single-slot Queue behaves as a Mailbox. Still Mailboxes are provided as a distinct service from Queues because a Queue Control Block is roughly three times larger than a Mailbox, plus Queue methods are considerably heavier. As Mailboxes are extremely handy, providing them as a standalone mechanism allows composing them with other features while keeping Queues disabled entirely.
====

For both Queues and Mailboxes, if your message is a 4-byte message -- such as an UINT value -- they can (and probably should) be passed by copy: just cast to (VOID*) when transmitting, and cast back to UINT when receiving. _Yet, this should be an option only if you are unwilling to use Streams._  

##### Usage Example: Asynchronous 'Zero-copy' Message Passing 

Queues purpose is to transmit the pointer of a message that is kept on a memory block. Two example below demonstrates its usage.

====
```C
struct mesg
{
    UINT key;
    const CHAR* string; /* a shallow copy will not get this */
};

#define N_MESG 8
#define MESG_SIZE sizeof(struct mesg)

BYTE mesgPool[N_MESG][MESG_SIZE]; /* pool of mesg buffers */
struct mesg* buf[N_MESG]; /* to store addresses */

RK_MEM mem; /* allocator */
RK_QUEUE mqueue; /* queue */

/* for testbench */
const CHAR *messages[N_MESG] =
{ "Message 0", "Message 1", "Message 2", "Message 3", "Message 4", "Message 5",
        "Message 6", "Message 7" };

VOID kApplicationInit(VOID)
{
    /* init allocator */
    kMemInit(&mem, (VOID *) mesgPool, MESG_SIZE, N_MESG);
    /* init mailbox */
    kQueueInit(&mqueue, (VOID *) buf, 8);
}
VOID Task1(VOID* args)
{
    RK_UNUSEARGS
    UINT i = 0;
    struct mesg *sendPtr;
    while (1)
    {
        /* allocate buffer */
        sendPtr = NULL;
        /* sendPtr points to a pool mesgPool address */
        sendPtr = (struct mesg*) kMemAlloc(&mem);
        if (sendPtr != NULL)
        {

            sendPtr->key = i;
            sendPtr->string = messages[i];
            kprintf("Sending: %s \n\r", sendPtr->string);
            /* mesgPool address is enqueued */
            kQueuePost(&mqueue, sendPtr, RK_WAIT_FOREVER);
            i += 1;
            i %= N_MESG;
        }
        else
        {
            kYield(); /* no more mesg buffers, yield */
        }
    }
}

VOID Task2(VOID* args)
{
    RK_UNUSEARGS
    struct mesg *recvPtr = NULL;
    while (1)
    {
        kQueuePend(&mqueue, (VOID **) &recvPtr, RK_WAIT_FOREVER); /* will block when empty */
        kprintf("Received: %s \n\r", recvPtr->string);
        kBusyDelay(2); /* pretend working */
        kMemFree(&mem, (VOID *) recvPtr); /* free memory */
    }
}

``` 

image::images/mboxqueue.png[width=25%]
====

The data scope is managed by allocating a different buffer for every `post()`, and the receiver is accountable for deallocating the buffer after consuming the message. 
The receiver gets an address of a message. The design must guarantee its integrity. After consuming the contents, the receiver frees the memory block.

##### Usage Example: Work Queue

This example demonstrates how to implement a work queue pattern using a Mail Queue.

Multiple producer tasks (Sensor, PID Controller, and UI) create Job objects and submit their addresses to a Mail Queue (jobQueue).

In this example the worker thread is logging what is happening on the system. As it runs on lowest priority, it keeps the system responsiveness with minimal intrusion.
 
The same pattern can support actual processing. You could either embed a function pointer in each job for fully dynamic behaviour, or define a command ID and use a central dispatch table in the worker thread to invoke appropriate handlers. These are all implementations of the _Active Object Pattern_.

====

```C
 


/* Job and queue definitions */
#define MAX_JOBS 8

typedef struct {
    BYTE length;
    BYTE payload[64];
} Job;

static Job jobPoolBuf[MAX_JOBS];
static RK_MEM jobPool;
static Job *jobQueueBuf[MAX_JOBS];
static RK_QUEUE jobQueue;

/* Plant model state */
static volatile float plantTemp = 25.0f;
static const float ambientTemp = 20.0f;

/* Convert plantTemp to integer for logging */
INT readTemp(VOID)
{
    return (INT)plantTemp;
}

/* Simulate button every 2s */
INT buttonPressed(VOID)
{
    return ((kTickGet() % 2000) < 20); /* the condition will hold true for 20ms every 2s */
}

VOID kApplicationInit( VOID)
{
    kMemInit( &jobPool, jobPoolBuf, sizeof(Job), MAX_JOBS);
    kQueueInit( &jobQueue, jobQueueBuf, MAX_JOBS);
}

/* PID Controller Task (High priority) */
/* note: this is a sloppy zero-effort tunning 
just for printing something */
VOID PIDControllerTask( VOID *args)
{
    RK_UNUSEARGS
    
    const float Kp=1.0f, Ki=0.1f, Kd=0.05f;
    float prev=plantTemp;
    float integral=0.0f;
    const float dt=0.5f;

    while(1)
    {
        /* Read plant state */
        float measure = plantTemp;
        /* PID compute */
        float error = 25.0f - measure;
        integral += error * dt;
        float derivative = (measure - prev) / dt;
        float output = Kp*error + Ki*integral - Kd*derivative;
        prev = measure;

        /* Apply to plant model */
        /* the plant cooling model: (temp-amb)*0.1  */
        plantTemp += (output - (plantTemp - ambientTemp)*0.1f) * dt;

        /* Post log job */ 
        
        Job *job = kMemAlloc( &jobPool);
        if(job)
        {
            CHAR buf[32];
            formatFloat (buf, sizeof(buf), output);
            snprintf( (CHAR*)job->payload, sizeof(job->payload),
                     "[CTRL] O=%s T=%d", buf, readTemp());
            job->length = strlen((CHAR*)job->payload);
            if(kQueuePost( &jobQueue, job, RK_NO_WAIT) != RK_SUCCESS)
            {
            /*as the worker thread is freeing the memory blocks 
             if the queue is full and we do not want to block
             we free the allocated memory; otherwise it would leak
            */
                kMemFree( &jobPool, job);
            }
        }
        kSleepUntil( 500);
    }
}

/* Sensor Task (Mid priority) */
VOID TempSensorTask( VOID *args)
{
    RK_UNUSEARGS
    while(1)
    {
        Job *job = kMemAlloc(&jobPool);
        if(job)
        {
            snprintf( (CHAR*)job->payload, sizeof(job->payload),
                     "[SENSOR] T=%dC", readTemp());
            job->length = strlen((CHAR*)job->payload);
            if(kQueuePost( &jobQueue, job, RK_NO_WAIT) != RK_SUCCESS)
            {
                kMemFree( &jobPool, job);
            }
        }
        kSleepUntil(1000);
    }
}

/* UI Task (Low priority) */
/* this is to cause a temperature disturbance */
VOID UIButtonTask( VOID *args)
{
    RK_UNUSEARGS
    while(1)
    {
        if(buttonPressed())
        {   
            plantTemp -= plantTemp*0.15f  /* disturb the temperature */
            Job *job = kMemAlloc( &jobPool);
            if(job)
            {
                snprintf((CHAR*)job->payload, sizeof(job->payload),
                         "[BTN] Temp: %d", (INT)plantTemp);
                job->length = strlen((CHAR*)job->payload);
                if(kQueuePost( &jobQueue, job, RK_NO_WAIT) != RK_SUCCESS)
                {
                    kMemFree( &jobPool, job);
                }
            }
        }
        kSleepUntil( 2000);
    }
}

/* Worker Task (Lowest priority) */
VOID WorkerTask( VOID *args)
{
    RK_UNUSEARGS
    Job *job=NULL;
    while(1)
    {
        if(kQueuePend( &jobQueue, (VOID**)&job, RK_WAIT_FOREVER)==RK_SUCCESS)
        {
            printf("[WORKER] %s\n", job->payload);
            kMemFree( &jobPool, job);
        }
    }
}


``` 
image::images/workqueue.png[width=25%]

====

==== Stream Queue
[width="100%",options="header"]
|===============================
|Message Stream Control Block
|Storage address
|Write Address
|Read Address
|Message Block Size
|Max of messages
|Message Count
|Owner Task  
|===============================
Streams resemble classic (named) Pipes. The difference is that messages have _a fixed size_. On the other hand, pipes transmit and receive any number of bytes for each operation.
 
For each Stream, the user provides a buffer address with enough capacity (number of messages _x_ message size). Ther kernel will handle it as a ring buffer. 

The message size associated with a Message Stream instance is defined on its initialisation. On transmission, a _deep copy_  of a message from the sender's storage to the queue takes place; on reception, it moves from the queue to the receiver's storage. 

[NOTE]
Although a message size is associated with a Stream Queue object, the concrete message type depends on the application.


The important primitives for Message Streams are `send()`, `recv()`, `jam()` and `peek()`.

Sending to a full queue (optionally) blocks the sender. Likewise, receiving from an empty queue. 

===== Stream Message-Size

*Stream Queues must have _fixed_ message-sizes multiples of a _WORD_. Besides, they must be a power-of-two: 1, 2, 4, 8, 16, 32... (words).*

_RK0_ does not establish an upper bound, although I would say that a good cap is 8 words for the regular _RK0_ target. One has to experiment, though. If a message becomes too large it is introducing prohibitive latency, the user needs to transmit the message address - i.e., configure the Stream to carry 1-word message-size.

- Load/Store instructions are optimised to fetch 32-bit words. If message size are bounded on a 4-byte boundary, these operations can be executed in a single cycle. 

- If larger than 1 word, the power-of-two (double-word), is a CPU-aware design choice  to prevent unalignment issues.

- Misaligned memory makes castings unsafe, leading to complex faults, performance penalties or undefined behaviour.



[TIP]
====
Deep Copies are usually needed on message passing, but introduce significant overhead.

Design choice: Be CPU-aware and constrain data-size to power-of-two words. 

Benefits: speeds up the copy, achieves more deterministic behaviour, improves run-time safety.

====

Code-wise, we optimise using pointer arithmetics on pointer to words:

```C
/* Optimised deep copy; guaranteed mesgSize>0 */
/* destPtr and srcPtr are pointers to a word */
#define RK_CPY(destPtr, srcPtr, mesgSize) \
do {                                   \ 
      while (--mesgSize)               \ 
      {                                \ 
     /* if mesgSize is 1, this is NOT executed */
        *(destPtr++) = *(srcPtr++)     \
      };                               \  
     /* the last or the only copy is executed now */
     *(destPtr++) = *(srcPtr++)       \ 
     DMB \ /* ensure order  */
   } while(0U)
   
```

===== Usage Example: Averaging Sensor Values

Below is an illustrative snippet of a _Queueing Pattern_. 

The goal is to calculate the average value of 4 types of sensors. 

Here is convienient to highlight an important aspect -- given its reactive nature, real-time system software is typically _I/O bounded_, tasks that are sensitive to I/O activity have higher priority than _CPU-bounded_ tasks, i.e., those processing data.

A task receives measured sensor values from an ISR on a periodic rate. (The ISR is emulated by a Soft Timer).

Then it enqueues this data to a consumer - that will process the average value for each of 4 sensors.

The inter-task communication is designed as follows:

. The producer pends on a Mailbox that ISR posts to.

. The data extracted from the Mailbox is placed on a queue that has the processing task as the consumer. 

. As the producer priority must be higher than the consumer, eventually the queue will get full. 

.  The first enqueued item is received by the consumer; then it pends on its private binary semaphore, when the dequeue operation results on an empty queue error. 

. From now on, the consumer will only be activated when the queue is full - the producer checks the number of items within the queue and signals the consumer.

. This is done with a purpose: the consumer will use the inactive producer time to offload the queue and process the average value. 

Here the queue size was set as 8 items. This is an arbritrary value; the optimal queue size would take into account the producer-consumer ratio and the worst execution time of both. 


====
```c
#define kPend(timeout) \
	do { kSignalGet(0x1, RK_FLAGS_ANY, NULL, timeout); } while(0)

#define kSignal(taskhandle) \
	do { kSignalSet(taskhandle, 0x01); } while(0)

typedef enum
{
	TEMPERATURE=1, HUMIDITY, CO2, FLOW
}SensorType_t;



/* sensor types */
struct sensorMsg
{
    SensorType_t sensorType;
    ULONG sensorValue;

};

typedef struct sensorMsg Mesg_t;

#define N_MESSAGE 8
#define MESSAGE_SIZE (sizeof(Mesg_t))/4 /* WORDS! */
#define N_SENSOR    4
#define AVG_WINDOW_SIZE   10 /* 10 samples */

RK_STREAM sensorStream;/* the stream kobject */
Mesg_t mesgBuf[N_MESSAGE] = {0};/* queue buffer */
RK_TIMER timerT1;
RK_MBOX sensorBox;
static Mesg_t sample = {0};
static UINT sampleErr;
VOID callBackISR( VOID *args)
{
    RK_UNUSEARGS
    sample.sensorType = (rand() % 4) + 1;
    switch (sample.sensorType)
    {
        case TEMPERATURE:
            sample.sensorValue = ( ULONG) rand() % 50;
            break;
        case HUMIDITY:
            sample.sensorValue = ( ULONG) rand() % 100;
            break;
        case CO2:
            sample.sensorValue = ( ULONG) rand() % 1000;
            break;
        case FLOW:
            sample.sensorValue = ( ULONG) rand() % 10;
            break;
        default:
            break;
    }
    RK_ERR err = kMboxPost( &sensorBox, &sample, RK_NO_WAIT);
    if (err != RK_SUCCESS)
        sampleErr ++;

}

VOID kApplicationInit( VOID)
{
    RK_ERR err = kStreamInit( &sensorStream, ( VOID*) mesgBuf, MESSAGE_SIZE,
    N_MESSAGE);
    kassert( err==RK_SUCCESS);
    err = kTimerInit( &timerT1, 3, 3, callBackISR, NULL, RK_TIMER_RELOAD);
    kassert( err==RK_SUCCESS);
    err = kMboxInit( &sensorBox, NULL);
    kassert( err==RK_SUCCESS);
}

VOID Task1( VOID *args)
{
    RK_UNUSEARGS
    Mesg_t *recvSample = NULL;
    while (1)
    {
        RK_ERR errmbox = kMboxPend( &sensorBox, ( VOID**) &recvSample,
                RK_WAIT_FOREVER);
        kassert( errmbox==RK_SUCCESS);
        ULONG nMesg = kStreamQuery( &sensorStream);
        if (nMesg <= N_MESSAGE - 1)
        {
            RK_ERR err = kStreamSend( &sensorStream, &sample, RK_NO_WAIT);
/* fill up queue and signal consumer task */
            if (err == RK_SUCCESS)
            {
                CHAR const *sensorTypeStr = NULL;
                if (recvSample->sensorType == 1)
                    sensorTypeStr = "TEMP";
                if (recvSample->sensorType == 2)
                    sensorTypeStr = "HUM";
                if (recvSample->sensorType == 3)
                    sensorTypeStr = "CO2";
                if (recvSample->sensorType == 4)
                    sensorTypeStr = "FLOW";

                RK_TICK_DIS
                kprintf( "ENQ: [@%d, %s, %lu] \n\r", kTickGet(), sensorTypeStr,
                        recvSample->sensorValue);
                RK_TICK_EN
            }
        }
        else
        {
            kSignal( task2Handle);
        }
    }
}

/* for each sensor:
 . a ring buffer of AVG_WINDOW_SIZE values
 . sum of values
 . an index table (=enum - 1 eg., HUMIDITY IDX=2-1=1)
 */
static ULONG ringBuf[N_SENSOR][AVG_WINDOW_SIZE];
static ULONG ringSum[N_SENSOR] = {0};
static UINT ringIndex[N_SENSOR] = {0};

void Task2( void *args)
{

    RK_UNUSEARGS
    Mesg_t readSample;
    while (1)
    {

        RK_ERR err = kStreamRecv( &sensorStream, ( VOID*) &readSample,
        RK_NO_WAIT);
        if (err == RK_SUCCESS)
        {
            UINT sensorIdx = readSample.sensorType - 1;

/* remove oldest sample */
            ULONG oldest = ringBuf[sensorIdx][ringIndex[sensorIdx]];
            ringSum[sensorIdx] -= oldest;

/* push new sample */
            ringBuf[sensorIdx][ringIndex[sensorIdx]] = readSample.sensorValue;
            ringSum[sensorIdx] += readSample.sensorValue;

/* index incr-wrap */
            ringIndex[sensorIdx] ++;
            ringIndex[sensorIdx] %= AVG_WINDOW_SIZE;

/* simple average */
            ULONG avg = ringSum[sensorIdx] / AVG_WINDOW_SIZE;

/* we disable tick to display */
            RK_TICK_DIS

            CHAR const *sensorTypeStr = NULL;
            if (readSample.sensorType == 1)
                sensorTypeStr = "TEMP";
            if (readSample.sensorType == 2)
                sensorTypeStr = "HUM";
            if (readSample.sensorType == 3)
                sensorTypeStr = "CO2";
            if (readSample.sensorType == 4)
                sensorTypeStr = "FLOW";

            kprintf( "DEQ: [@%d, %s, %lu] | AVG: %lu \n\r", kTickGet(),
                    sensorTypeStr, readSample.sensorValue, avg);

            RK_TICK_EN

        }
        else
        {

             kPend( RK_WAIT_FOREVER);

        }
    }
}

```
====
image::images/streamqueue.png[width=30%]



==== Summing Up: Stream Queues vs Mail Queues

While both are Message Queues, they are distinct designs that lead to distinct ideal use cases. Note that Mail Queues are particularly difficult to generalise.

[cols="30%,35%,35%", options="header"]
|===
| Feature                | Mail Queue (Pointer-Based)             | Stream Queue (Deep Copy-Based)

| Message Storage        | Stores *pointers* to messages          | Stores *deep copies* of messages
| Message Size           | Either pointer-sized or can vary for each message. | Fixed (defined at queue initialisation)
| Memory Management      | Internal pre-allocated (1 pointer/message). Might need a second storage.          | Internal (pre-allocated buffer, N-words/message). 
| Data Ownership         | Sender/receiver manage lifecycle       | Kernel.
| Performance            | A 'zero-copy' transmission is faster.  | Deterministic. Kernel Optimised deep-copy.
| Best Use Cases          | Work Queues, Client-Server with dynamic payload, any case where zero-copy or 1-copy is feasible | Real-time data streaming (e.g., sensor pipelines, inter-device communication).
|===

=== Message Passing ownership

[TIP]
====
Priority Inversion happens on Message-Passing for similar but subtle different reasons from resource sharing.

_Design Choice_: add an ownership mechanism for a message passing object -- a well-defined receiver, so priority propagation can be applied.

Benefit: This preserves strict real-time guarantees, making sure a high-priority task never waits indefinitely for a lower-priority task to finish message operations
====

Using queues to communicate between multiple tasks is chaos. Many senders to many receivers ends up unpredictable. We often want N:1 (senders:receiver, N can be 1). This _1_ makes it easier to reason on the dynamics. 

On real-time design, we often expect to see blocking _send()_ operations, on 1:1 or N:1 channels - a blocking _send()_ on a 1:N (broadcast) would be very odd.

==== Priority Inversion on Message Passing

While sharing some similarities, there are subtle differences on blocking on a shared-resource (by blocking on a locked mutex), and blocking on a message passing object.

_Assuming cases we do not want messages to be overwritten_, a sender when accessing a queue is acquiring an empty buffer. A receiver is acquiring a full buffer. They are competing for the same object but on different states. Thus, they depend on each other to change the object state. 


When a sender blocks on a full shared message object, it does not mean there is another writer using the resource; By design it is also unlikely there is a reader blocked on the waiting queue of the object, since every time a write operation completes, any reader blocked on the queue is readied. Whether it is dispatched or not is a scheduler concern. If its priority is higher than the task that has just finished, it will be immediately dispatched. If not, it is enqueued on the ready queue until it is eventually picked.

[TIP]
This means the problem of priority inversion arises from waiting for the consumer rather than from direct contention among multiple senders.

So if the sender priority is higher, maybe it could be propagated to the reader. But, _which_ reader? (This is the reason semaphores cannot implement priority inheritance protocol -- the waiter task cannot a know potential signaller).

With that in mind, there is the option to set _ownership_: `setowner(mesgpass, taskhandle)`. From now own, only the owner task can receive from that service object -- a blocking `send()` knows the the target task and can raise its priority. 

(As 1:N communication normally non-blocking on real-time systems, there is no mechanism to establish 'sender ownership'.)

If other task that not the owner tries to receive from a kernel message-passing object that has an owner, it fails.

These kernel objects now will _resemble an aspect of Ports_ - a common way of representing tasks on _message-passsing kernels_. (Strictly they are not Ports,  as RK0 is not a message-passing kernel - although I do like the approach.)

 


=== Most-Recent Message Protocol (MRM)

[width="100%",options="header"]
|===============================
|MRM Control Block
|MRM Buffer Allocator  
|Data Buffer Allocator
|Current MRM Buffer Address
|Data Size (Message Size) 
|===============================

[width="100%",options="header"]
|===============================
|MRM Buffer 
|Data Buffer Address
|Readers Count
|===============================


[width="100%",options="header"]
|===============================
|Data Buffer
|_Application-dependent_
|===============================

[TIP]
====
There is not much of a practical difference between a message that does not arrive and one with no useful (stale) data. But when wrong (or stale) data is processed - e.g., to define a set point on a loop - a system can fail badly.

Design Choice: provide a broadcast asynchronous message-passing scheme that guarantees data freshness and integrity for all readers. 

Benefits: The system has a mechanism to meet strict deadlines that cannot be predicted on design time.

====
Control loops reacting to unpredictable time events - like a robot scanning an environment or a drive-by-wire system - require a different message-passing approach - readers cannot "look at the past" and cannot block. The most recent data must be delivered lock-free and have guaranteed integrity.

==== Functional Description

An _MRM_ works as a _1-to-many asynchronous Mailbox_ - with a lock-free specialisation that enables several readers to get the most recent deposited message with no integrity issues. Whenever a reader reads an MRM buffer, it will find the most recent data transmitted (which also implies always finding data). 
A writer will always have a buffer to deposit a message. 

The core idea on he MRM protocol is that readers can only access the buffer that is classified as the '_the most recent buffer_'. After a writer _publish()_ a message, that will be the only message readers can _get()_ -- any former message being processed by a reader, was grabbed _before_ a new _publish()_ - and, from now on can only be _unget()_. 

To clarify further, the communication steps are listed:

. A producer first reserves an MRM Buffer - the reserved MRM Buffer is not available for reading until it is published.

. A message is copyied into the MRM Buffer and the buffer is _published_. From now on, it is _the most recent buffer_. The former published message is no longer visibile for new readers.

. A reader starts by _getting_ an MRM Buffer.  A `get()` operation delivers a copy of the message to the reader's scope and a pointer to the buffer where the message is. Importantly, this operation increases the number of readers associated to that MRM Buffer. 

. Before ending its cycle, the task releases (`unget()`) the buffer; on releasing, the kernel checks if the caller task _is the last reader_, _and_ if it is _not the current MRM Buffer_. 

. If the above conditions are met, the the `unget()` operation will cause return the buffer to the pool. If there are more readers OR it is the current buffer, it remains as available for new readers, until the writer issues a `reserve()`.

. When `reserve` operation detects the most recent buffer still has readers - a new buffer is allocated to be written and published. 

. As explained, whenever a new message is published, the former buffer is no longer visibile for new reader, and eventually drops to 0 readers - when it will be returned to the pool by an `unget()` operation.

==== MRM Control Block Configuration


What might lead to some confusion when initialising an MRM Control Block is the need for two different pools:

- One pool will be the storage for the MRM Buffers - a structure that stores the _address on which the data representing a message is_, plus the number of readers attached to that message. 

- Another pool is for the message itself, which is the buffer that will keep the raw message data. Let's call it _data buffers_.  

- Both pools have the same number of elements: the number of tasks communicating + 1. 

- The size of the data buffers is application-dependent - and is passed as a number of _words_. The minimal message size is 32-bit. 

- If using data structures, keep it aligned to 4 to take advantage of the performance of aligned memory.


==== Usage Example

Consider a modern car - speed variations are of interest in many modules. With a somehow "naive" approach, let us consider three modules and how they should react when speed varies:

. *Cruise Control:* For the Cruise Control, a speed increase might signify the driver wants manual control back, and it will likely turn off.

. *Windshield Wipers:* If they are on, a speed change can reflect on the electric motor's adjustments to the air resistance.

. *Radio:* Speed changes reflect the aerodynamic noise - the radio volume might need adjustment.

As the variations are unpredictable, we need a mechanism to deliver the last speed in order of importance for all these modules. From highest to lowest priority, Cruise, Whipers, and Radio are the three modules that range from safety to comfort.

To emulate this scenario, we can write an application with a higher priority task that sleeps and wakes up at pseudo-random times to produce random values that represent the (unpredictable) speed changes.

We assume readers are periodic - as on most control loops - with a priority that decreases with its period increase. This is represented in the snippet below.

_Callout Timers_ are used to periodically `post()` to a listener's private semaphore, releasing the task that started blocked on it. The snippet does nothing besides printing its current pair _(last read speed, time when it was read)_.


====

```C

#define N_MRM (5)        /* Number of MRMs N Tasks + 1 */
#define MRM_MESG_SIZE (2) /* In WORDS */

RK_MRM MRMCtl;/* MRM control block */

RK_MRM_BUF buf[N_MRM];/* MRM pool */

UINT data[MRM_MESG_SIZE][N_MRM];/* message data pool */

RK_TIMER timerT2;
RK_TIMER timerT3;
RK_TIMER timerT4;

VOID kApplicationInit( VOID)
{
    kMRMMemInit( &MRMCtl, buf, data, N_MRM, MRM_MESG_SIZE);

}

/* timers callbacks */
VOID callbackT2( VOID *args)
{
    RK_UNUSEARGS
    kSignal( task2Handle);
}

VOID callbackT3( VOID *args)
{
    RK_UNUSEARGS
    kSignal( task3Handle);
}
VOID callbackT4( VOID *args)
{
    RK_UNUSEARGS
    kSignal( task4Handle);
}

VOID Task1( VOID *args)
{
    RK_UNUSEARGS
    UINT write[2];
    while (1)
    {

        RK_TICK sleepTicks = (( RK_TICK) rand() % 14) + 1;
        kSleepUntil( sleepTicks);

        UINT currTick = kTickGet();
        UINT speedValue = ( UINT) rand() % 170;
        write[1] = currTick;
        write[0] = speedValue;
/* grab a buffer */
        RK_MRM_BUF *bufPtr =  kMRMReserve( &MRMCtl);
        if (bufPtr != NULL)
        {
            kMRMPublish( &MRMCtl, bufPtr, write);
        }
        else
        {/* cannot fail */
            kassert( 0);
        }
/* publish  */
        kprintf( "! @ %dT: SPEED UPDATE: %u \n\r", currTick, speedValue);

    }
}

void Task2( void *args)
{
    RK_UNUSEARGS
    UINT read[2];
/* parms: timer, phase delay, period, callout function, args,
     RELOAD/ONESHOT */
    kTimerInit( &timerT2, 0, 3, callbackT2, NULL, RK_TIMER_RELOAD);
    while (1)
    {
        SLEEP: /* pend on its own semaphore */
        kPend( RK_WAIT_FOREVER);
/* copy the current buffer data to read and return its address */
        RK_MRM_BUF *readBuf = kMRMGet( &MRMCtl, read);
        if (readBuf == NULL)
        {
            kassert(0);/*cant happen*/
        }
/* use read[] to process the contents */
        kprintf( "@ %dT CRUISE: (%u, %uT) \n\r", kTickGet(), read[0], read[1]);
/* use the buffer address to release the buffer */
        kMRMUnget( &MRMCtl, readBuf);

    }
}

VOID Task3( VOID *args)
{
    RK_UNUSEARGS
    UINT read[2];
/* parms: timer, phase delay, period, callout function, args,
       RELOAD/ONESHOT */
    kTimerInit( &timerT3, 0, 5, callbackT3, NULL, RK_TIMER_RELOAD);
    while (1)
    {
        SLEEP: /* pend on its own semaphore */
        kPend( RK_WAIT_FOREVER);
/* copy the current buffer data to read and return its address */
        RK_MRM_BUF *readBuf = kMRMGet( &MRMCtl, read);
        if (readBuf == NULL)
        {
            kassert(0);/*cant happen*/
        }
        kprintf( "@ %dT WHIPERS: (%u, %uT) \n\r", kTickGet(), read[0], read[1]);
        kMRMUnget( &MRMCtl, readBuf);/* release buffer */

    }
}
VOID Task4( VOID *args)
{
    RK_UNUSEARGS
    UINT read[2];
/* parms: timer, phase delay, period, callout function, args,
       RELOAD/ONESHOT */
    kTimerInit( &timerT4, 0, 7, callbackT4, NULL, RK_TIMER_RELOAD);
    while (1)
    {
        SLEEP: /* pend on its own semaphore */
        kPend( RK_WAIT_FOREVER);
/* copy the current buffer data to read and return its address */
        RK_MRM_BUF *readBuf = kMRMGet( &MRMCtl, read);
        if (readBuf == NULL)
        {
            kassert(0);/*cant happen*/
        }
        kprintf( "@ %dT RADIO: (%u, %uT) \n\r", kTickGet(), read[0], read[1]);
        kMRMUnget( &MRMCtl, readBuf);/* release the buffer */
    }

}
====

As the speed update interval is randomly chosen in the range [1,14], when an update happens, we expect to see the following cases:

- All tasks read the updated pair (speed, time)
- Not all tasks receive the updated pair because another update happens in between.
- No tasks receive an update - because another happens too soon.
- All tasks receive an update and will keep rereading the same values - because another update takes longer than the task period.

All these cases are on the image of what was printed on a terminal:

image::images/pumpdrop.png[width=30%]

_PS: MRMs draws inspiration from 'CABs' (Cyclical Asynchronous Buffers) in the HARTIK Operating System (Buttazzo, 1993)_.



---

image::images/mascott.png[width=10%,align=center]

(C) _2025 Antonio Giacomelli | All Rights Reserved | http://kernel0.org/[www.kernel0.org]_
